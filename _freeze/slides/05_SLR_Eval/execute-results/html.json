{
  "hash": "afae2e53f9499a990299a640eb2efa01",
  "result": {
    "markdown": "---\ntitle: \"SLR: Model Evaluation\"\nauthor: \"Nicky Wakim\"\ntitle-slide-attributes:\n    data-background-color: \"#213c96\"\ndate: \"01/17/2023\"\ncategories: [\"Week 1\"]\nformat: \n  revealjs:\n    theme: [default, simple_NW.scss]\n    toc: true\n    toc-depth: 1\n    toc-title: Class Overview\n    chalkboard: true\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: SLR 1\n    html-math-method: mathjax\n    highlight-style: ayu\nexecute:\n  echo: true\n  freeze: auto  # re-render only when source changes\neditor: \n  markdown: \n    wrap: 72\n---\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngapm <- read_csv(\"data/lifeexp_femlit_2011.csv\")\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nRows: 188 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): country\ndbl (2): life_expectancy_years_2011, female_literacy_rate_2011\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n:::\n:::\n\n\n\n# Learning Objectives\n\n1.  Identify the aims of your research and see how they align with the\n    intended purpose of simple linear regression\n\n2.  Identify the simple linear regression model and define statistics\n    language for key notation\n\n3.  Illustrate how ordinary least squares (OLS) finds the best model\n    parameter estimates\n\n4.  Solve the optimal coefficient estimates for simple linear regression\n    using OLS\n\n5.  Apply OLS in R for simple linear regression of real data\n\n## Topics\n\n\n-   LINE assumptions\n\n-   checking assumptions\n-   residual analysis\n-   outlier detection \n\n# Next class???\n\n## What are the LINE conditions?\n\nFor \"good\" model fit and to be able to make inferences and predictions\nbased on our models, 4 conditions need to be satisfied.\n\nBriefly:\n\n-   **L** inearity of relationship between variables\n-   **I** ndependence of the Y values\n-   **N** ormality of the residuals\n-   **E** quality of variance of the residuals (homoscedasticity)\n\n[More in\ndepth](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions):\n\n-   **L** : there is a linear relationship between the mean response (Y)\n    and the explanatory variable (X),\n-   **I** : the errors are independent—there’s no connection between how\n    far any two points lie from the regression line,\n-   **N** : the responses are normally distributed at each level of X,\n    and\n-   **E** : the variance or, equivalently, the standard deviation of the\n    responses is equal for all levels of X.\n\n## L: Linearity of relationship between variables\n\nIs the association between the variables linear?\n\n-   Diagnostic tools:\n    -   Scatterplot\n    -   Residual plot (see later section for E : Equality of variance of\n        the residuals)\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 108 rows containing non-finite values (`stat_smooth()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 108 rows containing non-finite values (`stat_smooth()`).\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 108 rows containing missing values (`geom_point()`).\n```\n:::\n\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-3-1.png){width=1152}\n:::\n:::\n\n\n\n## I: Independence of the residuals ($Y$ values)\n\n-   **Are the data points independent of each other?**\n\n-   Examples of when they are *not* independent, include\n\n    -   repeated measures (such as baseline, 3 months, 6 months)\n    -   data from clusters, such as different hospitals or families\n\n-   This condition is checked by reviewing the study *design* and not by\n    inspecting the data\n\n-   How to analyze data using regression models when the $Y$-values are\n    not independent is covered in BSTA 519 (Longitudinal data)\n\n# N: Normality of the residuals\n\n-   Extract residuals from regression model in R\n-   Diagnostic tools:\n    -   Distribution plots of residuals\n    -   QQ plots\n\n## N: Normality of the residuals\n\n-   The responses Y are normally distributed at each level of x\n\n![](/img_slides/OLSassumptions-1.png){fig-align=\"center\"}\n\n::: {style=\"font-size: 60%;\"}\n<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>\n:::\n\n## Extract model's residuals in R\n\n-   First extract the residuals' values from the model output using the\n    `augment()` function from the `broom` package.\n-   Get a tibble with the orginal data, as well as the residuals and\n    some other important values.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, \n                data = gapm)\naug1 <- augment(model1) \n\nglimpse(aug1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRows: 80\nColumns: 9\n$ .rownames                  <chr> \"1\", \"2\", \"5\", \"6\", \"7\", \"8\", \"14\", \"22\", \"…\n$ life_expectancy_years_2011 <dbl> 56.7, 76.7, 60.9, 76.9, 76.0, 73.8, 71.0, 7…\n$ female_literacy_rate_2011  <dbl> 13.0, 95.7, 58.6, 99.4, 97.9, 99.5, 53.4, 9…\n$ .fitted                    <dbl> 53.94643, 73.14897, 64.53453, 74.00809, 73.…\n$ .resid                     <dbl> 2.7535654, 3.5510294, -3.6345319, 2.8919074…\n$ .hat                       <dbl> 0.13628996, 0.01768176, 0.02645854, 0.02077…\n$ .sigma                     <dbl> 6.172684, 6.168414, 6.167643, 6.172935, 6.1…\n$ .cooksd                    <dbl> 1.835891e-02, 3.062372e-03, 4.887448e-03, 2…\n$ .std.resid                 <dbl> 0.48238134, 0.58332052, -0.59972251, 0.4757…\n```\n:::\n:::\n\n\n\n## Check normality with \"usual\" distribution plots\n\nNote that below I save each figure, and then combine them together in\none row of output using `grid.arrange()` from the `gridExtra` package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhist1 <- ggplot(aug1, aes(x = .resid)) +\n  geom_histogram()\n\ndensity1 <- ggplot(aug1, aes(x = .resid)) +\n  geom_density()\n\nbox1 <- ggplot(aug1, aes(x = .resid)) +\n  geom_boxplot()\n\nlibrary(gridExtra) # NEW!!!\ngrid.arrange(hist1, density1, box1, nrow = 1)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-5-1.png){width=1152}\n:::\n:::\n\n\n\n## Normal QQ plots (QQ = quantile-quantile)\n\n-   It can be tricky to eyeball with a histogram or density plot whether\n    the residuals are normal or not\n-   QQ plots are often used to help with this\n\n::: columns\n::: {.column width=\"60%\"}\n-   *Vertical axis*: **data quantiles**\n    -   data points are sorted in order and\n    -   assigned quantiles based on how many data points there are\n-   *Horizontal axis*: **theoretical quantiles**\n    -   mean and standard deviation (SD) calculated from the data points\n    -   theoretical quantiles are calculated for each point, assuming\n        the data are modeled by a normal distribution with the mean and\n        SD of the data\n:::\n\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-6-1.png){width=480}\n:::\n:::\n\n\n:::\n:::\n\n-   **Data are approximately normal if points fall on a line.**\n\nSee more info at\n<https://data.library.virginia.edu/understanding-QQ-plots/>\n\n## Examples of Normal QQ plots (1/5)\n\n-   **Data**:\n    -   Body measurements from 507 physically active individuals\n    -   in their 20's or early 30's\n    -   within normal weight range.\n\n![](/img_slides/qq_wristdiam.png){fig-align=\"center\"}\n\n## Examples of Normal QQ plots (2/5)\n\nSkewed right distribution\n\n<br>\n\n![](/img_slides/qq_weights.png){fig-align=\"center\"}\n\n## Examples of Normal QQ plots (3/5)\n\nLong tails in distribution\n\n<br>\n\n![](/img_slides/qq_biliac.png){fig-align=\"center\"}\n\n## Examples of Normal QQ plots (4/5)\n\nBimodal distribution\n\n<br>\n\n![](/img_slides/qq_forearm.png){fig-align=\"center\"}\n\n## Examples of Normal QQ plots (5/5)\n\n![](/img_slides/qq_forearm_gender.png){fig-align=\"center\"}\n\n## QQ plot of residuals of `model1`\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n```\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n```\n:::\n\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-7-1.png){width=1152}\n:::\n:::\n\n\n\n::: columns\n::: {.column width=\"50%\"}\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(aug1, aes(sample = .resid)) + \n  stat_qq() +     # points\n  stat_qq_line()  # line\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-9-1.png){width=480}\n:::\n:::\n\n\n:::\n:::\n\n## Compare to randomly generated Normal QQ plots\n\nHow \"*good*\" we can expect a QQ plot to look depends on the sample size.\n\n-   The QQ plots on the next slides are randomly generated\n\n    -   using random samples from actual standard normal distributions\n        $N(0,1)$.\n\n-   Thus, all the points in the QQ plots **should theoretically** fall\n    in a line\n\n-   However, there is sampling variability...\n\n## Randomly generated Normal QQ plots: n=100\n\n-   Note that `stat_qq_line()` doesn't work with randomly generated\n    samples, and thus the code below manually creates the line that the\n    points should be on (which is $y=x$ in this case.)\n\n::: columns\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 90%;\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamplesize <- 100\n\nrand_qq1 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n```\n:::\n\n\n:::\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)\n```\n\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-11-1.png){width=768}\n:::\n:::\n\n\n:::\n:::\n\n## Examples of simulated Normal QQ plots: n=10\n\nWith fewer data points,\n\n-   simulated QQ plots are more likely to look \"less normal\"\n-   even though the data points were sampled from normal distributions.\n\n::: columns\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 90%;\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamplesize <- 10  # only change made to code!\n\nrand_qq1 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n```\n:::\n\n\n:::\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)\n```\n\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-13-1.png){width=768}\n:::\n:::\n\n\n:::\n:::\n\n## Examples of simulated Normal QQ plots: n=1,000\n\nWith more data points,\n\n-   simulated QQ plots are more likely to look \"more normal\"\n\n::: columns\n::: {.column width=\"50%\"}\n::: {style=\"font-size: 90%;\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsamplesize <- 1000 # only change made to code!\n\nrand_qq1 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  # line y=x\n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\") \n\nrand_qq2 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq3 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n\nrand_qq4 <- ggplot() +\n  stat_qq(aes(sample = rnorm(samplesize))) + \n  geom_abline(intercept = 0, slope = 1, \n              color = \"blue\")\n```\n:::\n\n\n:::\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngrid.arrange(rand_qq1, rand_qq2, \n             rand_qq3, rand_qq4, ncol =2)\n```\n\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-15-1.png){width=768}\n:::\n:::\n\n\n:::\n:::\n\n## Back to our example\n\n::: columns\n::: {.column width=\"40%\"}\nResiduals from Life Expectancy vs. Female Literacy Rate Regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(aug1, \n      aes(sample = .resid)) + \n  stat_qq() + \n  stat_qq_line() \n```\n:::\n\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-17-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n\nSimulated QQ plot of Normal Residuals with n = 80\n\n::: columns\n::: {.column width=\"40%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# number of observations \n# in fitted model\nnobs(model1) \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 80\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  stat_qq(aes(\n    sample = rnorm(80))) + \n  geom_abline(\n    intercept = 0, slope = 1, \n    color = \"blue\")\n```\n:::\n\n\n:::\n\n::: {.column width=\"60%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n:::\n:::\n\n# E: Equality of variance of the residuals {.nostretch}\n\n-   Homoscedasticity\n-   Diagnostic tool: **residual plot**\n\n![](/img_slides/OLSassumptions-1.png){fig-align=\"center\" width=\"60%\"}\n\n::: {style=\"font-size: 60%;\"}\n<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>\n:::\n\n## Residual plot\n\n-   $x$ = explanatory variable from regression model\n    -   (or the fitted values for a multiple regression)\n-   $y$ = residuals from regression model\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(aug1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \".rownames\"                  \"life_expectancy_years_2011\"\n[3] \"female_literacy_rate_2011\"  \".fitted\"                   \n[5] \".resid\"                     \".hat\"                      \n[7] \".sigma\"                     \".cooksd\"                   \n[9] \".std.resid\"                \n```\n:::\n:::\n\n\n\n<br>\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(aug1, \n       aes(x = female_literacy_rate_2011, \n           y = .resid)) + \n  geom_point() +\n  geom_abline(\n    intercept = 0, \n    slope = 0, \n    color = \"orange\") +\n  labs(title = \"Residual plot\")\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](05_SLR_Eval_files/figure-html/unnamed-chunk-23-1.png){width=576}\n:::\n:::\n\n\n:::\n:::\n\n## E: Equality of variance of the residuals (Homoscedasticity)\n\n-   The **variance** or, equivalently, the standard deviation of the\n    responses is **equal for all values of x**.\n-   This is called **homoskedasticity** (top row)\n-   If there is **heteroskedasticity** (bottom row), then the assumption\n    is not met.\n\n![](/img_slides/heteroskedastic.png){fig-align=\"center\"}\n\n# $R^2$ = Coefficient of determination\n\nAnother way to assess model fit\n\n## $R^2$ = Coefficient of determination (1/2)\n\n-   Recall that the correlation coefficient $r$ measures the strength of\n    the linear relationship between two numerical variables\n-   $R^2$ is usually used to measure the strength of a *linear fit*\n    -   For a simple linear regression model (one numerical predictor),\n        $R^2$ is just the square of the correlation coefficient\n-   In general, $R^2$ is the proportion of the variability of the\n    dependent variable that is **explained** by the independent\n    variable(s)\n\n$$R^2 = \\frac{\\textrm{variance of predicted y-values}}\n{\\textrm{variance of observed y-values}} = \\frac{\\sum_{i=1}^n(\\widehat{y}_i-\\bar{y})^2}\n{\\sum_{i=1}^n(y_i-\\bar{y})^2} \n = \\frac{s_y^2 - s_{\\textrm{residuals}}^2}\n{s_y^2}$$ $$R^2 = 1- \\frac{s_{\\textrm{residuals}}^2}\n{s_y^2}$$ where $\\frac{s_{\\textrm{residuals}}^2}{s_y^2}$ is the\nproportion of \"unexplained\" variability in the $y$ values,\\\nand thus $R^2 = 1- \\frac{s_{\\textrm{residuls}}^2}{s_y^2}$ is the\nproportion of \"explained\" variability in the $y$ values\n\n## $R^2$ = Coefficient of determination (2/2)\n\n-   Recall, $-1<r<1$\n\n-   Thus, $0<R^2<1$\n\n-   In practice, we want \"high\" $R^2$ values, i.e. $R^2$ as close to 1\n    as possible.\n\nCalculating $R^2$ in R using `glance()` from the `broom` package:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglance(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.411         0.403  6.14      54.4 1.50e-10     1  -258.  521.  529.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n:::\n\n```{.r .cell-code}\nglance(model1)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4109366\n```\n:::\n:::\n\n\n\n::: callout-warning\n-   A model can have a high $R^2$ value when there is a curved pattern.\n-   Always first check whether a linear model is reasonable or not.\n:::\n\n## $R^2$ in `summary()` R output\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nlm(formula = life_expectancy_years_2011 ~ female_literacy_rate_2011, \n    data = gapm)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-22.299  -2.670   1.145   4.114   9.498 \n\nCoefficients:\n                          Estimate Std. Error t value Pr(>|t|)    \n(Intercept)               50.92790    2.66041  19.143  < 2e-16 ***\nfemale_literacy_rate_2011  0.23220    0.03148   7.377  1.5e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.142 on 78 degrees of freedom\n  (108 observations deleted due to missingness)\nMultiple R-squared:  0.4109,\tAdjusted R-squared:  0.4034 \nF-statistic: 54.41 on 1 and 78 DF,  p-value: 1.501e-10\n```\n:::\n:::\n\n\n\nCompare to the square of the correlation coefficient $r$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- cor(x = gapm$life_expectancy_years_2011, \n    y = gapm$female_literacy_rate_2011,\n    use =  \"complete.obs\")\nr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.6410434\n```\n:::\n\n```{.r .cell-code}\nr^2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.4109366\n```\n:::\n:::\n",
    "supporting": [
      "05_SLR_Eval_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
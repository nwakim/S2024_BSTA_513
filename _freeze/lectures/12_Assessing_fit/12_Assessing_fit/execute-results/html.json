{
  "hash": "1166f5ba64a3447d38e7fd4fcfb04320",
  "result": {
    "markdown": "---\ntitle: \"Lesson 12: Assessing Model Fit\"\nauthor: \"Nicky Wakim\"\ntitle-slide-attributes:\n    data-background-color: \"#C2352F\"\ndate: \"05/13/2024\"\nformat: \n  revealjs:\n    theme: \"../simple_NW.scss\"\n    chalkboard: true\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: \"Lesson 12: Assessing Model Fit\"\n    html-math-method: mathjax\n    highlight-style: ayu\nexecute:\n  echo: true\n  freeze: auto  # re-render only when source changes\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n:::\n\n\n\n# Learning Objectives\n\n1.  Use test statistics of goodness-of-fit to determine if our preliminary final model fits the data well\n\n    -   Using Pearson residual statistic (ùëã\\^2) \\~ ùúí\\_(ùêΩ‚àí(ùëù+1))\\^2\n    -   Using Hosmer and Lemeshow goodness-of-fit statistic (ùê∂¬†ÃÇ) \\~ ùúí\\_(ùëî‚àí2)\\^2\n\n2.  Apply ROC AUC to determine how well model predicts binary outcome\n\n3.  Apply AIC and BIC as a summary measure to make additional comparisons between potential models\n\n\n\n::: {.cell}\n<style type=\"text/css\">\n.reveal code {\n  max-height: 80% !important;\n}\n</style>\n:::\n\n\n\n## Overview (NEED TO REVISIT!!)\n\n-   Once the preliminary final model has been determined, we need to assess the fit of the model\n\n-   Variable selection is no longer our focus at this stage\n\n    -   We want to find answer to whether the model fits the data adequately\n\n-   Assessing the Goodness of Fit or Assessing model fit\n\n    -   Assess how well our fitted logistic regression model predicts/estimates the observed outcomes\n\n    -   Comparison: fitted/estimated outcome vs. observed outcome\n\n## Overview (NEED TO REVISIT!!)\n\n-   The model building strategies we have discussed so far only assess the importance of covariates\n\n    -   It did not assess model fit\n\n-   Previous in model building, we made relative comparisons between models\n\n    -   Our conclusions were limited to: Model 1 (full model) fits data better than Model 2 (reduced model)\n\n-   Assessing goodness of fit is\n\n    -   Not a relative comparison\n    -   It is an absolute comparison\n    -   To compare the fitted model to the largest possible model (saturated model)\n    -   Model adequacy vs. Model comparison\n\n## Poll Everywhere Question 1\n\n## Components to Assess Model Fit\n\n-   The model fits the data well if\n\n    -   Summary measures of the distance between the predicted/estimated/fitted and observed Y are small\n\n    -   The contribution of each pair (predicted and observed) to these summary measures is unsystematic and is small relative to the error structure of the model\n\n    -   It is possible to see a ‚Äúgood‚Äù summary measure of the distance between predicted and observed Y with some substantial deviation from fit for a few subjects\n\n## Components to Assess Model Fit\n\n1.  Computation and evaluation of **overall measures of fit**\n\n2.  Examination of other measures of the difference or **distance between the observed and fitted/predicted values**\n\n3.  Examination of the **individual components** of the summary statistics (will do in Lesson 14: Model Diagnostics)\n\n-   Today we focus on #1 and #2\n\n## Summary Measures of Goodness of Fit\n\n# Learning Objectives\n\n## Comparing fitted outcome to observed outcome\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
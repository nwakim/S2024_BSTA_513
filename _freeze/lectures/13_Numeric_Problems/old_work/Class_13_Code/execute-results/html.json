{
  "hash": "7563faa0495c7622a7d771f2c1ee97e7",
  "result": {
    "markdown": "---\ntitle: \"Class 13 Code\"\nauthor: \"Nicky Wakim\"\ndate: \"2023-05-21\"\noutput: html_document\n---\n\n<style type=\"text/css\">\n.main-container {\n  max-width: 1100px;\n  margin-left: auto;\n  margin-right: auto;\n}\n</style>\n\n<style type=\"text/css\">\nbody, td {\n   font-size: 16px;\n}\ncode.r{\n  font-size: 16px;\n}\npre {\n  font-size: 16px\n}\n</style>\n\n## Set up for the code\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(car)\nlibrary(mctest)\nlibrary(epiDisplay)\nlibrary(kableExtra)\nlibrary(dplyr)\n```\n:::\n\n\n## Zero cell in contingency table\n\n### One covariate\n\n::: {.cell}\n\n```{.r .cell-code}\nex1 = as.data.frame(rbind(cbind(outcome = rep(1, 39), x = c(rep(\"One\", 7), rep(\"Two\", 12), rep(\"Three\", 20))), \n      cbind(outcome = rep(0, 21), x = c(rep(\"One\", 13), rep(\"Two\", 8))))) %>%\n  mutate(outcome = as.numeric(outcome), \n         x = factor(x, levels = c(\"One\", \"Two\", \"Three\")))\n```\n:::\n\n\nRunning glm from the data\n\n::: {.cell}\n\n```{.r .cell-code}\nex1_glm = glm(outcome ~ x, data = ex1, family = binomial())\nsummary(ex1_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = outcome ~ x, family = binomial(), data = ex1)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)\n(Intercept)   -0.6190     0.4688  -1.320    0.187\nxTwo           1.0245     0.6543   1.566    0.117\nxThree        20.1851  2404.6705   0.008    0.993\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.694  on 59  degrees of freedom\nResidual deviance: 52.818  on 57  degrees of freedom\nAIC: 58.818\n\nNumber of Fisher Scoring iterations: 18\n```\n:::\n\n```{.r .cell-code}\nlogistic.display(ex1_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLogistic regression predicting outcome \n \n            OR(95%CI)            P(Wald's test) P(LR-test)\nx: ref.=One                                     < 0.001   \n   Two      2.79 (0.77,10.04)    0.117                    \n   Three    583822600.8 (0,Inf)  0.993                    \n                                                          \nLog-likelihood = -26.4092\nNo. of observations = 60\nAIC value = 58.8183\n```\n:::\n:::\n\n\nCombine groups 2 and 3:\n\n::: {.cell}\n\n```{.r .cell-code}\nex1_23 = ex1 %>% mutate(x = factor(x, levels = c(\"One\", \"Two\", \"Three\"), labels = c(\"One\", \"Two\", \"Two\")))\nex1_23_glm = glm(outcome ~ x, data = ex1_23, family = binomial())\nsummary(ex1_23_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = outcome ~ x, family = binomial(), data = ex1_23)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)   \n(Intercept)  -0.6190     0.4688   -1.32  0.18668   \nxTwo          2.0053     0.6132    3.27  0.00107 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.694  on 59  degrees of freedom\nResidual deviance: 65.930  on 58  degrees of freedom\nAIC: 69.93\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nEliminate the third group:\n\n::: {.cell}\n\n```{.r .cell-code}\nex1_two = ex1 %>% filter(x != \"Three\")\nex1_two_glm = glm(outcome ~ x, data = ex1_two, family = binomial())\nsummary(ex1_two_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = outcome ~ x, family = binomial(), data = ex1_two)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)\n(Intercept)  -0.6190     0.4688  -1.320    0.187\nxTwo          1.0245     0.6543   1.566    0.117\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.352  on 39  degrees of freedom\nResidual deviance: 52.818  on 38  degrees of freedom\nAIC: 56.818\n\nNumber of Fisher Scoring iterations: 4\n```\n:::\n:::\n\n\nTreat covariate as continuous:\n\n::: {.cell}\n\n```{.r .cell-code}\nex1_cont = ex1 %>% mutate(x = as.numeric(x))\ntable(ex1_cont$outcome, ex1_cont$x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   \n     1  2  3\n  0 13  8  0\n  1  7 12 20\n```\n:::\n\n```{.r .cell-code}\nex1_cont_glm = glm(outcome ~ x, data = ex1_cont, family = binomial())\nsummary(ex1_cont_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = outcome ~ x, family = binomial(), data = ex1_cont)\n\nCoefficients:\n            Estimate Std. Error z value Pr(>|z|)    \n(Intercept)  -2.7201     0.8755  -3.107  0.00189 ** \nx             1.8285     0.4843   3.775  0.00016 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.694  on 59  degrees of freedom\nResidual deviance: 56.883  on 58  degrees of freedom\nAIC: 60.883\n\nNumber of Fisher Scoring iterations: 5\n```\n:::\n\n```{.r .cell-code}\nlogistic.display(ex1_cont_glm)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nLogistic regression predicting outcome \n \n               OR(95%CI)          P(Wald's test) P(LR-test)\nx (cont. var.) 6.22 (2.41,16.08)  < 0.001        < 0.001   \n                                                           \nLog-likelihood = -28.4417\nNo. of observations = 60\nAIC value = 60.8834\n```\n:::\n:::\n\n\n### Interaction\n\n::: {.cell}\n\n```{.r .cell-code}\noutcome = c(rep(1, 35), rep(0, 25))\nx1 = c(rep(1, 5), rep(0, 2), rep(1, 10), rep(0, 2), rep(1, 15), rep(0, 1),\n       rep(1, 5), rep(0, 8), rep(1, 2), rep(0, 6), rep(0, 4))\nx2 = c(rep(1, 7), rep(2, 12), rep(3, 16), \n       rep(1, 13), rep(2, 8), rep(3, 4))\nex2 = data.frame(outcome = outcome, x1 = x1, x2 = x2)\n```\n:::\n\n\n## Complete Separation\n\n::: {.cell}\n\n```{.r .cell-code}\ny = c(0,0,0,0,1,1,1,1)\nx1 = c(1,2,3,3,5,6,10,11)\nx2 = c(3,2,-1,-1,2,4,1,0)\nex3 = data.frame(outcome = y, x1 = x1, x2= x2)\nex3\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  outcome x1 x2\n1       0  1  3\n2       0  2  2\n3       0  3 -1\n4       0  3 -1\n5       1  5  2\n6       1  6  4\n7       1 10  1\n8       1 11  0\n```\n:::\n\n```{.r .cell-code}\nm1 = glm(y ~ x1 + x2, family=binomial)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n:::\n\n```{.r .cell-code}\nsummary(m1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial)\n\nCoefficients:\n              Estimate Std. Error z value Pr(>|z|)\n(Intercept)    -66.098 183471.723   0.000        1\nx1              15.288  27362.843   0.001        1\nx2               6.241  81543.720   0.000        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.1090e+01  on 7  degrees of freedom\nResidual deviance: 4.5454e-10  on 5  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 24\n```\n:::\n\n```{.r .cell-code}\nlibrary(logistf)\nm1_f = logistf(y ~ x1 + x2, family=binomial)\nsummary(m1_f)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nlogistf(formula = y ~ x1 + x2, family = binomial)\n\nModel fitted by Penalized ML\nCoefficients:\n                  coef  se(coef)   lower 0.95 upper 0.95     Chisq          p\n(Intercept) -2.9748898 1.7244237 -15.47721665 -0.1208883 4.2179522 0.03999841\nx1           0.4908484 0.2745754   0.05268216  2.1275832 5.0225056 0.02501994\nx2           0.4313732 0.4988396  -0.65793078  4.4758930 0.7807099 0.37692411\n            method\n(Intercept)      2\nx1               2\nx2               2\n\nMethod: 1-Wald, 2-Profile penalized log-likelihood, 3-None\n\nLikelihood ratio test=5.505687 on 2 df, p=0.06374636, n=8\nWald test = 3.624899 on 2 df, p = 0.1632538\n```\n:::\n:::\n\n\n\n## Collinearity\n\nBring in GLOW study again\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(aplore3)\nglow = glow500\nglow2 = glow %>% mutate(raterisk2 = factor(raterisk, levels = c(\"Less\", \"Same\", \"Greater\"), \n                                            labels = c(\"Less and Same\", \"Less and Same\", \"Greater\"))) \n```\n:::\n\n\nVIF\n\n::: {.cell}\n\n```{.r .cell-code}\nmain_eff = glm(fracture ~ age + height + priorfrac + momfrac + \n                 armassist + raterisk2, \n                 data = glow2, family = binomial)\n\nglow_model = glm(fracture ~ age + height + priorfrac + momfrac + \n                   armassist + raterisk2 + weight + bmi, \n                 data = glow2, family = binomial)\n\nlibrary(car)\nvif(main_eff)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n      age    height priorfrac   momfrac armassist raterisk2 \n 1.169168  1.067010  1.118853  1.021981  1.103161  1.067098 \n```\n:::\n\n```{.r .cell-code}\nvif(glow_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       age     height  priorfrac    momfrac  armassist  raterisk2     weight \n  1.368259  19.561368   1.134230   1.028988   1.300983   1.120429 166.214541 \n       bmi \n152.406849 \n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglow3 = glow2 %>% mutate(height_sq = height^2)\nheight2 = glm(fracture ~ age + height + priorfrac + momfrac + \n                 armassist + raterisk2 + height_sq, \n                 data = glow3, family = binomial)\n# summary(height2)\nvif(height2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n       age     height  priorfrac    momfrac  armassist  raterisk2  height_sq \n  1.173304 571.542009   1.120032   1.023053   1.101958   1.069178 571.172305 \n```\n:::\n:::\n\n\nCentering height helps with the VIF:\n\n::: {.cell}\n\n```{.r .cell-code}\nglow4 = glow2 %>% mutate(height_c = (height - mean(height)), \n                         height_c_sq = height_c^2)\nheight_c_2 = glm(fracture ~ age + height_c + priorfrac + momfrac + \n                 armassist + raterisk2 + height_c_sq, \n                 data = glow4, family = binomial)\n# summary(height_c_2)\nvif(height_c_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n        age    height_c   priorfrac     momfrac   armassist   raterisk2 \n   1.173304    1.072721    1.120032    1.023053    1.101958    1.069178 \nheight_c_sq \n   1.006067 \n```\n:::\n:::",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
{
  "hash": "eea0a31ac266dfc3aeea4dbbc236d049",
  "result": {
    "markdown": "---\ntitle: \"Lesson 14: Model Building\"\nsubtitle: \"With an emphasis on prediction\"\nauthor: \"Nicky Wakim\"\ntitle-slide-attributes:\n    data-background-color: \"#C2352F\"\ndate: \"05/20/2024\"\nformat: \n  revealjs:\n    theme: \"../simple_NW.scss\"\n    chalkboard: true\n    slide-number: true\n    show-slide-number: all\n    width: 1955\n    height: 1100\n    footer: \"Lesson 14: Model Building\"\n    html-math-method: mathjax\n    highlight-style: ayu\nexecute:\n  echo: true\n  freeze: auto  # re-render only when source changes\neditor_options: \n  chunk_output_type: console\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(tidytext)\n#library(textrecipes)\nlibrary(here)\nlibrary(aplore3)\n\ntheme_set(theme_minimal())\n\n\nmean_age = mean(glow500$age) %>% round()\nglow1 = glow500 %>% mutate(age_c = age - mean_age)\n```\n:::\n\n\n\n# Learning Objectives\n\n\n## Some important definitions\n\n-   **Model selection**: picking the \"best\" model from a set of possible models\n\n    -   Models will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n\n \n\n-   **Model selection strategies**: a process or framework that helps us pick our \"best\" model\n\n    -   These strategies often differ by the approach and criteria used to the determine the \"best\" model\n\n \n\n-   **Overfitting**: result of fitting a model so closely to our *particular* sample data that it cannot be generalized to other samples (or the population)\n\n## Bias-variance trade off\n\n::: columns\n::: {.column width=\"50%\"}\n\n-   Recall from 512/612: MSE can be written as a function of the bias and variance\n\n    $$\n    MSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n    $$\n    \n    -   **We no longer use MSE in logistic regression to find the best fit model, BUT the idea between the bias and variance trade off holds!**\n\n-   For the same data:\n\n    -   More covariates in model: less bias, more variance\n    \n        -   Potential overfitting: with new data does our model still hold?\n\n    -   Less covariates in model: more bias, less variance\n\n:::\n\n::: {.column width=\"50%\"}\n[![Source: http://scott.fortmann-roe.com/docs/BiasVariance.html](images/biasvariance_tradeoff.png){width=\"1000\"}](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n:::\n:::\n\n## The goals of association vs. prediction\n\n::: columns\n::: column\n::: definition\n::: def-title\nAssociation / Explanatory / One variable's effect\n:::\n\n::: def-cont\n-   **Goal:** Understand one variable's (or a group of variable's) effect on the response after adjusting for other factors\n\n-   Mainly interpret odds ratios of the variable that is the focus of the study\n\n:::\n:::\n:::\n\n::: column\n::: proposition\n::: prop-title\nPrediction\n:::\n\n::: prop-cont\n-   **Goal:** to calculate the most precise prediction of the response variable\n\n-   Interpreting coefficients is not important\n\n-   Choose only the variables that are strong predictors of the response variable\n\n    -   Excluding irrelevant variables can help reduce widths of the prediction intervals\n\n:::\n:::\n:::\n:::\n\n## Model selection strategies for *categorical* outcomes\n\n::: columns\n::: column\n::: definition\n::: def-title\nAssociation / Explanatory / One variable's effect\n:::\n\n::: def-cont\n-   Selection of potential models is tied more with the research context with some incorporation of prediction scores\n\n \n\n-   Pre-specification of multivariable model\n\n-   Purposeful model selection\n\n    -   \"Risk factor modeling\"\n\n-   Change in Estimate (CIE) approaches\n\n    -   Will learn in Survival Analysis (BSTA 514)\n:::\n:::\n:::\n\n::: column\n::: proposition\n::: prop-title\nPrediction\n:::\n\n::: prop-cont\n-   Selection of potential models is fully dependent on prediction scores\n\n \n\n-   Logistic regression with more refined model selection\n    \n    -   Regularization techniques (LASSO, Ridge, Elastic net)\n    \n-   Machine learning realm\n    \n    -   Decision trees, random forest, k-nearest neighbors, Neural networks\n:::\n:::\n:::\n:::\n\n## Before I move on...\n\n-   We CAN use purposeful selection from last quarter in **any** type of generalized linear model (GLM)\n\n    -   This includes logistic regression!\n    \n \n    \n-   The best documented information on purposeful selection is in the Hosmer-Lemeshow textbook on logistic regression\n\n    -   [Textbook in student files is linked here](https://ohsuitg-my.sharepoint.com/:b:/r/personal/wakim_ohsu_edu/Documents/Teaching/Classes/S2024_BSTA_513_613/Student_files/Textbooks/Hosmer_Applied_Logistic_Regression.pdf?csf=1&web=1&e=3tVxMV)\n    \n    -   Purposeful selection starts on page 89 (or page 101 in the pdf)\n\n \n\n-   I will not discuss purposeful selection in this course \n\n    -   Be aware that this is a tool that you can use in any regression!\n\n## Okay, so prediction of categorical outcomes\n\n-   **Classification:** process of predicting categorical responses/outcomes\n\n    -   Assigning a category outcome based on an observation's predictors\n    \n-   Common classification methods ([good site on brief explanation of each](https://www.mathworks.com/campaigns/offers/next/choosing-the-best-machine-learning-classification-model-and-avoiding-overfitting.html))\n\n    -   Logistic regression\n    -   Naive Bayes\n    -   k-Nearest Neighbor (KNN)\n    -   Decision Trees\n    -   Support Vector Machines (SVMs)\n    -   Neural Networks\n\n## Logistic regression is a classification method\n\n-   But to be a good classifier, our model needs to built a certain way\n     \n-   Prediction depends on type of variable/model selection! \n\n    -   This is when it can become machine learning\n    \n-   So the big question is: how do we select this model??\n    \n## Poll Everywhere Question 1\n\n## Overview of the process\n\n-   \n\n## Splitting data\n\n-   Let's keep this in context of the outcome\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(glow1, aes(x = fracture)) + geom_bar()\n```\n\n::: {.cell-output-display}\n![](14_Model_Building_files/figure-revealjs/unnamed-chunk-2-1.png){width=960}\n:::\n\n```{.r .cell-code}\nglow = glow1 %>%\n    dplyr::select(-sub_id, -site_id, -phy_id, -age)\n```\n:::\n\n\n## Splitting data\n\n-   Want to split our data into training and testing sets\n-   Stratify by fracture: because we have imbalanced data\n\n::: {.cell}\n\n```{.r .cell-code}\nglow_split = initial_split(glow, strata = fracture)\nglow_split\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n<Training/Testing/Total>\n<374/126/500>\n```\n:::\n\n```{.r .cell-code}\nglow_train = training(glow_split)\nglow_test = testing(glow_split)\n```\n:::\n\n\n## Fitting the logistic regression model\n\n-   Use Lasso\n\n[Lasso with interactions!!](https://strakaps.github.io/post/glinternet/) Using interactions??\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlasso_mod = logistic_reg(penalty = 0.001, mixture = 1) %>%\n            set_engine(\"glmnet\")\n```\n:::\n\n\n## build recipe??\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglow_rec = recipe(fracture ~ ., data = glow_train) %>%\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk) \n\nglow_workflow = workflow() %>%\n      add_model(lasso_mod) %>% add_recipe(glow_rec)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglow_folds = vfold_cv(glow_train, v = 5, strata = fracture)\n\nglow_fit_rs = glow_workflow %>% \n      fit_resamples(glow_folds, control = control_resamples(save_pred=T))\n\nglow_train_metrics = collect_metrics(glow_fit_rs)\nglow_train_metrics\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  <chr>       <chr>      <dbl> <int>   <dbl> <chr>               \n1 accuracy    binary     0.754     5 0.0206  Preprocessor1_Model1\n2 brier_class binary     0.176     5 0.00902 Preprocessor1_Model1\n3 roc_auc     binary     0.675     5 0.0357  Preprocessor1_Model1\n```\n:::\n\n```{.r .cell-code}\nglow_train_pred = collect_predictions(glow_fit_rs)\n\nglow_train_pred %>%\n    group_by(id) %>%\n    roc_curve(truth = fracture, .pred_No) %>%\n    autoplot()\n```\n\n::: {.cell-output-display}\n![](14_Model_Building_files/figure-revealjs/unnamed-chunk-6-1.png){width=960}\n:::\n\n```{.r .cell-code}\nglow_fit = glow_workflow %>% fit(data = glow_train)\n\nglow_test_pred = predict(glow_fit, new_data = glow_test, type = \"prob\") %>%\n    bind_cols(glow_test)\n\nglow_test_pred %>% \n    roc_curve(truth = fracture, .pred_No) %>%\n    autoplot()\n```\n\n::: {.cell-output-display}\n![](14_Model_Building_files/figure-revealjs/unnamed-chunk-6-2.png){width=960}\n:::\n\n```{.r .cell-code}\nglow_test_pred %>% \n    roc_auc(truth = fracture, .pred_No)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 roc_auc binary         0.652\n```\n:::\n\n```{.r .cell-code}\nglow_test_pred %>% filter(fracture == \"No\", .pred_Yes > 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 7 × 14\n  .pred_No .pred_Yes priorfrac weight height   bmi premeno momfrac armassist\n     <dbl>     <dbl> <fct>      <dbl>  <int> <dbl> <fct>   <fct>   <fct>    \n1    0.493     0.507 Yes         54.4    158  21.8 Yes     No      No       \n2    0.488     0.512 Yes         58.5    155  24.3 No      No      Yes      \n3    0.298     0.702 Yes         54.9    159  21.7 No      Yes     Yes      \n4    0.433     0.567 No          99.8    153  42.6 Yes     No      Yes      \n5    0.499     0.501 No          50.8    150  22.6 No      No      Yes      \n6    0.368     0.632 Yes         55.3    152  23.9 No      No      Yes      \n7    0.429     0.571 No         113.     152  49.1 Yes     No      Yes      \n# ℹ 5 more variables: smoke <fct>, raterisk <fct>, fracscore <int>,\n#   fracture <fct>, age_c <dbl>\n```\n:::\n\n```{.r .cell-code}\nglow_test_pred %>% filter(fracture == \"Yes\", .pred_No > 0.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 27 × 14\n   .pred_No .pred_Yes priorfrac weight height   bmi premeno momfrac armassist\n      <dbl>     <dbl> <fct>      <dbl>  <int> <dbl> <fct>   <fct>   <fct>    \n 1    0.768     0.232 No          49.9    151  21.9 No      No      No       \n 2    0.598     0.402 Yes         88.5    158  35.5 No      No      Yes      \n 3    0.850     0.150 Yes         94.3    173  31.5 No      No      Yes      \n 4    0.551     0.449 No          60.3    148  27.5 Yes     No      Yes      \n 5    0.527     0.473 Yes         78.5    165  28.8 No      Yes     Yes      \n 6    0.822     0.178 Yes         54.4    165  20.0 No      No      No       \n 7    0.715     0.285 No          46.3    158  18.5 No      No      No       \n 8    0.579     0.421 Yes         63.5    165  23.3 Yes     No      Yes      \n 9    0.849     0.151 No          68      173  22.7 No      Yes     No       \n10    0.673     0.327 Yes         56.2    157  22.8 No      No      No       \n# ℹ 17 more rows\n# ℹ 5 more variables: smoke <fct>, raterisk <fct>, fracscore <int>,\n#   fracture <fct>, age_c <dbl>\n```\n:::\n:::\n\n\n\n## Other stufs\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(vip)\n\nvi_data = glow_workflow %>% \n    fit(glow_train) %>%\n    pull_workflow_fit() %>%\n    vi(lambda = 0.001) %>%\n    filter(Importance != 0)\n```\n:::\n\n\n\n\n\n## hfnekl\n\nhttps://github.com/tidyverse/datascience-box/tree/main/course-materials/_slides/u4-d07-prediction-overfitting\n\nhttps://datasciencebox.org/02-making-rigorous-conclusions",
    "supporting": [
      "14_Model_Building_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
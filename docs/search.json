[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BSTA 513/613: Categorical Data Analysis",
    "section": "",
    "text": "BSTA 513/613: Categorical Data Analysis\n\nSpring 2024\n \nWelcome to BSTA 513/613! In this course, we will continue to learn about regression analysis, but not with categorical outcomes. We will build some theoretical understanding in order to interpret and apply logistic regression models appropriately. We will learn how to build a logistic regression model, interpret the model and coefficients, and diagnose potential issues with our model.  \n\n\n\n\n\n \nLink to Student Files OneDrive\nLink to Echo360 page\n\n\nInstructor\n Dr. Nicky Wakim\n Vanport 622A\n wakim@ohsu.edu\n\n\nOffice Hours\nOH with Nicky\n W 3 - 4pm\nOH with Antara\n T 5:30 - 7pm\nOH with Ariel\n Th 3:30 - 5pm\n\n\nCourse details\n Mondays, Wednesdays\n April 1 - June 12\n 1:00 PM - 2:50 PM\n In-person, RPV Room B/C\n\n\nContacting me\nE-mail or Slack is the best way to get in contact with me. I will try to respond to all course-related e-mails within 24 hours Monday-Friday.\n\n\n\n\n\n\n\n\n View the source on GitHub"
  },
  {
    "objectID": "homeworks.html",
    "href": "homeworks.html",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n4/11/24\n\n\nHomework 1\n\n\n6 min\n\n\n\n\n4/25/24\n\n\nHomework 2\n\n\n7 min\n\n\n\n\n5/9/24\n\n\nHomework 3\n\n\n5 min\n\n\n\n\n5/23/24\n\n\nHomework 4\n\n\n6 min\n\n\n\n\n6/6/24\n\n\nHomework 5\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n\n\nHomework 4 Part e help"
  },
  {
    "objectID": "homeworks.html#assignments",
    "href": "homeworks.html#assignments",
    "title": "Homework Assignments and Solutions",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nReading Time\n\n\n\n\n\n\n4/11/24\n\n\nHomework 1\n\n\n6 min\n\n\n\n\n4/25/24\n\n\nHomework 2\n\n\n7 min\n\n\n\n\n5/9/24\n\n\nHomework 3\n\n\n5 min\n\n\n\n\n5/23/24\n\n\nHomework 4\n\n\n6 min\n\n\n\n\n6/6/24\n\n\nHomework 5\n\n\n5 min\n\n\n\n\n\n\nNo matching items\n\n\n\nHomework 4 Part e help"
  },
  {
    "objectID": "homeworks.html#solutions",
    "href": "homeworks.html#solutions",
    "title": "Homework Assignments and Solutions",
    "section": "Solutions",
    "text": "Solutions\nPlease note that you need to download the .html file to see the LaTeX math properly.\n\n\n\nHomework\n.qmd file\n.html file\n\n\n\n\n1\n\n\n\n\n2\n\n\n\n\n3\n\n\n\n\n4\n\n\n\n\n5"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Here is a list of the major resources that have been mentioned in class:\n\nExtra materials for the class\n\nTable for summary of hypothesis tests\nCoefficient interpretations\nLaTeX in .qmd formatting by Ariel\n\n\n\nCourse sites\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nMeike’s 511 Website\n\nReview\n\n\nJessica’s R programming class site\n\nData Management\n\n\n\n\n\n\n\n\n\n\nR coding\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nggplot histograms\n\nReview\n\n\nProbability distributions and their R code\n\nReview\n\n\nVarious hypothesis tests and their R code\n\nReview\n\n\nJessica’s R programming class site\n\nData Management\n\n\n\n\n\n\n\n\n\n\nAcademic Help\n\n\n\nResource\nLink\nClass Mentioned\n\n\n\n\nStudent Academic Support Services\n\nIntro"
  },
  {
    "objectID": "syllabus.html#description",
    "href": "syllabus.html#description",
    "title": "BSTA 513/613 Syllabus",
    "section": "Description",
    "text": "Description\nWelcome to BSTA 513/613! In this course, we will continue to learn about regression analysis, but not with categorical outcomes. We will build some theoretical understanding in order to interpret and apply logistic regression models appropriately. We will learn how to build a logistic regression model, interpret the model and coefficients, and diagnose potential issues with our model.\n\nCourse Learning Objectives\nAt the end of this course, students should be able to…\n\nApply and interpret a variety of hypothesis-testing procedures for two-way and three-way contingency tables\nCompute and interpret measures of association for binary and ordinal data.\nCalculate and correctly interpret odds ratios using logistic regression, make comparison across groups and examine relationship between binary outcome and predictor variables.\nApply appropriate model-building strategies for logistic regression. Effectively use statistical computing packages for contingency table and logistic regression procedures.\nPerform Poisson regression analysis using count data and interpret model estimates, make comparison across groups and examine relationship between outcome and predictor variables.\nCoherently summarize methods and results of data analyses, and discuss in context of original health-related research questions to audiences with varied statistical background."
  },
  {
    "objectID": "syllabus.html#instructors",
    "href": "syllabus.html#instructors",
    "title": "BSTA 513/613 Syllabus",
    "section": "Instructors",
    "text": "Instructors\nHere is the instructor page. This also has office hours!"
  },
  {
    "objectID": "syllabus.html#meeting-times",
    "href": "syllabus.html#meeting-times",
    "title": "BSTA 513/613 Syllabus",
    "section": "Meeting Times",
    "text": "Meeting Times\nMondays          1:00 PM – 2:50 PM PST in RPV Room A/B\nWednesdays    1:00 PM – 2:50 PM PST in RPV Room A/B"
  },
  {
    "objectID": "syllabus.html#materials",
    "href": "syllabus.html#materials",
    "title": "BSTA 513/613 Syllabus",
    "section": "Materials",
    "text": "Materials\n\nTextbooks\n\nApplied Logistic Regression by David Hosmer, Stanley Lemeshow, and Rodney Sturdivant\nAn Introduction to Categorical Data Analysis by Alan Agresti\nIntroduction to Regression Methods for Public Health Using R by Ramzi W. Nahhas\n\nSpecifically Chapter 6\n\n\n\nSupplemental Readings (Optional)\n\nAn Introduction to R (free pdf available)\n\n\n\n\nOnline Resources\n\nSlack\nWe will use Slack as our main form of communication for the class. If you are unsure how to do a homework problem or have other questions, please ask me by posting your question(s) on Slack. Please know that Slack is not guarded by the OSHU firewall, so if you have a question about accommodations or any sensitive topics, you may wish to message me via email. You can still message me regarding sensitive information on Slack, but I will not initiate those conversations on Slack.\n**Please use this invitation link for our Slack workspace!**\nTips on asking questions:\n\nWhen reaching out for help for a homework problem, please include some context on what you have already tried.\n\nFor example, including a photo of your work thus far with an explanation of where you think you might be wrong is a quick way for me to look over your work and help you troubleshoot. This helps me see what parts you already understand and which you need help with. If your question involves code, please include the copied code (not a screenshot) and an attachment to your full file. This way I can see the exact line you need help with and the full code in case the problem starts earlier.\nIf you are unsure how to start a problem, look through your notes and the book for examples that you think might be similar. When reaching out, mention these examples and why you think they might be helpful, but also why you are still unsure on how to proceed.\nIf you only write something similar to “I don’t know how to do problem xxx,” then my response will be to ask you what you tried so far. Thus, it will be quicker for you to let me know this information right away.\n\nIf asking for help about a specific example that you don’t understand, please also provide some detail beyond “I don’t understand example xxx.”\n\nWhich steps of the problem do you not understand? You can refer to a line number, for example.\nIf you don’t understand the first step, do you understand the ones following it?\n\nIn general, when asking a question, please provide the homework number it is from along with the chapter and problem number. If it’s an example from the notes or book, an example number or slide number will help finding it.\nWant more tips on asking questions? This list on this site will help!\nFinally, if I don’t respond within a day and you need a response soon, please remind me by emailing or messaging me again.\n\n\n\nSakai\nWhile most course materials will be delivered online through this website, assignments will be turned in through Sakai, OHSU’s course management system. I will include a link on this website to the Sakai assignment page. \n\n\nWebex\nWebex software will be used for virtual office hours. To give everyone the best possible experience with Webex, I recommend the following best practices:\n\nPlease stay muted until you want to participate\nDuring office hours, please send a message in chat with your question or with a statement like “I have a question.” This makes sure I or the TA can address everyone’s questions in order. \nI encourage you to attend office hours with your video on. This helps me recognize you, and keep mental notes on what techniques/concepts I emphasize to facilitate your specific understanding. \n\n\n\nPoll Everywhere\nWe will use the Poll Everywhere tool as an interactive feature of the course. Poll Everywhere is a web-based application that allows students to participate by responding via text messages or by visiting a web page on an internet-enabled device (smartphone, tablet, laptop). Instructions will be displayed on-screen. The poll that is embedded within the presentation will update in real time. While there is no cost to use this software, standard text messaging rates will apply if you use your phone. Please make sure that you have a Poll Everywhere account before our first class. You are not required to use your OHSU/PSU email to make an account. \nDuring lectures I will pose questions to the class. These questions are designed to provide real-time feedback to both students and the instructor on how well students are grasping the material. This is meant to be an interactive, learning activity with NO contribution to your grade. Your identity will never be connected to your answers, so I encourage you to answer honestly.\n\n\nPennState STAT 504 Website\nPennState has a class offered to online MS students that has some overlap with our class. They have all their course notes posted on this page. This is a great source if you would like to see class notes with different phrasing.\nNot all of our topics are covered in their notes, but the most important ones are. If you are having trouble finding our course’s concepts on their page, please make ask me at Office Hours, after class, or in a private meeting. I do not explicitly state corresponding sections under our schedule because I believe it is important for you to develop skills involving resources and learning key words that can help you find answers. \n\n\nR: Statistical Computing Software\nStudents will use statistical software to complete homework assignments. Students are required to use R/RStudio for this course. R can be freely downloaded. Helpful documentation on installing R is available. I encourage you to install R prior to attending our first lecture. Please email me if you need help installing R or RStudio.\nYou will need to download the following three things:\n\nR https://www.r-project.org/\nRstudio https://posit.co/download/rstudio-desktop/\nQuarto https://quarto.org/docs/get-started/\n\n\nAdditional R Resources\nYour learning and practicing of R will hopefully not be limited to this course. One of the best aspects of programming in R is that many resources are freely available online. Here are just a few additional resources you may explore beyond this class to continue your training in R.\n\n\nUseful online R resources\n\nR for the rest of us\nStatistical tools for high-throughput data analysis. ggplot2 essentials\nR-bloggers\nStack Overflow for troubleshooting\nR Graphical Manual\nQuick-R. Accessing the power of R\nR for SAS, STATA, and SPSS Users\nggplot2\nLearn R 4 free\nJoin a local R user groups\nLearning Machines\n\n\n\n\nOnline R courses to complement or refresh material from class\n\nR for the rest of us\nCoursera: R programming\nedX: R basics\nData Carpentry: For Biologists\nData Carpentry: For Ecologists\nPsychiatric R\nR coder"
  },
  {
    "objectID": "syllabus.html#assessment",
    "href": "syllabus.html#assessment",
    "title": "BSTA 513/613 Syllabus",
    "section": "Assessment",
    "text": "Assessment\nThe course is structured around the following four components:\n\n\n\n\n\n\n\n\n\nComponent\nModality\nFrequency\nDescription\n\n\nLecture\nIn person\nTwice, Weekly\nCourse content is provided through in-person lectures. Lectures will consist of didactic lessons, interactive examples, and PollEverywhere questions. Sessions will be recorded through Explain Everything and posted to Sakai. Attending or viewing the lecture within 7 days of the original lecture date is mandatory. Class attendance will be taken through an Exit Ticket. If viewing the lecture asynchronously, you must take the Exit Ticket to verify your attendance.\n\n\nHomework\nOnline\nWeekly\nThe course includes 7 homework assignments. They are an opportunity for you to engage with important concepts, practice coding, and apply calculating skills. Homework assignments should be submitted online, and will be graded by me. Students are encouraged to work in groups for homework assignments, but each person should do their own summary and hand in their work. Homework assignments will be due on Thursday at 11 PM.\n\n\nQuizzes\nIn person\nEvery 3 weeks\nThe purpose of the quizzes is to assess how well you have achieved the learning objectives through questions covering important concepts, conducting statistical processes, and interpreting output. We will have our quizzes in-class, and it will be open book. Students must work on the quizzes independently.\n\n\nProject (Labs and Report)\nOnline\n4 labs, 1 final report\nThe project will be a combination of submitted labs that will span the quarter and one final report submitted at the end of the quarter. This is meant to translate the tools learned in the course to the work one may do in the workforce. This will help instill the procedure for shaping research goals, model selection, analyzing data, and interpreting meaningful results. Labs will guide you through the needed analysis and background for the project. The final report will summarize your work over the labs and more closely align with a journal article. Students will work independently on each lab.\n\n\n\n\n\n\n\n\n\n\nTypes of assessments\nThis class will use a combination of formative and summative assessments to build and test our knowledge. Below I define each of these types of assessments:\n\nFormative assessment: Activity or work meant to help students learn and practice. Feedback on these assessments are meant to help the instructor and student identify gaps in knowledge and highlight accomplishments.\nSummative assessment: Work meant to test how well students have achieved learning objectives. Grading of these assessments are meant to gauge how well a student grasps the learning objectives and will be able to use their knowledge outside of the classroom."
  },
  {
    "objectID": "syllabus.html#assessment-breakdown",
    "href": "syllabus.html#assessment-breakdown",
    "title": "BSTA 513/613 Syllabus",
    "section": "Assessment Breakdown",
    "text": "Assessment Breakdown\n\nGrading & Requirements\nLetter grades will be assigned roughly according to the following scheme: A (&gt;=93%), A- (90-92%), B+ (88-89%), B(83-87%), B- (82-80%), C+(78-79%), C(73-77%), C- (70-72%), D (60 – 69%), F(&lt;60%).\nGrades will be based on homework assignments, midterm exam, class “attendance”, and final exam, as follows:\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 513)\nPercentage of final grade (BSTA 613)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n33%\n28%\n\n\nQuizzes\nSummative\n4/22, 5/13, 6/3\n25%\n25%\n\n\nProject Labs\nFormative\nEvery 2-3 weeks\n25%\n25%\n\n\nProject Report\nSummative\n6/13\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\nMid-Quarter Feedback\nN/A\n5/2\n2%\n2%\n\n\n613 Readings\nFormative\nApprox. every other week\n0%\n5%\n\n\n\n\n\nHomework grading\nNo student has the same amount of time available to dedicate to homework. This class may not be a priority to you, you may be taking several other courses, or you may need to dedicate time to other activities. Homeworks are formative assessments, meaning its purpose is to help you learn and practice. To reduce the pressure on you to have perfect or complete homework, I have a very simple grading policy: Your homework will be given a check mark if you turn in 50% of the questions parts completed (whether the 50% is correct or wrong). I highly encourage you to stay up-to-date with the homeworks and put in as much effort as you can. This will be the most helpful work in this class!\nIf you turn in the homework on time, the TAs will give you feedback (on one or more complete problems). There is no penalty for turning in the homework late, but you will not get feedback on your work. Please make sure to check the solutions or go to office hours to assess your work.\n\n\nLab grading\nWhile these are formative assignments, it is important to complete the whole lab and put in your best effort. Points will be deducted based on the rubric if it is clear that effort is not made or tasks are skipped.\n\n\nViewing Grades in Sakai\nPoints you receive for graded activities will be posted to the Sakai Gradebook. Click on the Gradebook link on the left navigation to view your points."
  },
  {
    "objectID": "syllabus.html#course-instructor-evaluations",
    "href": "syllabus.html#course-instructor-evaluations",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course & Instructor Evaluations",
    "text": "Course & Instructor Evaluations\n\nOngoing Course Feedback\nThroughout the duration of the course, you are also welcome to informally and anonymously submit your feedback through this Microsoft Form or Class Exit Tickets. This form will be available on Sakai. Students can submit feedback at any time and this form will be reviewed regularly by me. Your responses will be anonymous unless you elect to leave your email address. If I have done anything to make you feel uncomfortable, please give me feedback so I can change my behavior. Ultimately, this class is for you, and my individual social identity/behavior should not inhibit your learning. Thank you for your help making BSTA 513/613 a more successful class! Examples of ongoing feedback are:\n\nNicky talks a little fast during lecture time. May you speak slower?\nDuring Office Hours, Dr. Wakim made a face when I asked a question. This face made me feel self-conscious about my question.\nDr. W asked me a question about my experience that made me feel like a monolith. Please do not assume I can speak on behalf of my social identity groups.\nThe in-class examples do not make me more interested in the material.\n\n\n\nMid-quarter Feedback\nDuring the middle of the quarter, I will ask you to submit guided, anonymous feedback. Completion of feedback will be count towards your grade. To insure anonymity, I will ask you to sign a separate, written statement that you completed the feedback.\n\n\nFinal Course Feedback\nAt the conclusion of the course, you will be asked to complete a formal online review of the course and the instructor. Your feedback on this University evaluation is critical to improving future student learning in this course as well as providing metrics relevant to the instructor’s career advancement (or lack of). Since our class is on the smaller side, everyone’s participation is needed for feedback to be released."
  },
  {
    "objectID": "syllabus.html#schedule",
    "href": "syllabus.html#schedule",
    "title": "BSTA 513/613 Syllabus",
    "section": "Schedule",
    "text": "Schedule\nPlease refer to the Schedule page. I will make changes to this schedule if we need more or less time on a concept. You do not need to read the corresponding chapters in the textbook for each class."
  },
  {
    "objectID": "syllabus.html#how-to-succeed-in-this-course",
    "href": "syllabus.html#how-to-succeed-in-this-course",
    "title": "BSTA 513/613 Syllabus",
    "section": "How to succeed in this course",
    "text": "How to succeed in this course\nEvery professor has different expectations when assigning certain work or providing certain resources. I want to walk through each class resource and assignment so that you know what you can do to succeed in this class. For resources, I want you to optimize the opportunities to learn. For assignments, I want you to know the strategies that students can use to learn the most and prepare for future exams.\n\nResources\n\n\n\n\n\n\n\n\nResource\nWhat is it?\nHow do I use it?\n\n\nOffice Hours\nBlocks of time a professor or TA dedicates for questions. The teaching staff will be located in a specific room. Several students may enter the space at a time and will ask specific or broad questions. If many students attend office hours, a queue will be created so that students can be served equally.\nThe main use of office hours is to ask questions about an assignment or lecture notes. You are welcome to sit and do homework in office hours. OH are also an informal way of meeting fellow students to collaborate with.\n\n\nLectures and lecture recordings\nTime shared between the professor and students where the professor conveys important class material. Material discussed in lectures include concepts, calculations, code, and examples. Lectures are a mix of presentation of information, working through examples together, interactive activities, and in-class polls.\nStudents should attend lectures in person if possible. You should attempt to understand new material presented by following the presentation slides, taking notes on additional details that may conveyed verbally, and working through examples with the professor. Students are encouraged to ask questions when you don’t understand the material at any point in the lecture.\n\n\nTextbooks\nWritten and published material that explains concepts, steps through calculations, provides examples, and provides practice problems. The listed textbooks is the basis for this course. While I am to cover all topics in class, the textbook provides alternative explanations and additional examples.\nWhile coming to class having read the accompanying textbook chapters helps understanding during class, I do not expect students to have read it. I see the textbook as a good resource if you are struggling with a specific topic after class, in need of an example while working on homework, or want additional practice when studying for the exam.\n\n\nWebsite\nThe course website is designed by me so that you have access to all the course materials in a more organized and flexible way. All resources delivered from me to you will be available on the website. Any assignments turned in will be through Sakai.\nYou can navigate through different course resources and information using the left-side tabs or top navigation bar. Course materials, like lecture notes, homework, data examples, and recordings, can be found under each week’s page under the schedule tab. You can also find the individual resources under the “Course Materials” tab on the left. Links to turn in assignments through Sakai will be given on the website. Please explore the tabs and get a sense of the organization.\n\n\nSakai\nSakai is a learning management system for higher ed. This is the university sanctioned LMS where we will submit assignments.\nYou will turn in assignments through Sakai under the “Submissions” tab. Generally, there will be a link to each assignment on the course website. You can also view your grades under “Gradebook” and links to Webex under “Webex.”\n\n\n\n\n\nAssignments\n\n\n\n\n\n\n\n\n\nAssignment\nType of assessment\nBefore you submit/take it\nAfter it is graded\n\n\nHomework\nFormative\n\nWork out each problem on your own as much as you can\nTalk through problems with a peer\nGo to Office Hours for help\nWrite down work that shows your thought process\nSearch your issue on Stack Exchange/Stack Overflow\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\n\n\n\nQuizzes\nSummative\n\nIdentify and achieve learning objectives in each lecture\nUnderstand why certain statistics tools are used for certain cases\nPractice testing yourself and others on concepts\nCome to Office Hours for help with specific problems or concepts\n\n\nReview the solutions\nReview your mistakes\nFor solutions that involve writing sentences, check with me or a TA if your answer fits the solution\nGo to Office Hours to ask about your solutions\nDo not ask for a regrade unless you have viewed the solutions\n\n\n\nProject Labs and Report\nFormative and Summtive\n\nStart the lab as early as possible\nWork on R coding and check with classmates on work\nCome to Office Hours for help with specific R work\nFor the report, compile your work from the labs, and decide what is important in the analysis.\n\n\nThis will be graded at the end of the semester, so you will not have a chance to interact with my feedback as much\nIf you have questions about your grade, you may email me\nKeep the project paper for future reference\nYou can add this project to your resume!\n\n\n\nClass Exit Tickets\nN/A\n\nBring appropriate electronic device to participate in polls\nComplete the survey during the last 5 minutes of class or after class within 7 days\n\n\nReview muddiest and clearest points from the week\n\n\n\n\nIf you would like any other course resources explained in this format, please request it through the Ongoing Course Feedback."
  },
  {
    "objectID": "syllabus.html#course-policies-and-resources",
    "href": "syllabus.html#course-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Policies and Resources",
    "text": "Course Policies and Resources\n\nLate Work Policy\nI encourage you to make your best effort to submit all assignments on time, but I understand circumstances arise that are beyond our control. Please see this Swansea University’s page on extenuating circumstances for some examples. Not all circumstances are covered here, so please reach out if you have questions. \n\nThe class will end on March 22, 2024. All coursework is expected to be completed by then. If you have extenuating circumstances, and need additional time to complete class assignments, please contact me. Together, we will come up with a plan for completion and to sort out registrar logistics.\nIf you have extenuating circumstances that may jeopardize your ability to do work for several weeks, please contact me. We will come up with a plan to keep you on track in the course and prevent any delay in your education.\nFor homework, there is a due date posted, but you may turn in the assignment any time before the class ends. I will give you the check regardless of when you submit the assignment. However, if you would like feedback on the homework, you must turn it in on time OR email me asking for feedback for your late homework.\nFor non-homework assignments, including labs, I ask you to email me directly. You can explain your circumstances and may ask me for an extension, but I won’t necessarily grant one.\nFor labs, you will have ONE no-questions-asked, 3-day extension. Please use this wisely! You just need to send me a quick email saying “I am using my no-questions-asked extension for Lab __.”\nIf you have a emergency involving your self, family, pet, friend, classmate, or anything/one deemed important to you, please do not worry about immediately contacting me. We can work something out after your emergency. If I contact you during an emergency, it is only because I am worried, and you do NOT need to respond until you are able. \n\n\n\nRegrade Policy\nIf you think a question was incorrectly graded, first compare your answer to the answer key. If you believe a re-grade would be appropriate, write an email to me containing the question and a short explanation as to why the question(s) was/were incorrectly graded. Deadline: One week after assignments were returned to class (late requests will not be considered).\n\n\nAttendance Policy\nYou are expected to attend class, participate in-class polls, and complete the exit ticket. For students who miss class or need a review, I will make video and audio recordings of lectures available. There are no guarantees against technical or other challenges for the recording availability or quality. For students who are unable to attend the class in-person and synchronously, viewing the recording within 7 days is acceptable. This is meant to keep you on track within the course and prevent a pile up of material. Make sure to complete the exit ticket to demonstrate attendance.\n\n\nPlagiarism and Attribution\nPlease note that this section has been motivated by Dr. Steven Bedrick’s Course Policies and Grading site for BMI 525. (Note that this is a good example of informal attribution of someone else’s work.)\nIn this class, it is easy to use ChatGPT or other AI tools to solve your homework for you. Many problems follow a basic structure that is especially easy for ChatGPT to solve. In this class, you may use ChatGPT to help with your homework. You may even ask for direct answers. However, there are a few things I do not want you to do:\n\nDo not copy ChatGPT’s answer directly into your homework. Your homework is graded for full credit if you turn it in, in any state, so turning in ChatGPT’s answers is unacceptable. I rather see half-written answers that show what you’re thinking than see a correct answer from ChatGPT.\nDo not stop once ChatGPT answered a question. If it gives an explanation, interact with it! Make sure you understand the thought process of ChatGPT. Try writing out the process to help cement it in your head. Check the answer with what we learn in class.\nDo not use ChatGPT on our quizzes! Hence, you need to really understand how to solve these problems even if you use ChatGPT on the homework.\n\nAt the end of the day, ChatGPT is a resource that will be available to you in a job and outside of school. Thus, we should use it as a tool in school as well! Let me know if ChatGPT helped you understand something! I would love to incorporate it into future classes!\n\n\n\n\n\n\nImportant\n\n\n\nYou can think of this class as assembling a toolbox. When a handyperson starts working for the first time, they need to buy their tools. For their first few jobs, they might need help finding their tools, or remembering which tool is best used for what action. Eventually, they get to know their tools well, and using them appropriately becomes second nature.\nFor now, ChatGPT can help us find and use our tools, but we need to work towards using them as second nature!"
  },
  {
    "objectID": "syllabus.html#course-expectations",
    "href": "syllabus.html#course-expectations",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Expectations",
    "text": "Course Expectations\n\nInstructor Expectations\nCommitment to your learning and your success\nI believe that everyone has the ability to be successful in this course and I have put a lot of effort into designing the course in a way that maximizes your learning to ensure your success. Please talk to me before or after class or stop by my office if there is anything you want to discuss or about which you are unclear. I want to be supportive of your learning and growth.\nInclusive & supportive learning community\nI believe that learning happens best when we all learn together, as a community. This means creating a space characterized by generous listening, civility, humility, patience, and hospitality. I will attempt to promote a safe climate where we examine content from multiple cultural perspectives, and I will strive to create and maintain a classroom atmosphere in which you feel free to both listen to others and express your views and ask questions to increase your learning.\nOpenness to feedback\nI appreciate straightforward feedback from you regarding how well the class is meeting your needs. Let me know if material is not clear or when its relevance to the student learning outcomes for the course is not apparent. In particular, let me know if you identify bias or stereotyping in my teaching materials as I will seek to continuously improve. Please also let me know if there’s an aspect of the class you find particularly interesting, helpful, or enjoyable!\nResponsiveness\nI will monitor email as well as the discussion board daily and try respond to all messages within 24 hours Monday-Friday.\nClear guidelines and prompt feedback on assignments\nI will provide clear instructions for all assignments, and a grading rubric when applicable. I will provide detailed feedback on your submissions and will update grades promptly in Sakai.\n\n\nStudent Expectations and Resources\nAttend class\nYou are expected to attend all scheduled class meetings synchronously or watch the recording within 7 days. Attendance is taken through exit tickets. If you have issues accessing the poll on a specific day, please let me know. \nParticipate\nI encourage you to participate actively in class and online discussions. I will expect all students, and all instructors, to be respectful of each other’s contributions, whether I agree with them or not. Professional interactions are expected.\nBuild rapport\nIf you find that you have any trouble keeping up with assignments or other aspects of the course, make sure you let me know as early as possible. As you will find, building rapport and effective relationships are key to becoming an effective professional. Make sure that you are proactive in informing me when difficulties arise during the quarter so that I can help you find a solution.\nComplete assignments\nAll assignments for this course will be submitted electronically through Sakai unless otherwise instructed.  I encourage you to make your best effort to submit all assignments on time, but I understand that sometimes circumstances arise that are beyond our control. If you need an extension, please contact me in congruence with the Late Policy.\nSeek help if you need it\nI believe it is important to support the physical and emotional well‐being of my students. If you are experiencing physical or mental health issues, I encourage you to use the resources on campus such as those listed below. If you have a health issue that is affecting your performance or participation in the course, and/or if you need help connecting with these resources, please contact me.\n\nStudent Health and Wellness Center (SHW), Website, 503-494-8665 (OHSU Students only)\nStudent Health and Counseling (SHAC), Website, 503-725-2800\n\nInform your instructor of any accommodations needed\nYou should speak with or email me before or during the first week of classes regarding any special needs. Students seeking academic accommodations should register with the appropriate service under the School policies below.\nSome religious holidays may occur on regularly scheduled class days. Because available class hours are so limited in number, we will have to hold class on all such days. Class video recordings will be available and you are encouraged to engage with the material outside of the regular class time. You are also encouraged to come to office hours with questions from the session.\nCommit to integrity\nAs a student in this course (and at PSU or OHSU) you are expected to maintain high degrees of professionalism, commitment to active learning and participation in this class and also integrity in your behavior in and out of the classroom.\nCheating and other forms of academic misconduct will not be tolerated in this course and will be dealt with firmly. Student academic misconduct refers to behavior that includes plagiarism, cheating on assignments, fabrication of data, falsification of records or official documents, intentional misuse of equipment or materials (including library materials), or aiding and abetting the perpetration of such acts. Preparation of exams, assigned on an individual basis, must represent each student’s own individual effort. When used, resource materials should be cited in conventional reference format."
  },
  {
    "objectID": "syllabus.html#course-communications",
    "href": "syllabus.html#course-communications",
    "title": "BSTA 513/613 Syllabus",
    "section": "Course Communications",
    "text": "Course Communications\nSakai/Slack announcements\nFor important/urgent matters, I will communicate with you using announcements via Sakai that will be delivered to your OHSU Email account as well as displayed in the Sakai course site Announcements section. I will copy these announcements in Slack if they do not involve changes to the schedule. Unfortunately, there are certain announcements that OHSU requires I initiate behind the firewall.\nGeneral course questions\nIt is normal to have many questions about things that relate to the course, such as clarification about assignments, course materials, or assessments. Please post these on our Slack Workspace. Please use the channels that I created for questions. You are encouraged to give answers and help each other. I will monitor these threads, so I will endorse or correct responses as needed. Please give me 24 hours to respond to questions within Monday-Friday. Work-life balance is important for me as well, so I will try to respond as quickly as I can within my healthy limits. \nE-mail\nE-mail should be used only for messages that are private in nature. Please send private messages to my OHSU email address (wakim@ohsu.edu). Messages sent through Sakai Inbox will not be answered. Do not send messages asking general information about the class; please post those on Slack instead."
  },
  {
    "objectID": "syllabus.html#further-student-resources",
    "href": "syllabus.html#further-student-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Further Student Resources",
    "text": "Further Student Resources\n\nSPH Writing Lab\nThe School of Public Health Writing Support serves graduate students (master’s and PhD) in SPH, offering help on all professional writing tasks, including class papers, dissertations, job application documents, personal statements, and grant applications, to name a few. Leslie Bienen, MFA, DVM offers one-on-one writing support and other workshops. Appointments are virtual for the time being. You can make an appointment by contacting writingsupportsph@pdx.edu or making an appointment through Calendly.\n\n\nGrammarly Subscription\nThe School of Public Health students have access to a subscription version of Grammarly. While Grammarly cannot improve the argument and flow of your work, it can help with spelling, grammar, and sentence structure. If you are interested in this tool, please add your name to this email form and they will get you added to the subscription. Be sure to use your PSU login credentials to access the form.\n\n\nStudent Wellness\nI am committed to supporting the physical and emotional well-being of my students. Both PSU and OHSU have designated centers for student health. For OHSU, students can visit the Behavioral Health site, where you can find more information including the number to make an appointment. All student visits are free. OHSU students also have access to PSU’s Counseling Services through the school’s Student Health & Counseling. Information on additional student resources for OHSU students are available on the OHSU Health and Wellness Resource page. \n\n\nSupport for Food Insecurity\nStudents across the country experience food insecurity at alarming rates. OHSU and PSU both provide a list of resources to help combat food insecurity. Of note, the Committee to Improve Student Food Security (CISFS) at PSU provides a Free Food Market on the second Monday of each month. OHSU also provides SNAP Enrollment Assistance. The Supplemental Nutrition Assistance Program (SNAP) allocates money towards food for individuals below a certain income level. If you make less than $2,430 monthly, you may wish to enroll.\n\n\nSupport for Students with Children\nStudents who have children can use the PSU resource: Resource Center for Students with Children. Resources are mostly focused on students with younger children. There are several great resources available, including: family-friendly study spaces, new baby starter packs, free kids clothing, and further information on financial resources for childcare."
  },
  {
    "objectID": "syllabus.html#school-policies-and-resources",
    "href": "syllabus.html#school-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "School Policies and Resources",
    "text": "School Policies and Resources\n\nSchool of Public Health Handbook\nAll students are responsible for following the policies and expectations outlined in the student handbook for their program of study. Students are responsible for their own academic work and are expected to have read and practice principles of academic honesty, as presented in the handbook.\n\n\nStudent Access & Accommodations\nThe School of Public Health values diversity and inclusion; we are committed to fostering mutual respect and full participation for all students. My goal is to create a learning environment that is equitable, usable, inclusive, and welcoming. If any aspects of instruction or course design result in barriers to your inclusion or learning, please notify me. \n\nIf you are already registered with disability services at either OHSU or PSU and you are taking a course at the opposite institution, you need to contact the office you’re registered with to transfer your accommodations.\nIf you are not already registered with a disability services office, and you have, or think you may have, a disability that may affect your work in this class, and feel you need accommodations, use the following table for guidance about which office to contact to initiate accommodations.\n\nResource Table\n\n\n\nEnrollment University and Standing\nWhere to Seek Accommodations\n\n\nUndergraduate School of Public Health major\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\n\n\n\nAll PSU-registering Dual Degree (MSW/MPH and MURP/MPH) Graduate School of Public Health Majors and all PSU-registering PhD students admitted prior to fall 2016.\nPSU’s Disability Resource Center\n\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\n\nGraduate School of Public Health major (irrespective of institution at which you register)\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\nNon-SPH major, PSU-enrolled student\nPSU’s Disability Resource Center\n503-725-4150\nSmith Memorial Student Union, Room 116\ndrc@pdx.edu\nwww.pdx.edu/drc\n\n\nNon-SPH major, OHSU-enrolled student\nOHSU’s Office for Student Access\n(503) 494-0082\nStudentAccess@OHSU.edu\nOHSU Auditorium Building 330\n\n\n\n \nFor more information related accessibility and accommodations, please see the “Statement Regarding Students with Disabilities” within the Institutional Policies section of this syllabus.\n\n\nTitle IX\nThe School of Public Health is committed to providing an environment free of all forms of prohibited discrimination and discriminatory harassment. The School of Public Health students who have questions about an incident related to Title IX are welcome to contact either the OHSU or PSU’s Title IX Coordinator and they will direct you to the appropriate resource or office. Title IX pertains to any form of sex/gender discrimination, discriminatory harassment, sexual harassment or sexual violence.\n\nPSU’s Title IX Coordinator is Julie Caron, she may be reached at titleixccordinator@pdx.edu or 503-725-4410. Julie’s office is located at 1600 SW 4th Ave, In the Richard and Maureen Neuberger Center RMNC - Suite 830.\nThe OHSU Title IX Coordinator’s may be reachedat 503-494-0258 or titleix@ohsu.edu and is located at 2525 SW 3rd St.\n\nPlease note that faculty and the Title IX Coordinators will keep the information you disclose private but are not confidential. If you would like to speak with a confidential advocate, who will not disclose the information to a university official without your written consent, you may contact an advocate at PSU or OHSU.\n\nPSU’s confidential advocates are available in Women’s Resource Center (serving all genders) in Smith Student Memorial Union 479. You may schedule an appointment by (503-725-5672) or schedule on line at https://psuwrc.youcanbook.me. For more information about resources at PSU, please see PSU’s Response to Sexual Misconduct website.\nOHSU’s advocates are available through the Confidential Advocacy Program (CAP) at 833-495-CAPS (2277) or by email CAPsupport@ohsu.edu, but please note, email is not a secure form of communication. Also visit www.ohsu.edu/CAP.\n\nAt OHSU, if you encounter any harassment, or discrimination based on race, color, religion, age, national origin or ancestry, veteran or military status, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity or expression, disability or any other protected status, please contact the Affirmative Action and Equal Opportunity (AAEO) Department at 503-494-5148 or aaeo@ohsu.edu.\nAt PSU, you may contact the Office of Equity and Compliance if you experience any form of discrimination or discriminatory harassment as listed above at equityandcompliance@pdx.edu or by calling 503-725-5919.\n\n\nTechnical Support\nThe OHSU ITG Help Desk is available to assist students with email account or network account access issues between 6 a.m. and 6 p.m., Monday through Friday at 503-494-2222. For technical support in using the Sakai Course Management System, please contact the Sakai Help Desk at 877-972-5249 or email us at sakai@ohsu.edu"
  },
  {
    "objectID": "syllabus.html#ohsu-competencies",
    "href": "syllabus.html#ohsu-competencies",
    "title": "BSTA 513/613 Syllabus",
    "section": "OHSU Competencies",
    "text": "OHSU Competencies\n\nList of OHSU Graduation Core Competencies\n\nProfessional Knowledge and Skills\nProfessionalism\nInformation Literacy\nCommunication\nTeamwork\nCommunity Engagement, Social Justice and Equity\nPatient Centered Care\n\nTo access a descriptive list of OHSU Graducation Core Competencies: OHSU Graduation Core Competencies"
  },
  {
    "objectID": "syllabus.html#institutional-policies-and-resources",
    "href": "syllabus.html#institutional-policies-and-resources",
    "title": "BSTA 513/613 Syllabus",
    "section": "Institutional Policies and Resources",
    "text": "Institutional Policies and Resources\n\nStatement Regarding Students with Disabilities\nOHSU is committed to inclusive and accessible learning environments in compliance with federal and state law. If you have a disability or think you may have a disability (mental health, attention-related, learning, vision, hearing, physical or health impacts) contact the Office for Student Access at (503) 494-0082 or OHSU Student Access to have a confidential conversation about academic accommodations. Information is also available at Student Access Website. Because accommodations may take time to implement and cannot be applied retroactively, it is important to have this discussion as soon as possible.\nPortland State students also have similar resources available via the PSU Disability Resource Center (website http://www.pdx.edu/drc ). Please contact the DRC at tel. (503) 725-4150 or email at drc@pdx.edu\n\n\nStudent Evaluation of Courses\nCourse evaluation results are extremely important and used to help improve courses and the learning experience of future students. Responses will always remain anonymous and will only be available to instructors after grades have been posted. The results of scaled questions and comments go to both the instructor and their unit head/supervisor. Refer to Student Evaluation of Courses and Instructional Effectiveness, *Policy No. 02-50-035.\n*To access the OHSU Student Evaluation of Courses and Instructional Effectiveness Policy, you must log into the OHSU O2 website.\n\n\nCopyright Information\nCopyright laws and fair use policies protect the rights of those who have produced the material. The copy in this course has been provided for private study, scholarship, or research. Other uses may require permission from the copyright holder. The user of this work is responsible for adhering to copyright law of the U.S. (Title 17, U.S. Code). To help you familiarize yourself with copyright and fair use policies, the University encourages you to visit its Copyright Web Page\nSakai course web sites contain material protected by copyrights held by the instructor, other individuals or institutions. Such material is used for educational purposes in accord with copyright law and/or with permission given by the owners of the original material. You may download one copy of the materials on any single computer for non-commercial, personal, or educational purposes only, provided that you (1) do not modify it, (2) use it only for the duration of this course, and (3) include both this notice and any copyright notice originally included with the material. Beyond this use, no material from the course web site may be copied, reproduced, re-published, uploaded, posted, transmitted, or distributed in any way without the permission of the original copyright holder. The instructor assumes no responsibility for individuals who improperly use copyrighted material placed on the web site.\n\n\nSyllabi Changes and Retention\nSyllabi are considered to be a learning agreement between students and the faculty of record. Information contained in syllabi, other than the minimum requirements, may be subject to change as deemed appropriate by the faculty of record in concurrence with the academic program and the Office of the Provost. Refer to the *Course Syllabi Policy, 02-50-050.\n*To access the OHSU Course Syllabus Policy, you must log into the OHSU O2 website.\n\n\nCommitment to Diversity & Inclusion\nOHSU is committed to creating and fostering a learning and working environment based on open communication and mutual respect. If you encounter sexual harassment, sexual misconduct, sexual assault, or discrimination based on race, color, religion, age, national origin, veteran’s status, ancestry, sex, marital status, pregnancy or parenting status, sexual orientation, gender identity, disability or any other protected status please contact the Affirmative Action and Equal Opportunity Department at 503-494-5148 or aaeo@ohsu.edu. Inquiries about Title IX compliance or sex/gender discrimination and harassment may be directed to the OHSU Title IX Coordinator at 503-494-0258 or titleix@ohsu.edu.\n\n\nModified Operations, Policy 01-40-010\nPortland Campus:  Marquam Hill and South Waterfront\nStudents should review O2 or call OHSU’s weather alert line at 503-494-9021 for the most up-to-date information on OHSU-wide modified operations which include but are not limited to delays or closures for inclement weather.\nIf your home institution is not on the Portland campus (Marquam Hill or South Waterfront, contact your home institution for more information.\n\n\nOHSU Resources Available to Students*:\nRemote Learning Resources\nThe Remote Learning webpage on O2 contains concise, practical resources, and strategies for students that need to quickly transition to a fully remote instructional format.\nRegistrar’s Office\nMackenzie Hall, Rm. 1120\n503-494-7800; Email the Registrar\nStudent Registration Information: \nTo Register for Classes\nOHSU ITG Help Desk\nRegular staff hours are 6 a.m. to 6 p.m., Monday through Friday, but phones are answered seven days a week, 24 hours a day. Call 503 494-2222.\nTeaching and Learning Center\nAcademic Support Counseling and Sakai Course Management System, please contact the TLC Help Desk at 877-972-5249 or email TLC Help Desk\nStudent Academic Support Services\nFor resources on improving student’s study strategies, time management, motivation, test-taking skills and more, Please access the Student Academic Support Services Sakai page. For one-on-one appointments or to arrange a workshop for students, please contact Emily Hillhouse.\nConfidential Advocacy Program\nSupport for OHSU employees, students, and volunteers who have experienced any form of sexual misconduct, including sexual harassment, sexual assault, intimate-partner violence, stalking, relationship/dating violence, and other forms — regardless of when or where it took place. Contact Us.\nConcourse Syllabus Management\nFor help with accessing your Concourse Syllabus:  Please contact the Sakai help Desk for all other Concourse inquiries please visit the Concourse Support - Sakai or please contact the Mark Rivera at rivermar@ohsu.edu or call 503-494-0934\nPublic Safety\nOHSU Public Safety-Portland Campus (Marquam Hill and South Waterfront)\n\nEmergency on Campus: 503-494-4444 (Portland)\nNon-emergency: 503-494-7744; Contact Public Safety\n\nStudent Health & Wellness Center \nBaird Hall, Rm. 18 (Primary Care) and Rm. 6 (Behavioral Health)\n503-494-8665; For urgent care after hours, 503-494-8311 and ask for the Nurse on call.\nWellness Center Information  \nWellness Center Website\nIf your home institution is not on the Portland campus, contact your home institution student support services for more information.\nOmbudsman Office\nGaines Hall, Rm. 117\n707 SW Gaines Street, Portland, OR 97239\n503-494-5397; Contact Ombudsman; Ombudsman Website\nLibrary: Biomedical Information Communication Center\nBICC Library Hours of Operation\n\n\nPrivacy While Learning\nStudents may be asked to take classes remotely through videoconferencing software like WebEx. Some of these remote classes will be recorded. Any recording will capture the presenter’s audio, video, and computer screen. Student video and audio will be recorded if and when you unmute your audio and share your video during the recorded sessions. These recordings will not be shared with or accessible to the public without prior written consent. \n\n\nStudent Central\nKey information for students across OHSU’s Schools of Dentistry, Medicine, Nursing, the OHSU-PSU School of Public Health and the College of Pharmacy. Student Central helps you find out more about student services, resources, policies and technology."
  },
  {
    "objectID": "extra_resources/Coefficient_interp.html",
    "href": "extra_resources/Coefficient_interp.html",
    "title": "Coefficient interpretations",
    "section": "",
    "text": "Keep in mind that words expected, average, and mean are interchangeable.\nThe following are required parts of the interpretation\n\nUnits of Y\nUnits of X\nDiscussing intercept: Mean or average or expected before Y\nDiscussing coefficient for continuous covariate: Mean or average or expected before difference, increase, or decrease\n\nOR: Mean or average or expected before Y\nOnly need before difference or Y!!\n\nConfidence interval\nIf other covariates in the model\n\nDiscussing intercept: Must state that variables are equal to 0\n\nor at their centered value if centered!\n\nDiscussing coefficient for covariate: Must state “adjusting for all other variables”, “Controlling for all other variables”, or “Holding all other variables constant”\n\nIf only one other variable in the model, then replace “all other variables” with the single variable name\n\n\n\nWhen the estimate of the population coefficient is…\n\nPositive: using the word “increase” in the place of “difference” helps with understanding the relationship\nNegative: using the word “decrease” in the place of “difference” helps with understanding the relationship\n\n\n\n\n\n\n\n\n\n\nPopulation Model\nVariable information\nCoefficient estimate interpretations\n\n\n\n\n\\(Y=\\beta_0+\\beta_1X_1+\\epsilon\\)\n\\(X_1\\): continuous covariate\n\n\\(\\widehat\\beta_0\\): Mean \\(Y\\) when \\(X_1\\) is 0\n\nExample: For someone who is 0 years old, the expected peak exercise heart rate is 214.233 beats per minute (95% CI: 204.918, 223.548)\n\n\\(\\widehat\\beta_1\\): Mean difference in \\(Y\\) per 1 unit increase in \\(X_1\\)\n\nExample: For every one year increase in age, the expected peak exercise heart rate decreases 0.834 bpm (95% CI: ….)\n\n\n\n\n\\(Y=\\beta_0+\\beta_1X^c_1+\\epsilon\\)\n\\(X^c_1\\): continuous covariate that is centered around its mean or median\n\n\\(\\widehat\\beta_0\\): Mean \\(Y\\) when \\(X^c_1\\) is at its mean or median\n\\(\\widehat\\beta_1\\): Mean difference in \\(Y\\) per 1 unit increase in \\(X^c_1\\)\n\n\n\n\\(Y=\\beta_0+\\beta_1X_1+\\beta_2X_2+\\epsilon\\)\n\\(X_1\\): continuous covariate\n\\(X_2\\): continuous covariate"
  },
  {
    "objectID": "homework/HW5.html",
    "href": "homework/HW5.html",
    "title": "Homework 5",
    "section": "",
    "text": "Important\n\n\n\nThis assignment is NO LONGER under construction !!! (3/1/2024)"
  },
  {
    "objectID": "homework/HW5.html#directions",
    "href": "homework/HW5.html#directions",
    "title": "Homework 5",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW5.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 513/613\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW3.html",
    "href": "homework/HW3.html",
    "title": "Homework 3",
    "section": "",
    "text": "Download the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW3.html#directions",
    "href": "homework/HW3.html#directions",
    "title": "Homework 3",
    "section": "",
    "text": "Download the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW3.html#questions",
    "href": "homework/HW3.html#questions",
    "title": "Homework 3",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis question is taken from the Hosmer and Lemeshow textbook. The ICU study data set consists of a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult intensive care unit (ICU). The dataset should be available within Course Materials. The major goal of this study was to develop a logistic regression model to predict the probability of survival to hospital discharge of these patients. In this question, the primary outcome variable is vital status at hospital discharge, STA. Clinicians associated with the study felt that a key determinant of survival was the patient’s age at admission, AGE. We will be building to a multivariable logistic regression model while adjusting for cancer part of the present problem (CAN), CPR prior to ICU admission (CPR), infection probable at ICU admission (INF), and level of consciousness at ICU admission (LOC).\nA code sheet for the variables to be considered is displayed in Table 1.5 below (from the Hosmer and Lemeshow textbook, pg. 23). We refer to this data set as the ICU data.\n\n\nPart a\nFrom the above list (AGE, CAN, CPR, INF, and LOC) of independent variables, identify if each is a continuous, binary, or multi-level (&gt;2) categorical variable.\n\n\nPart b\nFor the binary and multi-level categorical variables, please identify a reference group for each. Include justification for the reference group.\n\n\nPart c\nRefer back to Part c from Homework 2’s Question 4. Interpret the odds ratio for age in the simple logistic regression model. Please include the 95% confidence interval.\n\n\nPart d\nCompute the predicted probability of hospital discharge for a subject who is 63 years old. Compute the 95% confidence interval for the predicted probability and interpret the predicted probability.\n\n\nPart e\nFor the categorical variables (binary and multi-group), please mutate the variables within the ICU dataset to set your chosen reference groups.\n\n\nPart f\nWrite down the equation for the logistic regression model of STA on CPR.\n\n\nPart g\nUsing the glm() function, obtain the maximum likelihood estimates of the coefficient parameters of the logistic regression model in Part f. Using these estimates, write down fitted logistic regression model.\n\n\nPart h\nWrite a sentence interpreting the odds ratio for the coefficients in Part g’s model. Please include the 95% confidence interval.\n\n\nPart i\nWrite down the equation for the logistic regression model of STA on LOC.\n\n\nPart j\nUsing the glm() function, obtain the maximum likelihood estimates of the coefficient parameters of the logistic regression model in Part i. Present the coefficient estimates. No need to write out the fitted regression equation.\nPlease take note of the warnings that you receive from fitting the glm() model and any large coefficient estimate with large confidence intervals. In this case, we have a category within LOC that has very few observations. (We will discuss this more in Lesson 14: Numerical Problems)\nCheck the number of observations that have a deep stupor and death at discharge and the number of observations that have a deep stupor and live at discharge. You can do this using the table() function to create a contingency table.\n\n\nPart k\nWrite a sentence interpreting the odds ratio of death for the indicator of coma. Please include the 95% confidence interval.\n\n\n\nQuestion 2\nDid you take the mid-quarter survey??"
  },
  {
    "objectID": "homework/HW1.html",
    "href": "homework/HW1.html",
    "title": "Homework 1",
    "section": "",
    "text": "Caution\n\n\n\nI took out likelihood ratio tests in Question 5. We did not cover them!"
  },
  {
    "objectID": "homework/HW1.html#directions",
    "href": "homework/HW1.html#directions",
    "title": "Homework 1",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will not need to download datasets for this homework.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW1.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: “BSTA 513/613”: author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your .qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW1.html#questions",
    "href": "homework/HW1.html#questions",
    "title": "Homework 1",
    "section": "Questions",
    "text": "Questions"
  },
  {
    "objectID": "homework/HW2.html",
    "href": "homework/HW2.html",
    "title": "Homework 2",
    "section": "",
    "text": "This homework is designed to help you practice the following important skills and knowledge that we covered in Lessons 3-6:\n\nCalculate and interpret the estimated risk difference, relative risk, and odds ratios, and their confidence intervals\nExpand work on contingency tables to evaluate the agreement or reproducibility using Cohen’s Kappa\nUnderstand important differences between linear regression and logistic regression\nConstruct a simple logistic regression model\nTest a covariate for significance using the Wald test and LRT"
  },
  {
    "objectID": "homework/HW2.html#directions",
    "href": "homework/HW2.html#directions",
    "title": "Homework 2",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW1.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: “BSTA 513/613”: author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nWrite all answers in complete sentences as if communicating the results to a collaborator.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW4.html",
    "href": "homework/HW4.html",
    "title": "Homework 4",
    "section": "",
    "text": "Download the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 513/613\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW4.html#directions",
    "href": "homework/HW4.html#directions",
    "title": "Homework 4",
    "section": "",
    "text": "Download the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 513/613\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "homework/HW4.html#questions",
    "href": "homework/HW4.html#questions",
    "title": "Homework 4",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nThis question is taken from the Hosmer and Lemeshow textbook. The ICU study data set consists of a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult intensive care unit (ICU). The dataset should be available within Course Materials. The major goal of this study was to develop a logistic regression model to predict the probability of survival to hospital discharge of these patients. In this question, the primary outcome variable is vital status at hospital discharge, STA. Clinicians associated with the study felt that a key determinant of survival was the patient’s age at admission, AGE. We will be building to a multivariable logistic regression model while adjusting for cancer part of the present problem (CAN), CPR prior to ICU admission (CPR), infection probable at ICU admission (INF), and level of consciousness at ICU admission (LOC).\nA code sheet for the variables to be considered is displayed in Table 1.5 below (from the Hosmer and Lemeshow textbook, pg. 23). We refer to this data set as the ICU data.\nYou will need to use some of the transformations of variables from Homework 3, Question 1, Part e.\n\n\nPart a\nWrite down the equation for the logistic regression model of STA on AGE, CAN, CPR, and INF. How many parameters does this model contain?\n\n\nPart b\nUsing glm(), obtain the maximum likelihood estimates of the parameters of the logistic regression model in Part a. Using these estimates, write down the equation with the fitted values.\n\n\nPart c\nAssess the significance of the group of coefficients for all variables in the model using the likelihood ratio test. (Hint: part of the ratio in the LRT will be an intercept only model)\n\n\nPart d\nFit a new model using only CAN and INF as the predictors, including an interaction between CAN and INF. Write a sentence interpreting the odds ratio for the main effects in the model. Please include the 95% confidence interval.\n\n\nPart e\nFrom the above model, fill out the following table for the odds ratios. Note, you will only need to report two odds ratios and you already have one from Part d.\n\n\n\n\n\n\n\n\nCancer\nInfection\nOdds ratio\n\n\n\n\nCancer part of present problem\nInfection probable at ICU intake\n\n\n\n\n      No\n\n\n\n\n      Yes\nFILL HERE\n\n\nCancer not part of present problem\nInfection probable at ICU intake\n\n\n\n\n      No\n\n\n\n\n      Yes\nFILL HERE\n\n\n\nThis is a really good way to report odds ratios for interactions between two categorical predictors! Might want to keep this in mind for your project!!\n\n\nPart f\nCompute the predicted probability for a subject who does not have a present issue with cancer nor an infection upon admittance to the ICU. Compute the 95% confidence interval for the predicted probability. Can you use the Normal approximation? Interpret the predicted probability including the confidence interval."
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html",
    "href": "extra_resources/Latex_qmd_formatting.html",
    "title": "LateX and R Markdown Formatting",
    "section": "",
    "text": "This style of coding has a bunch of different yet completely equivalent names. It is a form of coding within markdown files that helps us write equations in an easily readable format.\n\nLatex\nLateX\nLaTeX\nMay occasionally see it written with a K instead of an X\nPronounced as “lay-tek” or “lah-tek”"
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html#centered",
    "href": "extra_resources/Latex_qmd_formatting.html#centered",
    "title": "LateX and R Markdown Formatting",
    "section": "3.1 Centered",
    "text": "3.1 Centered\nIf you want your equation to be centered in your output, use the double dollar sign ($$) on either side of your equation: $$\\alpha = 0.05$$\n\\[ \\alpha = 0.05 \\]\nTip: If you put your $$   $$ out first, whatever you type in between them will render as you’re typing! Sometimes this is helpful to catch if you’ve made a typo or misspelled a Greek letter or command you meant to use (more on these later)."
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html#in-line",
    "href": "extra_resources/Latex_qmd_formatting.html#in-line",
    "title": "LateX and R Markdown Formatting",
    "section": "3.2 In-Line",
    "text": "3.2 In-Line\nIf you want your equation to be in-line with your markdown text, use the single dollar sign ($) on either side of your equation. Using this, you can write things like $\\alpha$ to output (\\(\\alpha\\) = 0.05)."
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html#subscript",
    "href": "extra_resources/Latex_qmd_formatting.html#subscript",
    "title": "LateX and R Markdown Formatting",
    "section": "5.1 Subscript",
    "text": "5.1 Subscript\nOnly the first value (letter or number) following the subscript command will be subscripted, by default. If you want to make a value subscripted, use the underscore (_) like this:\nH_0: \\(H_0\\)\nWhatever you type next will not be subscripted:\nH_12: \\(H_12\\)\nTo subscript more than one value, use curly brackets:\nH_{12}: \\(H_{12}\\)"
  },
  {
    "objectID": "extra_resources/Latex_qmd_formatting.html#superscript",
    "href": "extra_resources/Latex_qmd_formatting.html#superscript",
    "title": "LateX and R Markdown Formatting",
    "section": "5.2 Superscript",
    "text": "5.2 Superscript\nSometimes we want values to appear above others as a superscript, such as when we’re squaring or cubing them. As with the subscript, if you want more than one value following the carrot to be superscripted, use curly brackets:\nr^2: \\(r^2\\)\nr^12: \\(r^12\\)\nr^{12}: \\(r^{12}\\)"
  },
  {
    "objectID": "project.html#labs",
    "href": "project.html#labs",
    "title": "Project Central",
    "section": "Labs",
    "text": "Labs\n\n\n\nLab\nDue Date\nTopics\n\n\n\n\nLab 1\n4/18\nExploring the question and data\n\n\nLab 2\n5/2\nEDA continued + Simple logistic regression\n\n\nLab 3\nOptional missing data code if you want to try it\n5/16\nMultiple logistic regression + optional missing data\n\n\nLab 4\n5/30\nBuilding and interpreting final model"
  },
  {
    "objectID": "project.html#report",
    "href": "project.html#report",
    "title": "Project Central",
    "section": "Report",
    "text": "Report\nReport Instructions\nDue 6/13/2024"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "ScheduleImportant timesCredit to\n\n\nLabs and homeworks may be switched around depending on the material learned in class.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApril\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n\n\nWeek 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n11\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n\n\n\nWeek 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n15\n\n\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\n17\n\n\n\n\n\n\n\n\n18\n\n\n\n\n\n\n\n\n19\n\n\n\n\n\n\n\n\n\n\nWeek 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 1 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n22\n\n\n\n\n\n\n\n\n23\n\n\n\n\n\n\n\n\n24\n\n\n\n\n\n\n\n\n25\n\n\n\n\n\n\n\n\n26\n\n\n\n\n\n\n\n\n\n\nWeek 4Quiz 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n29\n\n\n\n\n\n\n\n\n30\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMay\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n\n29\n\n\n\n\n\n\n\n\n30\n\n\n\n\n\n\n\n\n1\n\n\n\n\n\n\n\n\n2\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n\n\nWeek 5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLab 2 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\n7\n\n\n\n\n\n\n\n\n8\n\n\n\n\n\n\n\n\n9\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n\n\nWeek 6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 3 dueMidterm feedback due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n13\n\n\n\n\n\n\n\n\n14\n\n\n\n\n\n\n\n\n15\n\n\n\n\n\n\n\n\n16\n\n\n\n\n\n\n\n\n17\n\n\n\n\n\n\n\n\n\n\nWeek 7\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuiz 2 Opens at 2pm\n\n\n\n\n\n\n\n\nLab 3 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n20\n\n\n\n\n\n\n\n\n21\n\n\n\n\n\n\n\n\n22\n\n\n\n\n\n\n\n\n23\n\n\n\n\n\n\n\n\n24\n\n\n\n\n\n\n\n\n\n\nWeek 8Quiz 2 Closes at 1pm\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n27\n\n\n\n\n\n\n\n\n28\n\n\n\n\n\n\n\n\n29\n\n\n\n\n\n\n\n\n30\n\n\n\n\n\n\n\n\n31\n\n\n\n\n\n\n\n\n\n\nNo Class: Mem Day\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeek 9\n\n\n\n\n\n\n\n\nLab 4 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJune\n\n\n\n\n\nMON\n\n\nTUE\n\n\nWED\n\n\nTHU\n\n\nFRI\n\n\n\n\n\n\n\n\n\n\n3\n\n\n\n\n\n\n\n\n4\n\n\n\n\n\n\n\n\n5\n\n\n\n\n\n\n\n\n6\n\n\n\n\n\n\n\n\n7\n\n\n\n\n\n\n\n\n\n\nWeek 10Quiz 3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 5 due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\n\n\n\n\n\n\n\n\n11\n\n\n\n\n\n\n\n\n12\n\n\n\n\n\n\n\n\n13\n\n\n\n\n\n\n\n\n14\n\n\n\n\n\n\n\n\n\n\nWeek 11\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinal Project Due\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere is a tentative list of important, recurring times:\n\nHomeworks are always due at 11pm on the specified day (usually Thursday)\nLabs are always due at 11pm on the specified day (usually Thursday)\nExit tickets are due at 11pm 7 days after class\n\nSo for Monday classes, they are due the following Monday at 11pm\nFor Wednesday classes, they are due the following Wednesday at 11pm\n\nOffice hours with Nicky: Link to Webex\n\nTBD\n\nClass meets on Mondays and Wednesdays at 1 - 2:50 pm\n\n\n\nSpecial thank you to Andrew Bray, who taught the Quarto workshop I attended. This schedule page was mostly taken from the Schedule on his STAT 20 course page. You can find the .qmd file for Andrew’s schedule page on his Github."
  },
  {
    "objectID": "homework/HW1.html#purpose",
    "href": "homework/HW1.html#purpose",
    "title": "Homework 1",
    "section": "Purpose",
    "text": "Purpose\nThis homework is designed to help you practice the following important skills and knowledge that we covered in Lessons 1-2:\n\nPracticing and outlining your decision process to analyze the relationship between two categorical variables\nInterpreting research aims into questions/tests that can be answered with statistics\nUsing R to calculate sample proportions\nUsing R to calculate test statistic values for inference tests\nInterpreting new phrasing of questions that were introduced in class"
  },
  {
    "objectID": "homework/HW1.html#question-1",
    "href": "homework/HW1.html#question-1",
    "title": "Homework 1",
    "section": "Question 1",
    "text": "Question 1\nIf the probability that one white blood cell is a lymphocyte is 0.3, compute the probability of 2 lymphocytes out of 10 white blood cells. Also, compute the probability that at least 3 lymphocytes out of 10 white blood cells. You may calculate by hand, using a web app, or using R."
  },
  {
    "objectID": "homework/HW1.html#question-2",
    "href": "homework/HW1.html#question-2",
    "title": "Homework 1",
    "section": "Question 2",
    "text": "Question 2\nConsider a 2 x 2 table from a prospective cohort study:\n\n\n\n\n \n  \n      \n    Favorable \n    Unfavorable \n  \n \n\n  \n    Treatment \n    30 \n    20 \n  \n  \n    Placebo \n    10 \n    60 \n  \n\n\n\n\n\n\nPart a\nEstimate the probability of having favorable results for subjects in the treatment group. Report with the 95% confidence interval.\n\n\nPart b\nRepeat part a for the placebo group.\n\n\nPart c\nConduct a statistical test to evaluate whether there is an association between group and outcome. What is the name of the test? Write down the null and alternative hypotheses. Compute the test statistic (by hand or by a software). What distribution does the test statistic follow under the null hypothesis? Give the p-value and interpret your result."
  },
  {
    "objectID": "homework/HW1.html#question-3",
    "href": "homework/HW1.html#question-3",
    "title": "Homework 1",
    "section": "Question 3",
    "text": "Question 3\nConsider a cohort study with results shown as in following table:\n\n\n\n\n \n  \n      \n    Favorable \n    Unfavorable \n  \n \n\n  \n    Treatment \n    6 \n    1 \n  \n  \n    Placebo \n    2 \n    5 \n  \n\n\n\n\n\nConduct a statistical test to evaluate whether there is an association between group and outcome. Write down the null and alternative hypotheses. Compute the expected cell counts under null hypothesis. What is the name of the test? Give the p-value and interpret your result."
  },
  {
    "objectID": "homework/HW1.html#question-4",
    "href": "homework/HW1.html#question-4",
    "title": "Homework 1",
    "section": "Question 4",
    "text": "Question 4\nTable 4 shows the information of a selected group of adolescents on whether they use smokeless tobacco and their perception of risk for using smokeless tobacco.\nTable 4:\n\n\n\n\n \n  \n      \n    YES \n    NO \n  \n \n\n  \n    Minimal \n    25 \n    60 \n  \n  \n    Moderate \n    35 \n    172 \n  \n  \n    Substantial \n    10 \n    200 \n  \n\n\n\n\n\n\nPart a\nConduct a statistical test to examine general association between adolescent smokeless tobacco users and risk perception. What is the name of the test? Write down the null and alternative hypotheses. Compute the test statistic (use software). What distribution does the test statistic follow under the null hypothesis? Give the p-value and interpret your result.\n\n\nPart b\nIs there a trend of increased risk perception for smokeless tobacco users? What test would you use? State the test, state any assumptions, conduct the inference test, and state your conclusions."
  },
  {
    "objectID": "homework/HW1.html#question-5",
    "href": "homework/HW1.html#question-5",
    "title": "Homework 1",
    "section": "Question 5",
    "text": "Question 5\nStart making a comprehensive table or outline for the inference tests that we have covered. Here is a list of the tests we have covered:\n\nSingle proportion\nChi-squared test for general association\nLikelihood ratio test for general association\nFisher’s Exact test for general association\nCochran-Armitage test for trend\nMantel-Haenszel test for linear trend\n\nAnd here is a list of attributes to include:\n\nNumber of variables testing\nTypes of variables\nCriteria (if any)\nHypothesis test\nTest statistic (if we went over it)\nR code for test\nSample size / Power calculation (optional, not discussed in class)\nSpecial notes (optional)\n\nFor example, I could make a table with different rows corresponding to different tests and different columns for each attribute."
  },
  {
    "objectID": "homework/HW1.html#question-6",
    "href": "homework/HW1.html#question-6",
    "title": "Homework 1",
    "section": "Question 6",
    "text": "Question 6\nI want you to gain experience exploring a package and function. This is an important skill in coding that can help you grow as an applied statistician.\nIn your previous course, the function lm() was introduced to perform linear regression. In this class, we will heavily use the function glm(). By typing “?glm” in the R console, we can open the Help page for glm(). The following questions ask about the glm() function. You can Google or use R documentation to answer the questions.\nFeel free to read more about the differences between lm() and glm().\n\nPart a\nWhat does the input “family” mean? If I wanted to perform regression using a Poisson distribution, what would I input into family?\n\n\nPart b\nWhat is the default action for the “na.action” input?\n\n\nPart c\nHow does the glm() function fit our model? (Hint: see “method”)\n\n\nPart d\nDo you think the output of summary() will be the same for lm() and glm()?"
  },
  {
    "objectID": "homework/HW1.html#question-7",
    "href": "homework/HW1.html#question-7",
    "title": "Homework 1",
    "section": "Question 7",
    "text": "Question 7\nOPTIONAL\nLet’s make a decision tree on the different tests we learned! I would like you to make a flow chart for the different tests we learned in Classes 1 and 2. You’ll need to include characteristics for:\n\nNumber of variables (1, 2, or 3 - we will go over 3 variables in Class 4)\nNumber of categories in each variable\nSample size is small\nOrdinal/nominal independent variable\nOrdinal/nominal response variable(s)\n\nFor example, if I make a decision tree that includes end nodes for different animals (cat, dog, snake, turtle, and hawk) using yes/no characteristics (has a shell, woof/meows, has fur, or flies), then my flow chart would look like: See my example under Sakai Resources. You are welcome to draw this chart. I used SmartArt under the Insert tab in Word to create mine."
  },
  {
    "objectID": "homework/HW2.html#questions",
    "href": "homework/HW2.html#questions",
    "title": "Homework 2",
    "section": "Questions",
    "text": "Questions\n\nQuestion 1\nA study looked at the effects of oral contraceptive (OC) use on heart disease in women 40 to 44 years of age. The researchers prospectively tracked whether or not the women developed a myocardial infarction (MI) over a 3-year period. The table below summarizes their results with columns indicating whether or not women developed MI and rows indicating their OC use.\n\n\n\n\n \n  \n      \n    Yes \n    No \n  \n \n\n  \n    OC users \n    13 \n    4987 \n  \n  \n    Non-OC users \n    7 \n    9993 \n  \n\n\n\n\n\n\nPart a\nCompute the estimated risk difference comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart b\nCompute the estimated relative risk comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart c\nCompute the estimated odds ratio comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart d\nIs the OR a good approximation of the RR? Explain why or why not.\n\n\n\nQuestion 2\nOne important aspect of medical diagnosis is its reproducibility. Suppose that two different doctors examine 100 patients for dyspnea in a respiratory-disease clinic, and that doctor A diagnosed 15 patients as having dyspnea (while doctor B did not), doctor B diagnosed 10 patients as having dyspnea (while doctor A did not), and both doctor A and doctor B diagnosed 7 patients as having dyspnea.\n\nPart a\nConstruct a two-way contingency table to summarize the dyspnea diagnoses from doctor A and B.\n\n\nPart b\nCompute the Cohen’s kappa, 95% confidence interval, and interpret the results. What level of agreement does your kappa indicate?\n\n\n\nQuestion 3\nThis question is meant to emphasize the differences between linear regression and logistic regression. Each part will ask about different aspects of the two regression models. If the question has multiple choice answers, then you must write 1-2 sentences justifying your answer.\n\nPart a\nIn linear regression, what type of variable is our response/outcome variable?\n\nbinary\ncontinuous\ncount\nordinal\n\n\n\nPart b\nIn logistic regression, what type of variable is our response/outcome variable?\n\nbinary\ncontinuous\ncount\nnormal\n\n\n\nPart c\nWhat assumptions in linear regression? Please state the assumption name and characteristics of that assumption.\n\n\nPart d\nWhich assumptions of linear regression are violated if we try to fit a binary response using linear regression? You may choose more than one answer.\n\nIndependence\nLinearity\nNormality\nHomoscedasticity\n\n\n\nPart e\nPlease use our notes on generalized linear models (GLMs) to answer this question. What is the random component used in linear regression? What is the random component used in logistic regression? Name the specific variable type and distribution for it.\n\n\nPart f\nPlease use our notes on generalized linear models (GLMs) to answer this question. What link function do we use in linear regression? What link function do we use in logistic regression? Name the link and write out the function.\n\n\nPart g\nPlease use our notes on generalized linear models (GLMs) to answer this question. What is the systematic component used in simple linear regression? What is the systematic component used in simple logistic regression? Please write out the function for the systematic component for a single covariate.\n\n\nPart h\nHow do we determine our coefficient estimates (estimates of the parameter values) in linear regression? You may choose more than one answer.\n\nOrdinary least squares\nMaximum likelihood estimation\n\n\n\nPart i\nHow do we determine our coefficient estimates (estimates of the parameter values) in logistic regression? You may choose more than one answer.\n\nOrdinary least squares\nMaximum likelihood estimation\n\n\n\n\nQuestion 4\nThis question is taken from the Hosmer and Lemeshow textbook. The ICU study data set consists of a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult intensive care unit (ICU). The dataset should be available within Course Materials. The major goal of this study was to develop a logistic regression model to predict the probability of survival to hospital discharge of these patients. In this question, the primary outcome variable is vital status at hospital discharge, STA. Clinicians associated with the study felt that a key determinant of survival was the patient’s age at admission, AGE.\nA code sheet for the variables to be considered is displayed in Table 1.5 below (from the Hosmer and Lemeshow textbook, pg. 23). We refer to this data set as the ICU data.\n\n\nPart a\nWrite down the equation for the logistic regression model of STA on AGE. What characteristic of the outcome variable, STA, leads us to consider the logistic regression model as opposed to the usual linear regression model to describe the relationship between STA and AGE?\n\n\nPart b\nWrite down an expression for the log-likelihood for the logistic regression model in Part a. This will me a mathematical expression. Please do not use generic expressions like \\(\\pi(X)\\), instead replace \\(X\\) with the specific variables in this question.\n\n\nPart c\nUsing the glm() function, obtain the maximum likelihood estimates of the coefficient parameters of the logistic regression model in Part a. Using these estimates, write down fitted logistic regression model.\n\n\nPart d\nUse the Wald test to test whether or not the intercept (\\(\\beta_0\\)) of the logistic regression model is significantly different from 0. Make sure to include your: hypothesis test, code/work leading to the computed test statistic, output including the test statistic and p-value, and conclusion. Please refer to the Additional Tips to guide you on what a complete/correct answer contains.\n\n\nPart e\nUse the Likelihood Ratio test to test whether or not the coefficient for age (\\(\\beta_1\\)) of the logistic regression model is significantly different from 0. Make sure to include your: hypothesis test, code/work leading to the computed test statistic, output including the test statistic and p-value, and conclusion. Please refer to the Additional Tips to guide you on what a complete/correct answer contains. You do not need to include an interpretation of the coefficient since we have not covered this."
  },
  {
    "objectID": "homework/HW2.html#question-1",
    "href": "homework/HW2.html#question-1",
    "title": "Homework 2",
    "section": "Question 1",
    "text": "Question 1\nA study looked at the effects of oral contraceptive (OC) use on heart disease in women 40 to 44 years of age. The researchers prospectively tracked whether or not the women developed a myocardial infarction (MI) over a 3-year period. The table below summarizes their results with columns indicating whether or not women developed MI and rows indicating their OC use.\n\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 2 × 1\n  x[,\"Yes\"] [,\"No\"]\n      &lt;dbl&gt;   &lt;dbl&gt;\n1        13    4987\n2         7    9993\n\n\n\nPart a\nCompute the estimated risk difference comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart b\nCompute the estimated relative risk comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart c\nCompute the estimated odds ratio comparing OC users to non-OC users. Include a 95% CI for the estimate and interpretation of the estimated value.\n\n\nPart d\nIs the OR a good approximation of the RR? Explain why or why not."
  },
  {
    "objectID": "homework/HW2.html#question-2",
    "href": "homework/HW2.html#question-2",
    "title": "Homework 2",
    "section": "Question 2",
    "text": "Question 2\nI was intrigued by the results from Question 1, so I started researching the relationship between contraception and MI. In a journal review article, the authors present smoking status as a potential confounder for the relationship between OC use and MI. In Table 2 of their paper, they present various risk ratios for smoking and nonsmoking women. Use the below tables, that are recreated from one study’s data, to answer the following questions.\nNonsmokers:\n\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 2 × 1\n  x[,\"Yes\"] [,\"No\"]\n      &lt;dbl&gt;   &lt;dbl&gt;\n1        36     317\n2        72     521\n\n\nSmokers:\n\n\nWarning: 'tidy.numeric' is deprecated.\nSee help(\"Deprecated\")\n\n\n# A tibble: 2 × 1\n  x[,\"Yes\"] [,\"No\"]\n      &lt;dbl&gt;   &lt;dbl&gt;\n1       210     416\n2       236     586\n\n\n\nPart a\nCompute the stratum-specific ORs for each smoking status. Present the estimates with 95% Confidence interval.\n\n\nPart b\nIs it appropriate to compute the smoking-adjusted Mantel-Haenszel odds ratio for OC vs. non-OC users? Explain your reasoning. You may choose to perform the Breslow-Day test only if you want to explore a new topic. Otherwise, use sentences to explain your general reasoning.\nIf you answered yes, then estimate the value with its 95% confidence interval.\n\n\nAside (not a homework problem)\nIf you are interested in a short review of the relationship between oral contraceptive pills (OCP) and MI, see this article abstract. I cannot find the full article online, so if you find it, please pass along. The abstract states:\n\nAlthough OCP doses were subsequently reduced, epidemiologic evidence continued to support a smaller, but significant association between OCPs and hypertension.\n\nThis single statement packs in some interesting aspects of the research process and influences from society. To start, the OCP doses were reduced from their initial approval. However, the early studies (1980s-90s) that link increased risk of hypertension and MI to OCs were still some of the first articles presented in my Google search. You may also find it interesting to think about how the perception of contraception has changed since the 1980s, and how societal views can impact research questions."
  },
  {
    "objectID": "homework/HW2.html#question-3",
    "href": "homework/HW2.html#question-3",
    "title": "Homework 2",
    "section": "Question 3",
    "text": "Question 3\nThe data presented below are from a case-control study of bladder cancer. Subjects with and without bladder cancer were recruited and then questioned about their occupation and cigarette consumption.\n\n\n\n\n\n\n  \n    \n    \n      High risk occupation\n      High cigarette consumption\n      Cases\n      Controls\n    \n  \n  \n    Yes\nNo\n48\n100\n    Yes\nYes\n180\n175\n    No\nNo\n30\n24\n    No\nYes\n120\n80\n  \n  \n  \n\n\n\n\n\nPart a\nPlease present the data using two 2x2 contingency table for High-risk occupation and Disease status, stratified by Cigarette consumption.\n\n\nPart b\nPlease present the data for High-risk occupation and Disease status using a 2x2 contingency table (you should combine the cigarette consumption).\n\n\nPart c\nIs it possible, in reference to the study design, to estimate the OR for bladder cancer and occupation from this study? If it is possible, calculate (using formulas) the crude odds ratio for bladder cancer comparing high-risk to other occupations, and estimate (using formulas) the associated 95% confidence intervals.\n\n\nPart d\nCompute the stratum-specific ORs at each of the two levels of cigarette consumption. Present the estimates with 95% Confidence interval.\n\n\nPart e\nIs it possible to estimate the RR for bladder cancer and occupation from this study? If it is possible, calculate the crude relative risk for bladder cancer comparing high-risk to other occupations, as well as the stratum-specific RRs at each of the two levels of cigarette consumption. Present the estimates with 95% Confidence interval. If it is not possible, give your reason.\n\n\nPart f\nWe will assume we found that odds ratios are relatively the same using the Breslow-Day test (which is true). Calculate the smoking-adjusted Mantel-Haenszel odds ratio for high-risk versus other occupations. Present the estimates with 95% Confidence interval.\n\n\nPart g\nAre the odds of bladder cancer different for high-risk versus other occupations after we adjusted for smoking status? (Hint: You should be running an inference test.)\n\n\nPart h\nDo you think the smoking-adjusted Mantel-Haenszel odds ratio for high-risk versus other occupations should be reported? Give your reason."
  },
  {
    "objectID": "homework/HW2.html#question-4",
    "href": "homework/HW2.html#question-4",
    "title": "Homework 2",
    "section": "Question 4",
    "text": "Question 4\nOne important aspect of medical diagnosis is its reproducibility. Suppose that two different doctors examine 100 patients for dyspnea in a respiratory-disease clinic, and that doctor A diagnosed 15 patients as having dyspnea (while doctor B did not), doctor B diagnosed 10 patients as having dyspnea (while doctor A did not), and both doctor A and doctor B diagnosed 7 patients as having dyspnea.\n\nPart a\nConstruct a two-way contingency table to summarize the dyspnea diagnoses from doctor A and B.\n\n\nPart b\nCompute the Cohen’s kappa (No need to compute the confidence interval.)\n\n\nPart c\nHow would you characterize the agreement between doctor A and B? Please refer to any guidelines used.\n\n\nPart d\nIs your computed kappa greater than 0? Run the appropriate test and interpret your results."
  },
  {
    "objectID": "homework/HW2.html#question-5",
    "href": "homework/HW2.html#question-5",
    "title": "Homework 2",
    "section": "Question 5",
    "text": "Question 5\nThis question is only graded for completion. Please fill out this Microsoft Form to help determine your project groups."
  },
  {
    "objectID": "readings.html",
    "href": "readings.html",
    "title": "613 Readings",
    "section": "",
    "text": "These reading assignments are for 613 students only. 613 students, you must complete all the readings by 6/6. I have included some recommended due dates that will keep you on track if you would like.\nIn case Nicky messed up with any of the links, here’s a link to the folder!\n\n\n\nReading\nRecommended Due Date\nLink to Sakai\nArticle\n\n\n\n\n1\n4/11 @ 11pm\n\n\n\n\n2\n4/25 @ 11pm\n\n\n\n\n3\n5/2 @ 11pm\n\n\n\n\n4\n5/16 @ 11pm\n\n\n\n\n5\n5/30 @ 11pm\n\n\n\n\n\n\nInstructions for each assignment\nFor each reading assignment, students will write a summary including:\n\nstudy background,\nwhy the author think the research important,\nstatistical methods used to address the research question,\nand a discussion/critique on whether you agree or disagree with the authors’  science and statistics.\n\nYou may write complete sentences under each bullet point. Please keep your summary to less than 1 page (margins are up to you). There are no requirements on formatting for references."
  },
  {
    "objectID": "weeks/week_01_sched.html",
    "href": "weeks/week_01_sched.html",
    "title": "Week 1",
    "section": "",
    "text": "```{css, echo=FALSE} .title{ font-size: 40px; color: #213c96; background-color: #fff; padding: 0px; }\n.description{ font-size: 20px; color: #fff; background-color: #213c96; padding: 10px; } ```"
  },
  {
    "objectID": "weeks/week_01_sched.html#resources",
    "href": "weeks/week_01_sched.html#resources",
    "title": "Week 1",
    "section": "Resources",
    "text": "Resources\nBelow is a table with links to resources. Icons in blue mean there is an available file link.\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording(s)\n\n\n\n\n\nIntro to Course\n\n\n\n\n\n1\nFile Organization within R\n\n\n\n\n\n2\nIntroduction to Categorical Analysis\n\n\n\n\n\n\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser.\n\nFor example, in Chrome: I would click on the 3 vertical dots in the top right corner, then click Print, then change the Destination to “Save as PDF.”\nIt doesn’t seem to work well in Safari… Let me know if you’re having trouble.\n\n\nHere is the link to my Poll Everywhere!!"
  },
  {
    "objectID": "weeks/week_01_sched.html#announcements",
    "href": "weeks/week_01_sched.html#announcements",
    "title": "Week 1",
    "section": "Announcements",
    "text": "Announcements\n\nShared folder for students\n\nContains a few textbooks that you can use!\n\nWe will use Agresti and Hosmer & Lemeshaw mostly!\n\n\nMake sure you can get into the Echo360 page!!\nLast day to drop without penalty: 4/12/2024"
  },
  {
    "objectID": "weeks/week_01_sched.html#on-the-horizon",
    "href": "weeks/week_01_sched.html#on-the-horizon",
    "title": "Week 1",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "weeks/week_01_sched.html#class-exit-tickets",
    "href": "weeks/week_01_sched.html#class-exit-tickets",
    "title": "Week 1",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/1)\n Wednesday (4/3)"
  },
  {
    "objectID": "weeks/week_01_sched.html#additional-information",
    "href": "weeks/week_01_sched.html#additional-information",
    "title": "Week 1",
    "section": "Additional Information",
    "text": "Additional Information"
  },
  {
    "objectID": "weeks/week_01_sched.html#muddiest-points",
    "href": "weeks/week_01_sched.html#muddiest-points",
    "title": "Week 1",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why does the test of trend treat ordinal variables as quantitative rather than qualitative?\nWhen we treat something as qualitative, we can only look at differences between groups. This means we cannot rank the groups and look at the change across groups. By treating the ordinal variables as quantitative, we can look at the change as we move from one group to another (and over all the ranked categories).\n\n\n2. Organizing the tests in a tree\nHere’s a organizational tree that I took from Meike and expanded:"
  },
  {
    "objectID": "lessons.html",
    "href": "lessons.html",
    "title": "Lessons",
    "section": "",
    "text": "Lesson\nTopic\nSlides\nAnnotated Slides\nRecording(s)\n\n\n\n\n1\nFile Organization within R\n\n\n\n\n\n2\nIntroduction to Categorical Analysis\n\n\n\n\n\n3\nMeasurement of Association for Contingency Tables\n\n\n\n\n\n4\nMeasurements of Association and Agreement\n\n\n\n\n\n\n5\nSimple Logistic Regression\n\n\n\n\n\n6\nTests for GLMs using Likelihood function\n\n\n\n\n\n7\nPredictions and Visualizations in Simple Logistic Regression\n\n\n\n\n\n8\nInterpretations and Visualizations of Odds Ratios\n\n\n\n\n\n9\nMissing Data\nActivity:\n\n\n\n\n\n\n10\nMultiple Logistic Regression\n\n\n\n\n\n11\nInteractions\n\n\n\n\n\n12\nAssessing Model Fit\n\n\n\n\n\n13\nNumerical Problems\n\n\n\n\n\n14\nModel Diagnostics\n\n\n\n\n\n15\nModel Building\n\n\n\n\n\n16\nPoisson Regression\n\n\n\n\n\n17\nLog-binomial regression"
  },
  {
    "objectID": "weeks/week_02_sched.html#resources",
    "href": "weeks/week_02_sched.html#resources",
    "title": "Week 2",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n3\nMeasurement of Association for Contingency Tables\n\n\n\n\n\n4\nMeasurements of Association and Agreement\n\n\n\n\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page.\nFor the slides, once they are opened, if you would like to print or save them as a PDF, the best way to do this is from a computer internet browser:\n\nClick on the icon with three horizontal bars on the bottom left of the browser.\nClick on “Tools” with the gear icon at the top of the sidebar.\nClick on “PDF Export Mode.”\nFrom there, you can print or save the PDF as you would normally from your internet browser.\n\nNote: this process does not work very well on an iPad."
  },
  {
    "objectID": "weeks/week_02_sched.html#on-the-horizon",
    "href": "weeks/week_02_sched.html#on-the-horizon",
    "title": "Week 2",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 1 due this Thursday!"
  },
  {
    "objectID": "weeks/week_02_sched.html#announcements",
    "href": "weeks/week_02_sched.html#announcements",
    "title": "Week 2",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/8\n\nOffice hours!!\n\nTuesdays 5:30-7pm with Antara\nThursdays 3:30 - 5pm with Ariel\nFridays 2 - 3:30pm with Nicky\n\n\n\n\nWednesday 4/10\n\nEcho360: Let’s all double check that we can see the recordings\n\nLink to class site\n\nHomework question 5: no need to do LRT in the table\nLab 1 is up!!\nQuiz 1 decision\n\nOnline in Sakai\nWill open up on Monday at 2pm. You can chose to take it in the classroom or wait\nQuiz will close before class on Wednesday\nOpen book still\nPlease do not cheat\n\nIf I notice any unusual changes to quiz performance compared to last quarter then we will go back to the old way of giving quizzes\n\nMultiple choice with potentially some free response\n\nFor example: interpreting an OR would be divided into a multiple choice for the estimate, CI, and then writing a sentence to interpret the estimate"
  },
  {
    "objectID": "weeks/week_02_sched.html#class-exit-tickets",
    "href": "weeks/week_02_sched.html#class-exit-tickets",
    "title": "Week 2",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/8)\n Wednesday (4/10)"
  },
  {
    "objectID": "weeks/week_02_sched.html#muddiest-points",
    "href": "weeks/week_02_sched.html#muddiest-points",
    "title": "Week 2",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. “times greater than” vs just “times” in interpretation\nI’ve seen it both ways. It comes down to more of an English language nuance, with what seems to be a long battle between viewpoints. Or maybe more accurately, I grammatically correct way to contruct the sentence, but with people understanding the meaning the “incorrect” way. I tend to be more lenient when it comes to grammar in this way, but maybe that’s because I have a general distaste when languages are rigid and don’t accommodate how people currently speak and write.\n\n\n2. For the relative risks poll everywhere question #2, how were they derived?\n\nFor #1 with Trt A’s risk as 0.01 (aka \\(risk_A=0.01\\)) and Trt B’s risk as 0.001 (aka \\(risk_B=0.01\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.01}{0.001} = 10\\)\n\nFor #2 with Trt A’s risk as 0.41 (aka \\(risk_A=0.41\\)) and Trt B’s risk as 0.401 (aka \\(risk_B=0.401\\))\n\nThe risk ratio is: \\(\\widehat{RR} = \\dfrac{\\widehat{p}_1}{\\widehat{p}_2} = \\dfrac{risk_A}{risk_B} = \\dfrac{0.41}{0.401} = 1.02\\)\n\n\n\n\n3. Ranges that odds ratios can take (0, infinity) vs the ranges that risk ratios can take.\nYeah, so both can theoretically take on the range \\([0, \\infty)\\). Both are ratios, so we also have to think about the range of the denominator and numerator For relative risk, the numerator and denominator are probabilities that can only take values from 0 to 1. While for ORs, the denominator and numerator are odds that can be a range of values \\([0, \\infty)\\).\nThe main point I was trying to make was that once we observe one group’s proportion/probability, then RRs and ORs will differ in their potential range. Let’s say I observe the proportion fro group 1 and now know the numerator for RR and the odds in the numerator for OR. Because the RR has numerator and denominator that has ranges \\([0, 1]\\), if we know the proportion of group 1 (aka numerator value), then the ratio itself has a smaller range of values because the denominator can only be between 0 and 1. Because the OR has numerator and denominator that has ranges \\([0, \\infty)\\), if we know the proportion of group 1, then we do have a fixed numerator. However, the denominator can still be in \\([0, \\infty)\\).\n\n\n4. For the odds ratio equation that we reviewed today, is it different from ad/bc ? If they are different, when is it appropriate to use the equation we just reviewed over the other? p1/(1-p1) / p2/(1-p2)\nNope! These are the same! If you learned it that way, you can definitely use it when we are working with contingency tables. However, once we move into ORs from regression with multiple covariates, I think it’s better to understand the ORs and odds in terms of the probability/proportion.\n\n\n5. Not directly related to this class, but did we cover LRTs already? They’re mentioned in HW1 but aren’t in my notes.\nOops! Fixed it in the HW. No need to do anything with LRTs!\n\n\n6. In Epi, we were very strictly told that Odds Ratios were only to be used in one type of study. (I.e. we CAN NOT use them in cross-sectional and cohort studies) only case-control. So what is the application of attempting to utilize them, if each respective type of study already has a “pre-assigned” statistical method that suits it best?\nOdds ratios CAN be used in cross-sectional AND cohort studies. It is often an over-estimate of the relative risk in those situations, so it is important to interpret it ONLY as the odds ratio.\nEach respective study does not have a pre-assigned method. The only restriction is that relative risk cannot be used in case-control studies."
  },
  {
    "objectID": "instructors.html",
    "href": "instructors.html",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nLink to Webex!!\n\nWednesdays 3 - 4pm"
  },
  {
    "objectID": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "href": "instructors.html#instructor-nicole-nicky-wakim-phd",
    "title": "Instructors",
    "section": "",
    "text": "Email: wakim@ohsu.edu\nOffice: VPT 622A\n\nPronouns: she/her/hers\nYou are welcome to address me as Nicky (pronounced “nik-EE”), Dr. Wakim (pronounced “wah-KEEM”), Dr. W, Dr. Nicky, Professor, Professor Wakim, or any combination of the prior.\nBest method to contact: Office hours or Slack for general course questions or E-mail/Calendly appointments for private communication.\n\n\n\n\n\n\n\n\n\n\nBrief professor statement: As a professor, my main goal is to instill a growth mindset into my students. Growth mindset means we are NOT stuck in our abilities or knowledge, and that we all can and will grow! This course aims to be as transparent as possible. I want you to understand my motivation for assessments, questions, and lessons. I also want those assessments to be clear, so please ask for clarification whenever needed.\n\n\nLink to Webex!!\n\nWednesdays 3 - 4pm"
  },
  {
    "objectID": "instructors.html#teaching-assistants",
    "href": "instructors.html#teaching-assistants",
    "title": "Instructors",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\nAntara and Ariel were great students in my BSTA 513 class last year! They will have the following office hours and will help answer questions on Slack.\n\nAntara Vidyarthi\nLink to Zoom!!\n\nTuesdays 5:30 - 7pm\n\n\n\nAriel Weingarten\nLink to Webex!!\n\nThursdays 3:30 - 5pm"
  },
  {
    "objectID": "instructors.html#statistics-tutor-for-epidemiology-students",
    "href": "instructors.html#statistics-tutor-for-epidemiology-students",
    "title": "Instructors",
    "section": "Statistics Tutor for Epidemiology Students",
    "text": "Statistics Tutor for Epidemiology Students\n\nBecky Lanford\n\nEmail: lanford@ohsu.edu\nLink to Becky’s Calendly\n\nBecky can help with:\n\nStatistical coding support\nSupport with stats concepts you are learning in class\nData management and analysis plan scheming during your PE\n\nIntroduction from Becky:\n\nHello fellow MPH classmates! My name is Becky Lanford. I’m looking forward to helping support you in your coursework this quarter. A little about my background: I am currently in my final year in the MPH Epidemiology track and have completed most of my coursework including the biostatistics and epidemiology series (mostly working in R). I enrolled at the SPH as someone re-entering the workforce and quite new to statistical programming. Though I had previously completed a graduate degree (as a Physician Assistant/Associate), re-acclimating to graduate work and learning programming skills made for a steep learning curve my first academic year. I credit the collaborative learning environment at SPH - support of TA’s and classmates and availability of instructors - for helping me be successful. I hope I can help answer course-content questions, problem solve with you and find answers if I don’t have them myself. I know there are many challenges to being a graduate student and I am excited to help our public health student community grow stronger and more knowledgeable together."
  },
  {
    "objectID": "lectures/01_Data_Management/01_Data_Management.html#folder-organization",
    "href": "lectures/01_Data_Management/01_Data_Management.html#folder-organization",
    "title": "Lesson 1: Data Management",
    "section": "",
    "text": "For a project, I usually have the following folders\n\nBackground\nCode\nData_Raw\nData_Processed\nDissemination\nReports\nMeetings\n\n\n\n\nFor our class, I suggest making one folder for the course with the following folders in it:\n\nData\nHomework\nNotes\nProject\nQuizzes\nAnd other folders if you want"
  },
  {
    "objectID": "lectures/01_Data_Management/01_Data_Management.html#creating-project",
    "href": "lectures/01_Data_Management/01_Data_Management.html#creating-project",
    "title": "Lesson 1: Data Management",
    "section": "Creating project",
    "text": "Creating project\n\nGo into RStudio\nCreate new project"
  },
  {
    "objectID": "lectures/01_Data_Management/01_Data_Management.html#here-package",
    "href": "lectures/01_Data_Management/01_Data_Management.html#here-package",
    "title": "Lesson 1: Data Management",
    "section": "Here package",
    "text": "Here package\n::: columns ::: column - Good source for the here package\n-   Just substitute `.Rmd` with `.qmd`\n:::"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#still-me",
    "href": "lectures/00_Intro/00_Intro.html#still-me",
    "title": "Lesson 0: Introduction",
    "section": "Still me!",
    "text": "Still me!"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#some-important-tasks",
    "href": "lectures/00_Intro/00_Intro.html#some-important-tasks",
    "title": "Lesson 0: Introduction",
    "section": "Some important tasks",
    "text": "Some important tasks\n\nJoin the Slack page!\nStar the class website: https://nwakim.github.io/S2024_BSTA_513/\nComplete the WhenIsGood for office hours\n\nIf your calendar feels set, take 5 mintues to fill this out now!\nComplete by Thursday at 11pm!!!\n\nHighly suggest that you make an appointment with a learning specialist through Student Academic Support Services!\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#syllabus",
    "href": "lectures/00_Intro/00_Intro.html#syllabus",
    "title": "Lesson 0: Introduction",
    "section": "Syllabus",
    "text": "Syllabus\n\nNot many changes from last quarter’s syllabus\nAssessment breakdown is the same\n\n\n\n\n\n\n\n\n\n\n\nCourse activity\nType of Assessment\nDue Date\nPercentage of final grade (BSTA 513)\nPercentage of final grade (BSTA 613)\n\n\nHomework\nFormative\nEvery 1-2 weeks\n33%\n28%\n\n\nQuizzes\nSummative\n4/22, 5/13, 6/3\n25%\n25%\n\n\nProject Labs\nFormative\nEvery 2-3 weeks\n25%\n25%\n\n\nProject Report\nSummative\n6/13\n10%\n10%\n\n\nExit tickets (Attendance)\nN/A\nTwice Weekly\n5%\n5%\n\n\nMid-Quarter Feedback\nN/A\n5/2\n2%\n2%\n\n\n613 Readings\nFormative\nApprox. every other week\n0%\n5%"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#homework-grading",
    "href": "lectures/00_Intro/00_Intro.html#homework-grading",
    "title": "Lesson 0: Introduction",
    "section": "Homework grading",
    "text": "Homework grading\n\nSlightly new grading\nNow, need to turn in 50% of the homework completed to get check mark\nNoticed that demonstrating understanding in the project was correlated with completing the homework*\n\nNo formal analysis was done on this\nAnd there may be confounders like time available to commit to this class in general\n\nEither way, I think practice is the most important tool for learning\n\nSo I want to us to practice the work, but I’m trying to balance this with added stress"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#how-to-print-slides",
    "href": "lectures/00_Intro/00_Intro.html#how-to-print-slides",
    "title": "Lesson 0: Introduction",
    "section": "How to print slides",
    "text": "How to print slides\n\nAnyone have issues with this?\n\nI can show how to do it in Chrome and Safari\n\nInstructions on Quarto page"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#lets-take-10-minutes",
    "href": "lectures/00_Intro/00_Intro.html#lets-take-10-minutes",
    "title": "Lesson 0: Introduction",
    "section": "Let’s take 10 minutes",
    "text": "Let’s take 10 minutes\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLesson 0: Introduction"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#lets-take-10-minutes-for-student-survey",
    "href": "lectures/00_Intro/00_Intro.html#lets-take-10-minutes-for-student-survey",
    "title": "Lesson 0: Introduction",
    "section": "Let’s take 10 minutes for Student Survey",
    "text": "Let’s take 10 minutes for Student Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n−+\n11:00"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#homework-grading-in-syllabus",
    "href": "lectures/00_Intro/00_Intro.html#homework-grading-in-syllabus",
    "title": "Lesson 0: Introduction",
    "section": "Homework grading in syllabus",
    "text": "Homework grading in syllabus"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#quizzes",
    "href": "lectures/00_Intro/00_Intro.html#quizzes",
    "title": "Lesson 0: Introduction",
    "section": "Quizzes",
    "text": "Quizzes\n\nMight be delivered slightly differently\nWe will still have the 50 minutes at the end of class to take the quiz\n\nAnd the quiz should only take 30 minutes\n\nHowever, RPV does not have private rooms for those who need it\nI am work-shopping ideas on how to deliver the quiz, including:\n\nTake home and turn in on Wednesday\nOnline vs. on paper\nWays to mitigate cheating"
  },
  {
    "objectID": "lectures/01_Data_Management/01_Data_Management.html",
    "href": "lectures/01_Data_Management/01_Data_Management.html",
    "title": "Lesson 1: Data Management",
    "section": "",
    "text": "For a project, I usually have the following folders\n\nBackground\nCode\nData_Raw\nData_Processed\nDissemination\nReports\nMeetings\n\n\n\n\nFor our class, I suggest making one folder for the course with the following folders in it:\n\nData\nHomework\nNotes\nProject\nQuizzes\nAnd other folders if you want"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#course-learning-objectives",
    "href": "lectures/00_Intro/00_Intro.html#course-learning-objectives",
    "title": "Lesson 0: Introduction",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nAt the end of this course, students should be able to…\n\nApply and interpret some hypothesis-testing procedures for two-way and three-way contingency tables\nCompute and interpret measures of association for binary and ordinal data.\nCalculate and correctly interpret odds ratios using logistic regression, make comparison across groups and examine relationship between binary outcome and predictor variables.\nApply appropriate model-building strategies for logistic regression. Effectively use statistical computing packages for contingency table and logistic regression procedures.\nPerform Poisson regression analysis using count data and interpret model estimates, make comparison across groups and examine relationship between outcome and predictor variables.\nCoherently summarize methods and results of data analyses, and discuss in context of original health-related research questions to audiences with varied statistical background."
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#new-way-to-recordview-lectures",
    "href": "lectures/00_Intro/00_Intro.html#new-way-to-recordview-lectures",
    "title": "Lesson 0: Introduction",
    "section": "New way to record/view lectures",
    "text": "New way to record/view lectures\n\nI will be using Echo360 to automatically record time from 1-3pm in this room!\nIt will be LIVE\n\nSo you can watch in real time, but won’t be able to interact with us\nMight have a 10 second lag\nNo need to tell me if you cannot make class\n\nHere is the link to our site! I’ll keep posting it on the weekly pages\n\nCurrently a public link, but working on getting everyone an account so we can make it private\n\nI don’t have to post anything after class, so no issues with me forgetting to post it"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#folder-organization",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#folder-organization",
    "title": "Lesson 1: File Organization within R",
    "section": "Folder organization",
    "text": "Folder organization\n\nMake these folders in your computer\n\nOnly make them in OneDrive if you have a desktop connection\n\n\n\n\n\nFor a project, I usually have the following folders\n\nBackground\nCode\nData_Raw\nData_Processed\nDissemination\nReports\nMeetings\n\n\n\n\nFor our class, I suggest making one folder for the course with the following folders in it:\n\nData\nHomework\nNotes\nProject\nQuizzes\nAnd other folders if you want\n\n\n\n\n\nTake a few minutes to create these folders\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#creating-project",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#creating-project",
    "title": "Lesson 1: File Organization within R",
    "section": "Creating project",
    "text": "Creating project\n\nGo into RStudio\nCreate new project"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#here-package",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#here-package",
    "title": "Lesson 1: File Organization within R",
    "section": "Here package",
    "text": "Here package\n\n\n\nGood source for the here package\nJust substitute .Rmd with .qmd\nBasically, a .qmd file and .R file work differently\n\nWe haven’t worked much with .R files\n\nFor .qmd files, the automatic directory is the folder it is in\n\nBut we want it to be the main project folder\n\nhere can help with that"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#creating-a-new-qmd-file",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#creating-a-new-qmd-file",
    "title": "Lesson 1: File Organization within R",
    "section": "Creating a new qmd file",
    "text": "Creating a new qmd file\n\nBasic steps\n\nCreate new .qmd (under File or top left corner)\nDecide on document types/options\n\nLet me show you my process\nCreate a .qmd in your Sample_folder under any folder (maybe Notes is good)\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#creating-project-in-rstudio",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#creating-project-in-rstudio",
    "title": "Lesson 1: File Organization within R",
    "section": "Creating project in RStudio",
    "text": "Creating project in RStudio\n\nWay to designate a working directory: basically your home base when working in R\n\nWe have to tell R exactly where we are in our folders and where to find other things\nA project makes it easier to tell R where we are\n\nBasic steps to create a project\n\nGo into RStudio\nCreate new project for this class (under File or top right corner)\n\nOnce we have projects, we can open one are R will automatically know that its location is the start of our working directory\nLet me show you my process\n\nI will create one in my Sample_folder\nI will show you how I switch between classes\n\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R_notes.html#folder-organization",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R_notes.html#folder-organization",
    "title": "Lesson 1: File Organization within R",
    "section": "Folder organization",
    "text": "Folder organization\n\n\n\nFor a project, I usually have the following folders\n\nBackground\nCode\nData_Raw\nData_Processed\nDissemination\nReports\nMeetings\n\n\n\n\nFor our class, I suggest making one folder for the course with the following folders in it:\n\nData\nHomework\nNotes\nProject\nQuizzes\nAnd other folders if you want\n\n\n\n\n\nLet’s take a couple minutes to do this!"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R_notes.html#creating-project-in-rstudio",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R_notes.html#creating-project-in-rstudio",
    "title": "Lesson 1: File Organization within R",
    "section": "Creating project in RStudio",
    "text": "Creating project in RStudio\n\nBasic steps\n\nGo into RStudio\nCreate new project for this class (under File or top right corner)\n\nLet me show you my process\nLet’s take a couple minutes to do this!"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R_notes.html#creating-a-new-qmd-file",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R_notes.html#creating-a-new-qmd-file",
    "title": "Lesson 1: File Organization within R",
    "section": "Creating a new qmd file",
    "text": "Creating a new qmd file"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R_notes.html#here-package",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R_notes.html#here-package",
    "title": "Lesson 1: File Organization within R",
    "section": "Here package",
    "text": "Here package\n::: columns ::: column - Good source for the here package\n-   Just substitute `.Rmd` with `.qmd`\n:::\n\n\n\n\n\n\n\n\n\nLesson 1: File Organization within R"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#now-we-take-a-10-minute-break",
    "href": "lectures/00_Intro/00_Intro.html#now-we-take-a-10-minute-break",
    "title": "Lesson 0: Introduction",
    "section": "Now we take a 10 minute break!",
    "text": "Now we take a 10 minute break!\n\n\n\n−+\n10:00\n\n\n\n\n\nLesson 0: Introduction"
  },
  {
    "objectID": "lectures/00_Intro/00_Intro.html#new-exit-ticket-style",
    "href": "lectures/00_Intro/00_Intro.html#new-exit-ticket-style",
    "title": "Lesson 0: Introduction",
    "section": "New Exit ticket style",
    "text": "New Exit ticket style\n\nAll the questions are optional\nBut still open the link and submit!\n5 of the exit tickets are dropped!"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#looking-at-source-vs.-visual",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#looking-at-source-vs.-visual",
    "title": "Lesson 1: File Organization within R",
    "section": "Looking at Source vs. Visual",
    "text": "Looking at Source vs. Visual"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#using-onedrive",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#using-onedrive",
    "title": "Lesson 1: File Organization within R",
    "section": "Using OneDrive",
    "text": "Using OneDrive\n\nWe all have free access to OneDrive to store files\nLet’s login into our online accounts\nYou can also download OneDrive for your desktop\n\nAllows you to access the OneDrive from your computer’s interface instead of the browser\nCreates a link between your computer and the cloud!\n\nLet me show you mine\n\nI can access all the files through RStudio as well!\n\nLet’s take a couple minutes to log into OneDrive\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#now-download-some-data-from-my-the-onedrive",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#now-download-some-data-from-my-the-onedrive",
    "title": "Lesson 1: File Organization within R",
    "section": "Now download some data from my the OneDrive",
    "text": "Now download some data from my the OneDrive\n\nGo into the Student files folder\nDownload the dataset in Sample_folder, under Data\nCreate your own Sample_folder under Notes\n\nSave the dataset there\n\nAlternatively, you can download all of Sample_folder and then put that under Notes"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#here-package-1n",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#here-package-1n",
    "title": "Lesson 1: File Organization within R",
    "section": "Here package (1/n)",
    "text": "Here package (1/n)\n\n\n\nGood source for the here package\nJust substitute .Rmd with .qmd\nBasically, a .qmd file and .R file work differently\n\nWe haven’t worked much with .R files\n\nFor .qmd files, the automatic directory is the folder it is in\n\nBut we want it to be the main project folder\n\nhere can help with that"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#here-package-2n",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#here-package-2n",
    "title": "Lesson 1: File Organization within R",
    "section": "Here package (2/n)",
    "text": "Here package (2/n)\n\nInstall here package: you can do this in your console (not inside .qmd file)\n\n\ninstall.packages(\"here\")\n\n\nWithin your .qmd file\n\n\n\nLesson 1: File Organization within R"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#install-here-package",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#install-here-package",
    "title": "Lesson 1: File Organization within R",
    "section": "Install here package",
    "text": "Install here package\n\nInstall here package: you can do this in your console (not inside .qmd file)\n\n\ninstall.packages(\"here\")\n\n\nWithin your console, type here() and enter\n\nTry this with getwd() as well"
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html#using-here-to-load-data",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html#using-here-to-load-data",
    "title": "Lesson 1: File Organization within R",
    "section": "Using here() to load data",
    "text": "Using here() to load data\n\nThere here() function will start at the working directory (where your .Rproj file is) and let you write out a file path for anything\nTo load the dataset in our .qmd file, we will use:\n\n\ndata = read_excel(here(\"./Data/CH05Q01.xls\"))\ndata = read_excel(here(\"Data\", \"CH05Q01.xls\"))\n\n\n\nWatch out when using lubridate package simultaneously\n\n\nUse here::here() if you have lubridate loaded within same .qmd. This will tell R to use the function here() within the here package instead of lubridate’s here() function. To call lubridate’s function, we’d use lubridate::here()\n\n\n\n\n\n−+\n05:00\n\n\n\n\n\nLesson 1: File Organization within R"
  },
  {
    "objectID": "homework/HW1.html#questons-part-1",
    "href": "homework/HW1.html#questons-part-1",
    "title": "Homework 1",
    "section": "Questons Part 1",
    "text": "Questons Part 1\nThe following questions are intended to give you practice in understanding concepts and completing calculations.\n\nQuestion 1\nIf the probability that one white blood cell is a lymphocyte is 0.3, compute the probability of 2 lymphocytes out of 10 white blood cells. Also, compute the probability that at least 3 lymphocytes out of 10 white blood cells. You may calculate by hand, using a web app, or using R.\n\n\nQuestion 2\nConsider a 2 x 2 table from a prospective cohort study:\n\n\n\n\n \n  \n      \n    Favorable \n    Unfavorable \n  \n \n\n  \n    Treatment \n    30 \n    20 \n  \n  \n    Placebo \n    10 \n    60 \n  \n\n\n\n\n\n\nPart a\nEstimate the probability of having favorable results for subjects in the treatment group. Include an interpretation and report with the 95% confidence interval.\n\n\nPart b\nRepeat part a for the placebo group.\n\n\nPart c\nConduct a statistical test to evaluate whether there is an association between group and outcome. What is the name of the test? Make sure to follow the entire test process demonstrated in the slides.\n\n\n\nQuestion 3\nConsider a cohort study with results shown as in following table:\n\n\n\n\n \n  \n      \n    Favorable \n    Unfavorable \n  \n \n\n  \n    Treatment \n    6 \n    1 \n  \n  \n    Placebo \n    2 \n    5 \n  \n\n\n\n\n\nConduct a statistical test to evaluate whether there is an association between group and outcome. What is the name of the test? Make sure to follow the entire test process demonstrated in the slides.\n\n\nQuestion 4\nTable 4 shows the information of a selected group of adolescents on whether they use smokeless tobacco and their perception of risk for using smokeless tobacco.\nTable 4:\n\n\n\n\n \n  \n      \n    YES \n    NO \n  \n \n\n  \n    Minimal \n    25 \n    60 \n  \n  \n    Moderate \n    35 \n    172 \n  \n  \n    Substantial \n    10 \n    200 \n  \n\n\n\n\n\n\nPart a\nConduct a statistical test to examine general association between adolescent smokeless tobacco users and risk perception. What is the name of the test? Make sure to follow the entire test process demonstrated in the slides.\n\n\nPart b\nIs there a trend of increased risk perception for smokeless tobacco users? What test would you use? Make sure to follow the entire test process demonstrated in the slides."
  },
  {
    "objectID": "homework/HW1.html#questions-part-2",
    "href": "homework/HW1.html#questions-part-2",
    "title": "Homework 1",
    "section": "Questions Part 2",
    "text": "Questions Part 2\nThe following questions are intended to give you practice in connecting concepts that will help you make decisions in real world applications.\n\nQuestion 5\nStart making a comprehensive table or outline for the inference tests that we have covered. Here is a list of the tests we have covered:\n\nSingle proportion\nChi-squared test for general association\nFisher’s Exact test for general association\nCochran-Armitage test for trend\nMantel-Haenszel test for linear trend\n\nAnd here is a list of attributes to include:\n\nNumber of variables testing\nTypes of variables\nCriteria (if any)\nHypothesis test\nTest statistic (if we went over it)\nR code for test\nSample size / Power calculation (optional, not discussed in class)\nSpecial notes (optional)\n\nFor example, I could make a table with different rows corresponding to different tests and different columns for each attribute.\n\n\nQuestion 6\nI want you to gain experience exploring a package and function. This is an important skill in coding that can help you grow as an applied statistician.\nIn your previous course, the function lm() was introduced to perform linear regression. In this class, we will heavily use the function glm(). By typing “?glm” in the R console, we can open the Help page for glm(). The following questions ask about the glm() function. You can Google or use R documentation to answer the questions.\nFeel free to read more about the differences between lm() and glm().\n\nPart a\nWhat does the input “family” mean? If I wanted to perform regression using a Poisson distribution, what would I input into family?\n\n\nPart b\nWhat is the default action for the “na.action” input?\n\n\nPart c\nHow does the glm() function fit our model? (Hint: see “method”)\n\n\nPart d\nDo you think the output of summary() will be the same for lm() and glm()?\n\n\n\nQuestion 7\nOPTIONAL\nLet’s make a decision tree on the different tests we learned! I would like you to make a flow chart for the different tests we learned in Classes 1 and 2. You’ll need to include characteristics for:\n\nNumber of variables (1, 2, or 3 - we will go over 3 variables in Class 4)\nNumber of categories in each variable\nSample size is small\nOrdinal/nominal independent variable\nOrdinal/nominal response variable(s)\n\nFor example, if I make a decision tree that includes end nodes for different animals (cat, dog, snake, turtle, and hawk) using yes/no characteristics (has a shell, woof/meows, has fur, or flies), then my flow chart would look like: See my example under Sakai Resources. You are welcome to draw this chart. I used SmartArt under the Insert tab in Word to create mine."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-is-categorical-data-analysis",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What is Categorical Data Analysis?",
    "text": "What is Categorical Data Analysis?\n\nIn BSTA 512/612 (linear regression), we focused on continuous responses/outcomes\n\nWe included categorical variables only as covariates (aka predictors, independent variables, explanatory variables)\nExamples from 512/612: life expectancy (in years), IAT score (ranging from -2 to 2)\n\n\n   \n\nCategorical data analysis focuses on the statistical methods for categorical responses/outcomes\n\nExplanatory (or ‘independent’) variable can be of any type (continuous or categorical)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables",
    "text": "Types of Variables"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables",
    "text": "Types of Variables\n\n\n\nLesson 2: Introduction to Categorical Data Analysis"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#types-of-variables-outcomes-we-will-cover-in-this-course",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Types of Variables: Outcomes we will cover in this course",
    "text": "Types of Variables: Outcomes we will cover in this course"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What does this course cover?",
    "text": "What does this course cover?\n\nStrategies for assessing association between categorical response variable and a one explanatory variable\n\nHypothesis testing\nMeasure of association\nSimple logistic regression\n\n\n   \n\nStatistical modeling strategies for assessing association between the categorical response variable and a set of explanatory variables\n\nLogistic regression\n\nFor binary, ordinal, and multinomial outcomes\n\nPoisson regression\n\nFor counts outcomes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\nNeeded steps:\n\nEstimate proportion \\(\\widehat{p}\\)\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation\nConstruct 95% confidence interval\nWrite interpretation"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-does-this-course-cover-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What does this course cover?",
    "text": "What does this course cover?\n\n\nStrategies for assessing association between categorical response variable and a one explanatory variable\n\nHypothesis testing\n\n\n\n\n\nMeasure of association\nSimple logistic regression\n\n\n   \n\nStatistical modeling strategies for assessing association between the categorical response variable and a set of explanatory variables\n\nLogistic regression\n\nFor binary, ordinal, and multinomial outcomes\n\nPoisson regression\n\nFor counts outcomes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#more-resources",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "More resources",
    "text": "More resources\n\nFor a refresher or review of one proportion and differences in proportions\n\nAnd their power calculations\nFrom Meike’s BSTA 511 course (see Day 12!)\n\nFor a refresher or review of Chi-squared test or Fisher’s Exact test\n\nFrom Meike’s BSTA 511 course (see Day 13!)\n\n\n\n\nLesson 2: Introduction to Categorical Data Analysis"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 1",
    "text": "Poll everywhere question 1"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution",
    "text": "Binomial Distribution\n\nConsider a sample of \\(n\\) independent trials, each of which can have only two possible outcomes (“success” and “failure”)\nFor each trial: \\[\\begin{align} P( \\text{success}) & = p \\\\\n                  P( \\text{failure}) & = 1- p = q \\end{align}\\]\nBinomial distribution: distribution of the number of successes in n independent trials\nThe probability mass function for the binomial distribution is: \\[P(X=k) = {n \\choose k} p^k q^{n-k}, \\text{ for } k = 0, 1, ..., n\\]\n\n\\(E(X) = np\\)\n\\(Var(X) = npq = np(1-p)\\)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-r-commands",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution: R commands",
    "text": "Binomial Distribution: R commands\nR commands with their input and output:\n\n\n\n\n\n\n\nR code\nWhat does it return?\n\n\n\n\nrbinom()\nreturns sample of random variables with specified binomial distribution\n\n\ndbinom()\nreturns probability of getting certain number of successes\n\n\npbinom()\nreturns cumulative probability of getting certain number or less successes\n\n\nqbinom()\nreturns number of successes corresponding to desired quantile"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#binomial-distribution-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Binomial Distribution Example",
    "text": "Binomial Distribution Example\n\n\nExample\n\n\nIf the probability that one white blood cell is a lymphocyte is 0.2, compute the probability of 2 lymphocytes out of 10 white blood cells\n\n\n\\[P(X=2) = {10 \\choose 2} 0.2^2 (1-0.2)^{10-2}  = 0.3020\\]\n\ndbinom(2, 10, 0.2) %&gt;% round(4)\n\n[1] 0.302"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#normal-approximation-of-the-binomial-distribution",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Normal Approximation of the Binomial Distribution",
    "text": "Normal Approximation of the Binomial Distribution\n\nAlso known as: Sampling distribution of \\(\\widehat{p}\\)\nIF \\(X\\sim \\text{Binomial}(n,p)\\) and \\(np&gt;10\\) and \\(nq = n(1-p) &gt; 10\\)\n\nEnsures sample size (\\(n\\)) is moderately large and the \\(p\\) is not too close to 0 or 1\nOther resources use other criteria (like \\(npq&gt;5\\) or \\(np&gt;5\\))\nWhen looking at a sample, we use \\(\\widehat{p}\\) instead of \\(p\\) to check this!!\n\n\n \n\nTHEN approximately \\(𝑋\\sim \\text{Normal}\\big(\\mu_X = np, \\sigma_X = \\sqrt{np(1-p)} \\big)\\)\n\nOr we often write this as the sampling distribution of \\(\\widehat{p}\\): \\[\\widehat{p} \\sim \\text{Normal}\\bigg(\\mu_{\\widehat{p}} = p, \\sigma_{\\widehat{p}} = \\sqrt{\\dfrac{p(1-p)}{n}}\\bigg)\\]\n\nPretty good video behind the intuition of this (Watch 00:00 - 05:40)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-of-single-proportion",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-of-single-proportion",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate of Single Proportion",
    "text": "Estimate of Single Proportion\n\nEstimate of proportion:\n\n\\[\n\\widehat{p} = \\dfrac{\\# \\text{successes}}{\\# \\text{successes} + \\# \\text{failures}}\n\\]\n\nUse the sampling distribution of \\(\\widehat{p}\\) to contruct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate proportion:\n\n\n\\[\\begin{align} \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}} \\\\ \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) - instead of \\(\\sigma_{p} = \\sqrt{\\frac{p(1-p)}{n}}\\) - because we don’t know exactly what \\(p\\) is"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-smoking-status",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-smoking-status",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Smoking Status",
    "text": "Example: Smoking Status\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\nEstimate proportion \\(\\widehat{p}\\)\nCheck that \\(np&gt;10\\) in order to make normal approximation\nConstruct 95% confidence interval\nWrite interpretation"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-for-difference-in-proportions",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-for-difference-in-proportions",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate for Difference in Proportions",
    "text": "Estimate for Difference in Proportions\nWhat to use for SE in CI formula?\n\\[\\hat{p}_1 - \\hat{p}_2 \\pm z^* \\cdot SE_{\\hat{p}_1 - \\hat{p}_2}\\]\nSE in sampling distribution of \\(\\hat{p}_1 - \\hat{p}_2\\)\n\\[\\sigma_{\\hat{p}_1 - \\hat{p}_2} =\n\\sqrt{\n\\frac{p_1\\cdot(1-p_1)}{n_1} + \\frac{p_2\\cdot(1-p_2)}{n_2}} \\]\nProblem: We don’t know what \\(p\\) is - it’s what we’re estimating with the CI.\nSolution: approximate \\(p_1\\), \\(p_2\\) with \\(\\hat{p}_1\\), \\(\\hat{p}_2\\):\n\\[SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-heart-attack-study",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-heart-attack-study",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Heart Attack Study",
    "text": "Example: Heart Attack Study"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mcnemars-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "McNemar’s Test",
    "text": "McNemar’s Test\n\nMcNemar’s test should be used if data is from a matched pairs study\n\n \n\nWhat is a matched-pairs study?\n\nParticipants are paired based on key characteristics\nEach participant within a pair will be assigned to different treatment groups\n\nCategorical test that is parallel to the “paired t-test”\n\n \n\nR packages and functions\n\nNormal approximation: mcnemar.test() in built-in stats package\nExact test: mcnemar.exact() in exact2x2 package\n\n\n \n\nIf you would like more information of McNemar’s test, please see Rosner TB: 10.4 and 10.5: Paired Samples"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-tables",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-tables",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Tables",
    "text": "Contingency Tables"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association",
    "text": "Test of General Association\n\nGeneral research question: Are two variables (both categorical, nominal) associated with each other?\n\n \n\nTranslated to a hypothesis test:\n\n\\(H_0\\) : There is no association between the two variables / The variables are independent\n\\(H_1\\) : There is an association between the two variables / The variables are not independent\n\n\n   \n\nWe have two options for testing general association:\n\nChi-squared test\nFisher’s Exact test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-chi-squared",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-chi-squared",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: Chi-squared",
    "text": "Test of General Association: Chi-squared\n\nTest to see how likely is it that we observe our data given the null hypothesis (no association)\nWe use the null to calculate the expected cell counts and compare them to the observed cell counts\nRequirements to conduct Chi-squared test\n\nFor 2 x 2 contingency table:\n\nNo expected cell counts should be less than 10\n\nFor contingency table with 3x2, 3x3, 4x4, etc.:\n\nNo more than 20% of expected cell counts are less than 5\nNo expected cell counts are less than 1"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-likelihood-ratio-test-lrt",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-likelihood-ratio-test-lrt",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: Likelihood Ratio Test (LRT)",
    "text": "Test of General Association: Likelihood Ratio Test (LRT)"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-fishers-exact-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-fishers-exact-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: Fisher’s Exact Test",
    "text": "Test of General Association: Fisher’s Exact Test\n\nOnly necessary when expected counts in one or more cells is less than 5\nGiven row and column totals fixed, computes exact probability that we observe our data or more extreme data\nConsider a general 2 x 2 table:\n\n\n\nThe exact probability of observing a table with cells (a, b, c, d) can be computed based on the hypergeometric distribution\n\n\\[P(a, b, c, d) = \\dfrac{(a+b)!\\cdot(c+d)!\\cdot(a+c)!\\cdot(b+d)!}{n!\\cdot a!\\cdot b!\\cdot c!\\cdot d!}\\]\n\nNumerator is fixed and denominator changes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-heart-attack-study-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-heart-attack-study-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Heart Attack Study",
    "text": "Example: Heart Attack Study"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend",
    "text": "Test of Trend\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nYou can use a test of general association for non-ordinal variables\n\n\n \n\nTwo tests of trend that we we learn:\n\nCochran-Armitage test\n\nTests association between a binary response and an ordinal explanatory variable\n\nMantel-Haenszel test\n\nTest association between an ordinal response and an ordinal explanatory variable"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Cochran-Armitage test",
    "text": "Test of Trend: Cochran-Armitage test\n\nCochran-Armitage test for trend will determine if there is association between a binary response variable and an ordinal variable with 3 or more categories\nIt will test the trend of the proportions over the ordinal variable\n\nAnswers the question: Does the proportion of people with a “successful” outcome increase as the ordinal explanatory variable increases?\n\nCochran-Armitage test for trend is only suitable for 2 x C contingency tables"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Mantel-Haenszel test",
    "text": "Test of Trend: Mantel-Haenszel test\n\nWhen both variables are ordinal, we can conduct Mantel-Haenszel test of trend for linear association\nMantel-Haenszel test for linear trend is suitable for any R x C contingency tables with two ordinal variables\nHypothesis test:\n\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThere is no correlation between the two variables \\[ \\rho = 0\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThere is correlation between the two variables\n\\[ \\rho \\neq 0\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Smoking status example\n\n\nA cross-sectional study of 8681 patients was conducted to evaluate the nature of smoking status among people. Of the 8681 people, 4840 were nonsmokers and 3841 were smokers.\n\n\n\n\n\nEstimate proportion \\(\\widehat{p}\\) \\[ \\widehat{p} = \\dfrac{3841}{3841 + 4840} = \\dfrac{3841}{8681} = 0.44246\\]\nCheck that \\(n\\widehat{p}&gt;10\\) and \\(n(1-\\widehat{p})&gt;10\\) in order to make normal approximation \\[ n\\widehat{p} = 8681\\cdot0.4425 = 3841 &gt; 10\\] \\[ n(1-\\widehat{p}) = 8681\\cdot(1-0.4425) = 4840 &gt; 10\\]\n\n\n\nConstruct 95% confidence interval\n\n\\[ \\widehat{p} \\pm z^*_{0.975} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\]\n\nprop.test(x = 3841, n = 8681, correct = T)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  3841 out of 8681, null probability 0.5\nX-squared = 114.73, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.4319827 0.4529896\nsample estimates:\n        p \n0.4424605 \n\n\n\nWrite interpretation of estimate\n\nThe estimated proportion of smokers is 0.442 (95% CI: 0.432, 0.453).\nAdditional interpretation of CI: We are 95% confident that the (population) proportion of smokers is between 0.432 and 0.453."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll everywhere question 2",
    "text": "Poll everywhere question 2"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#summary-so-far",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Summary so far",
    "text": "Summary so far\n\nIntroduced categorical data as the response in analysis\nReviewed an important distribution (Binomial distribution) for categorical data analysis\nEstimated a single proportion from a sample with its confidence interval\nEstimated a difference in proportions from a sample with its confidence interval\n\n   \n\nCan we expand this to ask a more general question about association between a response and explanatory variable?\n\nWhat if there is more than 2 categories for either variable?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#basic-analytical-procedures",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#basic-analytical-procedures",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Basic Analytical Procedures",
    "text": "Basic Analytical Procedures\nGeneral procedure:\n \n\nPresent observed data\n\n \n\nDescriptive analysis\n\n \n\nInferential tests"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-of-single-proportion",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval of Single Proportion",
    "text": "Estimate and Confidence Interval of Single Proportion\n\nEstimate of proportion:\n\n\\[\n\\widehat{p} = \\dfrac{\\# \\text{successes}}{\\# \\text{successes} + \\# \\text{failures}}\n\\]\n\nUse the sampling distribution of \\(\\widehat{p}\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate proportion:\n\n\n\\[\\begin{align} \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}} \\\\ \\widehat{p} &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}} = \\sqrt{\\frac{\\hat{p}(1-\\hat{p})}{n}}\\) - instead of \\(\\sigma_{p} = \\sqrt{\\frac{p(1-p)}{n}}\\) - because we don’t know exactly what \\(p\\) is"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#estimate-and-confidence-interval-for-difference-in-proportions",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Estimate and Confidence Interval for Difference in Proportions",
    "text": "Estimate and Confidence Interval for Difference in Proportions\n\nUse the sampling distribution of \\(\\widehat{p}_1\\) and \\(\\widehat{p}_2\\) to construct the confidence interval:\n\n\\((1-\\alpha)\\%\\) confidence interval for estimate difference in proportions:\n\n\n\\[\\begin{align} \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot SE_{\\hat{p}_1 - \\hat{p}_2} \\\\ \\widehat{p}_1 - \\widehat{p}_2 &\\pm z^*_{(1-\\alpha/2)} \\cdot \\sqrt{\n\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}} \\end{align}\\]\n\nUsing \\(SE_{\\hat{p}_1 - \\hat{p}_2} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\) because we don’t know exactly what \\(p_1\\) and \\(p_2\\) are"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#step-1-present-observed-data",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#step-1-present-observed-data",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Step 1: Present Observed Data",
    "text": "Step 1: Present Observed Data\nCategorical data are often presented in tabular form, known as contingency table, which displays counts of outcomes in the cell • Smoking Example: 2 X 2 contingency table for smoking status by sex assigned at birth (sex AAB) • For ordinal variables: make sure level of rows and columns are sorted correctly"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#step-2-descriptive-analysis",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#step-2-descriptive-analysis",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Step 2: Descriptive Analysis",
    "text": "Step 2: Descriptive Analysis\nIncludes estimation of the proportions • Computed proportions should almost always be reported with 95% estimated confidence interval • If proportions are confusing as written, can use graphic representation (i.e. bar chart) • Smoking Example: 49.6% (95% CI: (48.1%, 51.0%)) of people assigned male at birth are nonsmokers and 62.5% (95% CI: (61.0%, 63.9%)) of people assigned female at birth are nonsmokers"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#step-3-inferential-tests",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#step-3-inferential-tests",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Step 3: Inferential Tests",
    "text": "Step 3: Inferential Tests\nTo evaluate the association between the categorical response variable and a categorical independent variable • If only two categories in both variables (Class 1 notes) • Use difference in proportions • If matched-pair, use McNemar’s test • If two variables with 2 or more categories (Today’s Notes) • For nominal categories: Chi-square test, Likelihood Ratio test, or Fisher’s exact test • For one nominal/one ordinal variable: Cochran-Armitage test • For ordinal categories: Mantel-Haenszel test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-tables-r-x-c",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Tables (R x C)",
    "text": "Contingency Tables (R x C)\n\nR X C contingency tables\n\nContains information for two discrete variables: one has R categories and the other has C categories.\nRefers to the number of rows (R) and number of columns (C) in the table\n\n\n \n\nFor two proportions: focused on 2 X 2 contingency tables\n\nR = 2, C = 2\n\n\n \n\nExpand our contingency tables to variables with 2 or more categories\n\nCategories can be ordinal or nominal"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#contigency-table-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#contigency-table-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contigency Table: Example",
    "text": "Contigency Table: Example\nLet’s say we are interested in learning the association between the development of breast cancer and age at first birth. Our first step in our basic analytical procedure is to display the data with a contingency table:\n{fig-align=“center”; width=“400”}\n\nThis is a 2 x 5 contingency table"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-associationtrend-of-r-x-c-contingency-table",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test Association/Trend of R X C Contingency Table",
    "text": "Test Association/Trend of R X C Contingency Table\n \n\n\nIf both variables are nominal, a test of general association will be sufficient\n\nTest of general association is the same regardless of R and C\nTest used for 2x2 contingency table same as 5x3 contingency table\nWe will cover:\n\nChi-squared test\nFisher Exact test\n\n\n\n\n\nIf one or both variables are ordinal, a test of trend may be of interest\n\nTreats ordinal variables as quantitative rather than qualitative (nominal scale)\nTest of trend has greater power than the test of general association\nWe will cover:\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-5",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-5",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study",
    "text": "Example: Strong Heart Study\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND) to study prevalence and incidence of cardiovascular disease and to identify risk factors. We will be examining the 4-year cumulative incidence of diabetes with one risk factor, glucose tolerance.\n \n\nImpaired glucose: normal or impaired glucose tolerance at baseline visit (between 1988 and 1991)\n \nDiabetes: Indicator of diabetes at follow-up visit (roughly four years after baseline) according to two-hour oral glucose tolerance test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-12",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (1/2)",
    "text": "Example: Strong Heart Study (1/2)\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND) to study prevalence and incidence of cardiovascular disease and to identify risk factors. We will be examining the 4-year cumulative incidence of diabetes with one risk factor, glucose tolerance.\n \n\nImpaired glucose: normal or impaired glucose tolerance at baseline visit (between 1988 and 1991)\n \nDiabetes: Indicator of diabetes at follow-up visit (roughly four years after baseline) according to two-hour oral glucose tolerance test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#example-strong-heart-study-22",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Example: Strong Heart Study (2/2)",
    "text": "Example: Strong Heart Study (2/2)\nThere is a total of 1664 American Indians in the dataset, with the following distribution of folks with diabetes and glucose tolerance:\n \n\n\n\n#shs_data = read.csv(file = here(\"./data/SHS_data.csv\"))\n\n\nSHS = tibble(Diabetes = c(rep(\"Not diabetic\", \n                   1338), \n                   rep(\"Diabetic\", 326)),\n              Glucose = c(rep(\"Normal\", \n                  1004),#Not diabetic\n          rep(\"Impaired\", 334),\n          rep(\"Normal\", \n              128), #Diabetic\n          rep(\"Impaired\", 198)))\n\n\n\n\nDisplaying the contingency table in R\nSHS %&gt;% tabyl(Glucose, Diabetes) %&gt;% \n  adorn_totals(where = c(\"row\", \"col\")) %&gt;% \n  gt() %&gt;% \n  tab_stubhead(label = \"Glucose Impairment\") %&gt;%\n  tab_spanner(label = \"Diabetes\", \n              columns = c(\"Not diabetic\", \"Diabetic\")) %&gt;%\n  tab_options(table.font.size = 45)\n\n\n\n\n\n\n  \n    \n    \n      Glucose\n      \n        Diabetes\n      \n      Total\n    \n    \n      Not diabetic\n      Diabetic\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\nNeeded steps:\n\nEstimate the difference in proportions\nCheck that each cell has at least 10 individuals\nConstruct 95% confidence interval\nWrite interpretation of estimate"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#section-3",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "",
    "text": "Strong Heart Study\n\n\nWhat is the difference in proportions for American Indians that have diabetes comparing individuals with normal vs. impaired glucose?\n\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose\n      \n        Diabetes\n      \n      Total\n    \n    \n      Not diabetic\n      Diabetic\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\nEstimate the difference in proportions \\[ \\widehat{p}_1 -\\widehat{p}_2 = \\dfrac{198}{532} - \\dfrac{128}{1132} = 0.2591\\]\nCheck that each cell has at least 10 individuals\n\n\n\nConstruct 95% confidence interval\n\n\nprop.test(x = table(SHS$Glucose, SHS$Diabetes), \n          correct = T)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  table(SHS$Glucose, SHS$Diabetes)\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.2126963 0.3055162\nsample estimates:\n   prop 1    prop 2 \n0.3721805 0.1130742 \n\n\n\nWrite interpretation of estimate\n\nThe estimated difference in proportion of diabetic American Indians comparing is 0.259 (95% CI: 0.213, 0.306).\nAdditional interpretation of CI: We are 95% confident that the difference in (population) proportions of American Indians who have normal glucose tolerance and impaired glucose tolerance that developed diabetes is between 0.213 and 0.306."
  },
  {
    "objectID": "lectures/01_File_Organization_R/01_File_Organization_R.html",
    "href": "lectures/01_File_Organization_R/01_File_Organization_R.html",
    "title": "Lesson 1: File Organization within R",
    "section": "",
    "text": "We all have free access to OneDrive to store files\nLet’s login into our online accounts\nYou can also download OneDrive for your desktop\n\nAllows you to access the OneDrive from your computer’s interface instead of the browser\nCreates a link between your computer and the cloud!\n\nLet me show you mine\n\nI can access all the files through RStudio as well!\n\nLet’s take a couple minutes to log into OneDrive\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#basic-analytic-procedures",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#basic-analytic-procedures",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Basic Analytic Procedures",
    "text": "Basic Analytic Procedures\nGeneral procedure:\n \n\nPresent observed data\n\n \n\nDescriptive analysis\n\n \n\nInferential tests"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example-12",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example-12",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: SHS Example (1/2)",
    "text": "Test of General Association: SHS Example (1/2)\n\nMain question: Do American Indians with impaired glucose tolerance have a different incidence of diabetes?\n\nIs glucose tolerance associated with diabetes incidence among American Indians?\n\nWe have two variables, and both variables have two nominal categories\n\nDiabetes outcome: Not diabetic and Diabetic\nGlucose tolerance: Normal or Impaired\n\nAnswer research question with a test of general association\nHypothesis:\n\n\\(H_0\\) : no association between glucose tolerance and diabetes\n\\(H_1\\) : association exists between glucose tolerance and diabetes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example-22",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example-22",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: SHS Example (2/2)",
    "text": "Test of General Association: SHS Example (2/2)\n\nMain question: Do American Indians with impaired glucose tolerance have a different incidence of diabetes?\n\nIs glucose tolerance associated with diabetes incidence among American Indians?\n\nStart with the 2x2 contingency table\nWe have two variables, and both variables have two nominal categories\n\nDiabetes outcome: Not diabetic and Diabetic\nGlucose tolerance: Normal or Impaired\n\nAnswer research question with a test of general association"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: SHS Example",
    "text": "Test of General Association: SHS Example\n\nMain question: Do American Indians with impaired glucose tolerance have a different incidence of diabetes?\n\nIs glucose tolerance associated with diabetes incidence among American Indians?\n\nWe have two variables, and both variables have two nominal categories\n\nDiabetes outcome: Not diabetic and Diabetic\nGlucose tolerance: Normal or Impaired\n\n\n \n\nAnswer research question with a test of general association\nHypothesis:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes / Glucose tolerance and diabetes are independent\n\\(H_1\\) : There is an association between glucose tolerance and diabetes / Glucose tolerance and diabetes are not independent"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association",
    "text": "Test of General Association\n\nWe have three options for testing general association:\n\nChi-squared test\nLikelihood Ratio test (LRT)\nFisher’s Exact test"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#expected-cell-counts",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#expected-cell-counts",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Expected Cell Counts",
    "text": "Expected Cell Counts\n\nIs the sample size big enough for the chi-square test to be adequate? Expected cell counts?\nIf you want an explanation of how to calculate by hand, please see Vu and Harringtion TB (section 8.3.1, page 405)\nToo time consuming for this class, but R does it quickly using the"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-hypothesis-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-hypothesis-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Cochran-Armitage test: Hypothesis Test",
    "text": "Test of Trend: Cochran-Armitage test: Hypothesis Test\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThe proportions of successes are the same across all C ordinal values of the explanatory variable. \\[p_1 = p_2 = ... = p_C\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThe proportions of successes tend to increase as ordinal value of the explanatory variable increases\n\\[p_1 \\leq p_2 \\leq ... \\leq p_C\\]\nOR\nThe proportions of successes tend to decrease as ordinal value of the explanatory variable increases\n\\[p_1 \\geq p_2 \\geq ... \\geq p_C\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Cochran-Armitage test: Process",
    "text": "Test of Trend: Cochran-Armitage test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\nWe will not discuss the test statistic’s equation\nJust know it follows a Normal distribution\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Cochran-Armitage test: Example",
    "text": "Test of Trend: Cochran-Armitage test: Example\nWe are interested in learning the association between the development of breast cancer and age at first birth among people who have given birth"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-example-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-example-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Cochran-Armitage test: Example",
    "text": "Test of Trend: Cochran-Armitage test: Example\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nCancer = c(320, 1206, 1011, 463, 220)\nNo_Cancer = c(1422, 4432, 2893, 1092, 406)\nbscancer = matrix (c(Cancer, No_Cancer), nrow = 2, byrow = T)\nrownames(bscancer) = c(\"Cancer\",\"No Cancer\")\ncolnames(bscancer) = c(\"&lt;20\",\"20-24\",\"25-29\",\"30-34\",\"&gt;=35\")\nbscancer\n\n           &lt;20 20-24 25-29 30-34 &gt;=35\nCancer     320  1206  1011   463  220\nNo Cancer 1422  4432  2893  1092  406\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-example-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-cochran-armitage-test-example-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Cochran-Armitage test: Example",
    "text": "Test of Trend: Cochran-Armitage test: Example\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): The proportions of breast cancer are the same for all age levels of first birth. \\[p_1 = p_2 = ... = p_5\\]\n\\(H_1\\): The proportions of breast cancer tends to increase as level of age of first birth increases\n\n\n\\[p_1 \\leq p_2 \\leq ... \\leq p_5\\]\n\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\n\nlibrary(DescTools)\nCochranArmitageTest(bscancer)\n\n\n    Cochran-Armitage test for trend\n\ndata:  bscancer\nZ = 11.358, dim = 5, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that proportions of breast cancer are the same for all age levels of first birth (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that the proportion of of breast cancer increase as the the age at first birth increases."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Mantel-Haenszel test: Process",
    "text": "Test of Trend: Mantel-Haenszel test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Mantel-Haenszel test: Example",
    "text": "Test of Trend: Mantel-Haenszel test: Example\nA water treatment company is studying water additives and investigating how they affect clothes washing. The treatments studies where no treatment (plain water), the standard treatment, and a double dose of the standard treatment, called super. Washability was measured as low, medium and high. Are levels of washability associated with treatment?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test-example-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test-example-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Mantel-Haenszel test: Example",
    "text": "Test of Trend: Mantel-Haenszel test: Example\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nwater = matrix (c(27, 14, 5, 10, 17, 26, 5, 12, 50), nrow = 3, byrow = T)\nrownames(water) = c(\"plain\",\"standard\",\"super\")\ncolnames(water) = c(\"low\",\"medium\",\"high\")\nwater\n\n         low medium high\nplain     27     14    5\nstandard  10     17   26\nsuper      5     12   50\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test-example-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-trend-mantel-haenszel-test-example-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of Trend: Mantel-Haenszel test: Example",
    "text": "Test of Trend: Mantel-Haenszel test: Example\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): Water treatment and washability are not correlated. \\[ \\rho = 0\\]\n\\(H_1\\): Water treatment and washability are correlated.\n\n\n\\[ \\rho \\neq 0\\]\n\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\n\nlibrary(DescTools)\nMHChisqTest(water)\n\n\n    Mantel-Haenszel Chi-Square\n\ndata:  water\nX-squared = 50.602, df = 1, p-value = 1.132e-12\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that there is no correlation between washability and water treatment (\\(p = 1.13 \\cdot 10^{-12} &lt; 0.05\\)). There is sufficient evidence that level of water treatment is associated with washability."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-have-we-learned-today",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#what-have-we-learned-today",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "What have we learned today?",
    "text": "What have we learned today?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-chi-squared-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-chi-squared-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: Chi-squared test",
    "text": "Test of General Association: Chi-squared test\n\nTest to see how likely is it that we observe our data given the null hypothesis (no association)\n\n \n\nWe use the null to calculate the expected cell counts and compare them to the observed cell counts\n\n \n\nRequirements to conduct Chi-squared test\n\nFor 2 x 2 contingency table:\n\nNo expected cell counts should be less than 10\n\nFor contingency table with 3x2, 3x3, 4x4, etc.:\n\nNo more than 20% of expected cell counts are less than 5\nNo expected cell counts are less than 1"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIs the sample size big enough for the chi-square test to be adequate? What are the expected cell counts?\n\n \n\nIf you want an explanation of how to calculate by hand, please see Vu and Harringtion TB (section 8.3.1, page 405)\n\n \n\nToo time consuming for this class, but R does it quickly using the expected() function in the epitools package"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-expected-cell-counts-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Expected Cell Counts",
    "text": "Chi-squared test: Expected Cell Counts\n\nIn the Strong Heart Study…\n\n\nSHS_table = table(SHS$Glucose, SHS$Diabetes)\nSHS_table\n\n          \n           Diabetic Not diabetic\n  Impaired      198          334\n  Normal        128         1004\n\nlibrary(epitools)\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\n \n\n\n\n\n\nAll expected counts &gt; 5"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-chi-squared-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-chi-squared-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: Chi-squared test: Process",
    "text": "Test of General Association: Chi-squared test: Process\n\nCheck that the expected cell counts threshold is met\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-shs-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: SHS Example",
    "text": "Chi-squared test: SHS Example\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nAll expected cells are greater than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nchisq.test(x = SHS_table, correct = T)\n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  SHS_table\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-fisher-exact-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#test-of-general-association-fisher-exact-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Test of General Association: Fisher Exact test: Process",
    "text": "Test of General Association: Fisher Exact test: Process\n\nCheck the expected cell counts\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Fisher Exact test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fisher-exact-test-shs-example-not-appropriate-use-of-fisher-exact-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fisher-exact-test-shs-example-not-appropriate-use-of-fisher-exact-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher Exact test: SHS Example (not appropriate use of Fisher Exact test)",
    "text": "Fisher Exact test: SHS Example (not appropriate use of Fisher Exact test)\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nAll expected cells are greater than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nfisher.test(x = SHS_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  SHS_table\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 3.576595 6.048639\nsample estimates:\nodds ratio \n  4.644825 \n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#contingency-table-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Contingency Table: Example",
    "text": "Contingency Table: Example\nLet’s say we are interested in learning the association between the development of breast cancer and age at first birth. Our first step is typically to present the observed data:\n\n\nThis is a 2 x 5 contingency table"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#poll-everywhere-question-4",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test",
    "text": "Chi-squared test\n\nTest to see how likely is it that we observe our data given the null hypothesis (no association)\n\n \n\nWe use the null to calculate the expected cell counts and compare them to the observed cell counts\n\n \n\nRequirements to conduct Chi-squared test (expected cell counts)\n\nFor 2 x 2 contingency table:\n\nNo expected cell counts should be less than 10\n\nFor contingency table with 3x2, 3x3, 4x4, etc.:\n\nNo more than 20% of expected cell counts are less than 5\nNo expected cell counts are less than 1"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#chi-squared-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Chi-squared test: Process",
    "text": "Chi-squared test: Process\n\nCheck that the expected cell counts threshold is met\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\n\nOnly necessary when expected counts in one or more cells is less than 5\nGiven row and column totals fixed, computes exact probability that we observe our data or more extreme data\nConsider a general 2 x 2 table:\n\n\n\nThe exact probability of observing a table with cells (a, b, c, d) can be computed based on the hypergeometric distribution\n\n\\[P(a, b, c, d) = \\dfrac{(a+b)!\\cdot(c+d)!\\cdot(a+c)!\\cdot(b+d)!}{n!\\cdot a!\\cdot b!\\cdot c!\\cdot d!}\\]\n\nNumerator is fixed and denominator changes"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fisher-exact-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fisher-exact-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher Exact test: Process",
    "text": "Fisher Exact test: Process\n\nCheck the expected cell counts\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Fisher Exact test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: Process",
    "text": "Fisher’s Exact test: Process\n\nCheck the expected cell counts\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Fisher Exact test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#fishers-exact-test-shs-example-not-appropriate-use-of-fishers-exact",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)",
    "text": "Fisher’s Exact test: SHS Example (not appropriate use of Fisher’s Exact)\n\n\n\nCheck expected cell counts threshold\n\n\nexpected(SHS_table)\n\n          \n           Diabetic Not diabetic\n  Impaired  104.226      427.774\n  Normal    221.774      910.226\n\n\nWe’re going to pretend they are less than 5.\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\) : There is no association between glucose tolerance and diabetes\n\\(H_1\\) : There is an association between glucose tolerance and diabetes\n\n\n\n\nCalculate the test statistic and p-value for Chi-squared test in R\n\n\nfisher.test(x = SHS_table)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  SHS_table\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 3.576595 6.048639\nsample estimates:\nodds ratio \n  4.644825 \n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that glucose tolerance and diabetes are not associated (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that glucose tolerance and diabetes incidence are associated among American Indians."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test",
    "text": "Cochran-Armitage test\n\nCochran-Armitage test for trend will determine if there is association between a binary response variable and an ordinal variable with 3 or more categories\n\n \n\nIt will test the trend of the proportions over the ordinal variable\n\nAnswers the question: Does the proportion of people with a “successful” outcome increase as the ordinal explanatory variable increases?\n\n\n \n\nCochran-Armitage test for trend is only suitable for 2 x C contingency tables"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-hypothesis-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Hypothesis Test",
    "text": "Cochran-Armitage test: Hypothesis Test\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThe proportions of successes are the same across all C ordinal values of the explanatory variable. \\[p_1 = p_2 = ... = p_C\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThe proportions of successes tend to increase as ordinal value of the explanatory variable increases\n\\[p_1 \\leq p_2 \\leq ... \\leq p_C\\]\nOR\nThe proportions of successes tend to decrease as ordinal value of the explanatory variable increases\n\\[p_1 \\geq p_2 \\geq ... \\geq p_C\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (1/3)",
    "text": "Cochran-Armitage test: Example (1/3)\nWe are interested in learning the association between the development of breast cancer and age at first birth among people who have given birth"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (2/3)",
    "text": "Cochran-Armitage test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nCancer = c(320, 1206, 1011, 463, 220)\nNo_Cancer = c(1422, 4432, 2893, 1092, 406)\nbscancer = matrix (c(Cancer, No_Cancer), nrow = 2, byrow = T)\nrownames(bscancer) = c(\"Cancer\",\"No Cancer\")\ncolnames(bscancer) = c(\"&lt;20\",\"20-24\",\"25-29\",\"30-34\",\"&gt;=35\")\nbscancer\n\n           &lt;20 20-24 25-29 30-34 &gt;=35\nCancer     320  1206  1011   463  220\nNo Cancer 1422  4432  2893  1092  406\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Example (3/3)",
    "text": "Cochran-Armitage test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): The proportions of breast cancer are the same for all age levels of first birth. \\[p_1 = p_2 = ... = p_5\\]\n\\(H_1\\): The proportions of breast cancer tends to increase as level of age of first birth increases\n\n\n\\[p_1 \\leq p_2 \\leq ... \\leq p_5\\]\n\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\n\nlibrary(DescTools)\nCochranArmitageTest(bscancer)\n\n\n    Cochran-Armitage test for trend\n\ndata:  bscancer\nZ = 11.358, dim = 5, p-value &lt; 2.2e-16\nalternative hypothesis: two.sided\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that proportions of breast cancer are the same for all age levels of first birth (\\(p&lt;2.2\\cdot10^{-16}\\)). There is sufficient evidence that the proportion of of breast cancer increase as the the age at first birth increases."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#cochran-armitage-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Cochran-Armitage test: Process",
    "text": "Cochran-Armitage test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Cochran-Armitage test in R\n\nWe will not discuss the test statistic’s equation\nJust know it follows a Normal distribution\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test",
    "text": "Mantel-Haenszel test\n\nWhen both variables are ordinal, we can conduct Mantel-Haenszel test of trend for linear association\nMantel-Haenszel test for linear trend is suitable for any R x C contingency tables with two ordinal variables\nHypothesis test:\n\n\n\n\n\nNull Hypothesis (\\(H_0\\))\n\n\nThere is no correlation between the two variables \\[ \\rho = 0\\]\n\n\n\n\n\nAlternative Hypothesis (\\(H_1\\))\n\n\nThere is correlation between the two variables\n\\[ \\rho \\neq 0\\]"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-process",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Process",
    "text": "Mantel-Haenszel test: Process\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\nWe will not discuss the test statistic’s equation\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example",
    "text": "Mantel-Haenszel test: Example\nA water treatment company is studying water additives and investigating how they affect clothes washing. The treatments studies where no treatment (plain water), the standard treatment, and a double dose of the standard treatment, called super. Washability was measured as low, medium and high. Are levels of washability associated with treatment?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-1",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-1",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example",
    "text": "Mantel-Haenszel test: Example\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nwater = matrix (c(27, 14, 5, 10, 17, 26, 5, 12, 50), nrow = 3, byrow = T)\nrownames(water) = c(\"plain\",\"standard\",\"super\")\ncolnames(water) = c(\"low\",\"medium\",\"high\")\nwater\n\n         low medium high\nplain     27     14    5\nstandard  10     17   26\nsuper      5     12   50\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-2",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-2",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example",
    "text": "Mantel-Haenszel test: Example\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): Water treatment and washability are not correlated. \\[ \\rho = 0\\]\n\\(H_1\\): Water treatment and washability are correlated.\n\n\n\\[ \\rho \\neq 0\\]\n\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\n\nlibrary(DescTools)\nMHChisqTest(water)\n\n\n    Mantel-Haenszel Chi-Square\n\ndata:  water\nX-squared = 50.602, df = 1, p-value = 1.132e-12\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that there is no correlation between washability and water treatment (\\(p = 1.13 \\cdot 10^{-12} &lt; 0.05\\)). There is sufficient evidence that level of water treatment is associated with washability."
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-13",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (1/3)",
    "text": "Mantel-Haenszel test: Example (1/3)\nA water treatment company is studying water additives and investigating how they affect clothes washing (through measurements of abrasions, wearing, and color loss).\nThe treatments studies where no treatment (plain water), the standard treatment, and a double dose of the standard treatment, called super. Washability was measured as low, medium and high.\nAre levels of washability associated with treatment?"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-23",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (2/3)",
    "text": "Mantel-Haenszel test: Example (2/3)\n\nBefore we go into the hypothesis test procedure, we need to construct the contingency table in R\n\n\nwater = matrix (c(27, 14, 5, 10, 17, 26, 5, 12, 50), nrow = 3, byrow = T)\nrownames(water) = c(\"plain\",\"standard\",\"super\")\ncolnames(water) = c(\"low\",\"medium\",\"high\")\nwater\n\n         low medium high\nplain     27     14    5\nstandard  10     17   26\nsuper      5     12   50\n\n\n\nIt does not need to be pretty to use in function"
  },
  {
    "objectID": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "href": "lectures/02_Intro_CDA/02_Intro_CDA.html#mantel-haenszel-test-example-33",
    "title": "Lesson 2: Introduction to Categorical Data Analysis",
    "section": "Mantel-Haenszel test: Example (3/3)",
    "text": "Mantel-Haenszel test: Example (3/3)\n\n\n\n\\(\\alpha = 0.05\\)\nHypothesis test:\n\n\\(H_0\\): Water treatment and washability are not correlated. \\[ \\rho = 0\\]\n\\(H_1\\): Water treatment and washability are correlated.\n\n\n\\[ \\rho \\neq 0\\]\n\n\nCalculate the test statistic and p-value for Mantel-Haenszel test in R\n\n\nlibrary(DescTools)\nMHChisqTest(water)\n\n\n    Mantel-Haenszel Chi-Square\n\ndata:  water\nX-squared = 50.602, df = 1, p-value = 1.132e-12\n\n\n\nConclusion to the hypothesis test\n\nWe reject the null hypothesis that there is no correlation between washability and water treatment (\\(p = 1.13 \\cdot 10^{-12} &lt; 0.05\\)). There is sufficient evidence that level of water treatment is associated with washability."
  },
  {
    "objectID": "weeks.html",
    "href": "weeks.html",
    "title": "Weekly Pages",
    "section": "",
    "text": "Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nDescription\n\n\n\n\n\n\n4/1/24\n\n\nWeek 1\n\n\nIntroduction; File Organization\n\n\n\n\n4/8/24\n\n\nWeek 2\n\n\nMeasurements of Association and Agreement\n\n\n\n\n4/15/24\n\n\nWeek 3\n\n\nIntroduction to Logistic Regression\n\n\n\n\n4/22/24\n\n\nWeek 4\n\n\nPrediction, Visualization, and Interpretations\n\n\n\n\n4/29/24\n\n\nWeek 5\n\n\nMissing Data\n\n\n\n\n5/6/24\n\n\nWeek 6\n\n\nConfounders and Interactions\n\n\n\n\n5/13/24\n\n\nWeek 7\n\n\nAssessing Model Fit\n\n\n\n\n5/20/24\n\n\nWeek 8\n\n\nModel Diagnostics and Model Building\n\n\n\n\n5/27/24\n\n\nWeek 9\n\n\nFlex week\n\n\n\n\n6/3/24\n\n\nWeek 10\n\n\nOther generalized linear regressions\n\n\n\n\n6/10/24\n\n\nWeek 11\n\n\nSpill-Over Week\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weeks/week_03_sched.html#resources",
    "href": "weeks/week_03_sched.html#resources",
    "title": "Week 3",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n5\nSimple Logistic Regression\n\n\n\n\n\n6\nTests for GLMs using Likelihood function\n\n\n\n\n\n\nIf you ever have trouble with the above links to the videos, this Echo360 link should take you to our class page."
  },
  {
    "objectID": "weeks/week_03_sched.html#on-the-horizon",
    "href": "weeks/week_03_sched.html#on-the-horizon",
    "title": "Week 3",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 1 due this Thursday (4/18)\nQuiz 1 opens on Monday, 4/22, at 2pm and will close on Wednesday, 4/24, at 1pm"
  },
  {
    "objectID": "weeks/week_03_sched.html#class-exit-tickets",
    "href": "weeks/week_03_sched.html#class-exit-tickets",
    "title": "Week 3",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/15)\n Wednesday (4/17)"
  },
  {
    "objectID": "weeks/week_03_sched.html#announcements",
    "href": "weeks/week_03_sched.html#announcements",
    "title": "Week 3",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 4/15\n\nI am trying to stay on track of the Exit tickets this quarter\n\nThat may mean you have a 0 in your gradebook\nAs long as you complete the exit ticket within the 7 days, I will change the 0 to a 1\nI plan to have a scheduled block on Fridays to check them\n\n\n\n\nWednesday 4/17\n\nQuiz 1 info"
  },
  {
    "objectID": "weeks/week_03_sched.html#muddiest-points",
    "href": "weeks/week_03_sched.html#muddiest-points",
    "title": "Week 3",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Not entirely sure I understand what IRLS is about\nFair enough. It’s a little confusing. IWLS is an iterative solving technique that let’s us solve the coefficient estimates ( \\(\\beta_0\\) , \\(\\beta_1\\)) without solving the equations theoretically.\nWe start with an educated guess of the estimates, put them into the likelihood, and calculate the likelihood. Then we update the estimates using some complicated math, put them into the likelihood, and calculate the likelihood again. We compare the two likelihoods, and if the likelihood increases, then we keep going. We stop when the increase in likelihood between iterations is small. This means we are at or very close to the maximum likelihood.\n\n\n2. Link functions\nYes! Link functions are the important transformations we need to make to our outcome in order to connect them to our perdictors/covariates. Specifically, it’s the transformation we make to our mean/expected value.\nThe same link function can be used different types of outcomes. And here’s a few examples:\n\nContinuous data: identity\nBinary: logit, log\nCount/Poisson: log\n\nOur goal with link functions is to put our outcome on a flexible range so that any range of covariates can be mapped to it with coefficients. So think about trying to map age onto a 0 or 1… We can’t come up with an equation like \\(\\beta_0 + \\beta_1 Age\\) that perfectly maps to only 0’s and 1’s.\n\n\n3. Is GLM the umbrella over the other functions? The 4 functions all use different distributions, yes?\nGLM is the umbrella term for different types of regression! Not all types of regression have different outcome distributions. For example, a binary outcome can be used in logistic regression with the logit link or log-binomial regression with the log link.\n\n\n4. What would you need to change in your model to reduce a high IRLS number? As I understand it from the lecture, a high number suggests convergence but it appeared like something unfavorable even though a model that converges might be closer to maximum likelihood or maybe the distance to maximum likelihood\nA high number suggests that the model did NOT converge! Thus, we did not land on an estimate close to our maximum likelihood. You can think of the IRLS number as the number of iterations it is taking to find the maximum likelihood estimate (MLE). If it takes too many iterations, then it just stops without finding the MLE.\n\n\n5. We’re using linear vs logistic, but which are we focusing on? Regarding linear, how does linear used in categorical differ from continuous?\nWe are focusing on logistic! We cannot use linear regression on our binary outcomes anymore. When I say “linear” mapping I mean the mapping between our covariates and the transformed mean outcome using the link function.\n\n\n6. By the end of class (Lesson 6) my understanding is that the saturated model likelihood is the same between the two models being compared, right?\nYep!!\n\n\n7. The differences between each test and when to use them.\nIn terms of what each test is measuring:\n\nThe Wald test measures the distance between two potential values of \\(\\beta\\). One under the null and one under the alternative. The further they are from each other, the more evidence we have that they are different.\n\nThe Wald test approximates the differences in the likelihood function, but we do not actually compare the likelihoods under the null vs. alternative. We are only comparing the difference in the \\(\\beta\\) value, that is a reasonable approximation of the difference in the likelihood.\n\nThe Score test measures how close the tangent line of the likelihood function is to 0 (under the null). If it is close to 0 under the null, this indicates that our MLE of \\(\\beta\\) is not far from 0. Again, this is no a direct comparison of the likelihoods, but only an approximation of the difference.\nThe likelihood ratio test measures the difference in the log-likelihoods. This is a direct comparison of likelihoods, and is not an approximation!\n\nThus, we compare the likelihoods (horizontally, as someone asked) because we are making direct comparisons between the likelihood under the null and under the alternative."
  },
  {
    "objectID": "weeks/week_04_sched.html#resources",
    "href": "weeks/week_04_sched.html#resources",
    "title": "Week 4",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n7\nPredictions and Visualizations in Simple Logistic Regression\n\n\n\n\n\n8\nInterpretations and Visualizations of Odds Ratios"
  },
  {
    "objectID": "weeks/week_04_sched.html#on-the-horizon",
    "href": "weeks/week_04_sched.html#on-the-horizon",
    "title": "Week 4",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 2 due this Thursday"
  },
  {
    "objectID": "weeks/week_04_sched.html#class-exit-tickets",
    "href": "weeks/week_04_sched.html#class-exit-tickets",
    "title": "Week 4",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (4/22)\n\nMonday Exit ticket will not be graded bc of quiz\n\n Wednesday (4/24)"
  },
  {
    "objectID": "weeks/week_04_sched.html#announcements",
    "href": "weeks/week_04_sched.html#announcements",
    "title": "Week 4",
    "section": "Announcements",
    "text": "Announcements\nWednesday 4/24\n\nHave you all seen this??? Page on Basic Needs for students\n\nSPH Emergency funds\nCARE program\nCommittee for Improving Student Food Security\n\nLab 2 is up!\n\nFrom Lab 1:\n\nDO NOT USE ANY_HARDSHIP or MULT_HARDSHIP as your main variable\n\nThese are constructed from food insecurity variable\nSee the User Guide in the downloaded ICPSR folder\n\n\n\nQuiz 1 should be in!\nLab 1 feedback still in progress\nReview last quarter’s project\n\nOn Monday we will take 15 minutes to discuss changes to the project report instructions\nI will bring the learning objectives that I want to assess\nWe can rework or scrap parts of the report that do not assess these learning objectives\nAs part of the exit ticket, I will ask about your preferences as well"
  },
  {
    "objectID": "weeks/week_04_sched.html#muddiest-points",
    "href": "weeks/week_04_sched.html#muddiest-points",
    "title": "Week 4",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nNone?? Wowza!"
  },
  {
    "objectID": "weeks/week_05_sched.html",
    "href": "weeks/week_05_sched.html",
    "title": "Week 5",
    "section": "",
    "text": "MONDAY CLASS CANCELLED\n\n\n\nMonday’s class is cancelled! We will be back on Wednesday for a fun, random lecture on missing data!!"
  },
  {
    "objectID": "weeks/week_05_sched.html#resources",
    "href": "weeks/week_05_sched.html#resources",
    "title": "Week 5",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n9\nMissing Data\nActivity:"
  },
  {
    "objectID": "weeks/week_05_sched.html#on-the-horizon",
    "href": "weeks/week_05_sched.html#on-the-horizon",
    "title": "Week 5",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nLab 2 due 5/2 at 11pm\nHW 3 is up and due 5/9 at 11pm!"
  },
  {
    "objectID": "weeks/week_05_sched.html#class-exit-tickets",
    "href": "weeks/week_05_sched.html#class-exit-tickets",
    "title": "Week 5",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Wednesday (5/1)"
  },
  {
    "objectID": "weeks/week_05_sched.html#announcements",
    "href": "weeks/week_05_sched.html#announcements",
    "title": "Week 5",
    "section": "Announcements",
    "text": "Announcements\n\nWednesday 5/1\n\nQuiz 1 grades are up!\nSPH Student survey still going until 5/3!!\n\nIf you already did it, then yay!\nRaffling 2 iPads!\n\nOur mid quarter survey is up!\n\nDUE 5/9 at 11pm!!"
  },
  {
    "objectID": "weeks/week_05_sched.html#muddiest-points",
    "href": "weeks/week_05_sched.html#muddiest-points",
    "title": "Week 5",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nI couldn’t resist making a gif of me reacting to the fire alarm when I was editing the Echo360 recording."
  },
  {
    "objectID": "weeks/week_06_sched.html#resources",
    "href": "weeks/week_06_sched.html#resources",
    "title": "Week 6",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n10\nMultiple Logistic Regression\n\n\n\n\n\n11\nInteractions"
  },
  {
    "objectID": "weeks/week_06_sched.html#on-the-horizon",
    "href": "weeks/week_06_sched.html#on-the-horizon",
    "title": "Week 6",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nHomework 3 due 5/9 at 11pm\nMid-quarter feedback due 5/9 at 11pm"
  },
  {
    "objectID": "weeks/week_06_sched.html#class-exit-tickets",
    "href": "weeks/week_06_sched.html#class-exit-tickets",
    "title": "Week 6",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/6)\n Wednesday (5/8)"
  },
  {
    "objectID": "weeks/week_06_sched.html#announcements",
    "href": "weeks/week_06_sched.html#announcements",
    "title": "Week 6",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/6\n\nNicky’s office hours move to 3-4pm on Wednesdays\n\nI will stay after class and open Webex\n\nNotes on Homework 2\n\nContingency tables\n\nRev=”b”\n\nKnow the appropriate rev option\nriskratio() and oddsratio() reads first row as reference and first column as reference.\n\nQuestion 3 part h\n\nBoth a and b are correct\nMost ppl only marked one or the other\n\n\nWays to change outcome into factor or appropriate form for glm()\n\nicu$STA &lt;- ifelse(icu$STA==\"Died\",1,0)\nOR: icu = icu %&gt;% mutate(STA = as.factor(STA) %\\&gt;% relevel(ref = “Lived”))\n\nQuestion 4, part d\n\nTest for intercept wrong\nUsed coefficient for age\n\n\n\n\n\nWednesday 5/8\n\nLab 3 is up!"
  },
  {
    "objectID": "weeks/week_06_sched.html#muddiest-points",
    "href": "weeks/week_06_sched.html#muddiest-points",
    "title": "Week 6",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. Why use logistic instead of linear regression?\nFrom waaay back in our slides from Lesson 5! We violate several of the assumptions for linear regression when our outcome has only two options. One of the most important reasons is that we cannot map our predictors (that can be continuous of categorical) onto a binary outcome. We need to transform our binary outcome so it is on a continuous and unbounded scale (logit does that!)\n\n\n2. Where can we find more resources for making forest plots?\nYou can either use the code I gave in Lesson 10 or use the reference links from Lab 4 in Linear Models.\n\n\n3. And how do we make a likelihood probability table instead of a plot?\nI’m not sure what this means. Please post on Slack so we can clarify! We have discussed plotting predicted probability in Lesson 7.\n\n\n4. I am confused on why we would do the odds ratio of prior fracture vs no prior fracture (in Lesson 11).\nThe odds ratio is mostly used to compare the two cases using one value. We want to compare the odds of a new fracture. We want to see how those odds of a new fracture change when we have a prior fracture or when we do not have a prior fracture. We can calculate the odds ratio for prior fracture to do that. When we only have main effects in our model, this is easier to calculate. We only have one odds ratio. However, when we have an interaction between prior fracture and age, we need a way to demonstrate how the odds ratio for prior fracture changes with age.\n\n\n5. I’m still confused about the difference between Model 3 and the estimated odd ratio table on slide 39 and what each is telling us.\nAh, yes! I’ll clarify a little more in the slide. The first table includes the coefficient estimates (\\(\\widehat\\beta_1\\),\\(\\widehat\\beta_2\\), \\(\\widehat\\beta_3\\)), and the second table includes the odds ratios (\\(\\exp(\\widehat\\beta_1)\\),\\(\\exp(\\widehat\\beta_2)\\), \\(\\exp(\\widehat\\beta_3)\\))"
  },
  {
    "objectID": "weeks/week_11_sched.html#announcements",
    "href": "weeks/week_11_sched.html#announcements",
    "title": "Week 11",
    "section": "Announcements",
    "text": "Announcements"
  },
  {
    "objectID": "weeks/week_10_sched.html#resources",
    "href": "weeks/week_10_sched.html#resources",
    "title": "Week 10",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n16\nPoisson Regression\n\n\n\n\n\n17\nOther types of categorical regressions!"
  },
  {
    "objectID": "weeks/week_10_sched.html#on-the-horizon",
    "href": "weeks/week_10_sched.html#on-the-horizon",
    "title": "Week 10",
    "section": "On the Horizon",
    "text": "On the Horizon\n\nQuiz 3 open 6/3 at 2pm\n\nCloses on 6/5 at 1pm\n\nLab 4 due yesterday!\nHW 5 due 6/6 at 11pm\nProject report due 6/13 at 11pm"
  },
  {
    "objectID": "weeks/week_10_sched.html#announcements",
    "href": "weeks/week_10_sched.html#announcements",
    "title": "Week 10",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 6/3\n\nIn the last stretch of the project\n\nClass time next week dedicated to project report help\nI may have other time slots for project help next week. I just need to take a serious look at my calendar\n\nI’m attending a virtual stat ed conference, so I just need to balance meetings with conference attendance\n\n\nNo office hours this Wednesday 6/5\n#3 “The Last Bounce: Bunnies, Burritos, and DIY Bath Salts”\n\nWednesday, June 5th Noon-2pm\nStudent Success Center, 6th floor Vanport\n\n\n\n\nWednesday 6/5\n\nLast day of lecture!!\nA word on project grading\n\nIn the final lab, I gave you the option to do LASSO regression and focus on prediction. I know this created some confusion since we mostly set up the project as a question of association in Lab 1-3. We can still interpret the odds ratios from LASSO regression. I will be fairly lenient if reports are confused between prediction and association aims. I will try to give feedback on it, but I will not penalize any minor confusions.\nAnother word: My process starts harsh. I want to give you as much feedback as possible, and this will also reflect in lower assigned scores. At this point, I put the report grades into Sakai. I check to see if anyone’s overall course letter grade goes down. If less than ~5 course grades go down, then I revisit their project reports. If their report fails to demonstrate the most important learning objectives from the course, then I will keep the lower grade. If they demonstrate an understanding of the most important learning objectives, then I will adjust their score to increase their course grade. If more than 5 grades go down, then I revisit everyone’s reports. I will make a class wide grade bump in all reports.\nThe most important learning objectives are: understanding when and what test is appropriate, and interpreting odds ratios (from main effects and interactions)\n\nProject: LASSO\n\nYou can take the finalized formula for LASSO and use it in glm()\n\nIn this case, use the test data to come up with predictive values (like AUC)\nYou can run it on the full dataset to get the coefficient estimates and other diagnostic information\n\n\nGuide on figures"
  },
  {
    "objectID": "weeks/week_10_sched.html#class-exit-tickets",
    "href": "weeks/week_10_sched.html#class-exit-tickets",
    "title": "Week 10",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (6/3)\n Wednesday (6/5)"
  },
  {
    "objectID": "weeks/week_10_sched.html#muddiest-points",
    "href": "weeks/week_10_sched.html#muddiest-points",
    "title": "Week 10",
    "section": "Muddiest Points",
    "text": "Muddiest Points"
  },
  {
    "objectID": "weeks/week_07_sched.html#resources",
    "href": "weeks/week_07_sched.html#resources",
    "title": "Week 7",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n12\nAssessing Model Fit\n\n\n\n\n\n13\nNumerical Problems\n\npdf on github"
  },
  {
    "objectID": "weeks/week_07_sched.html#on-the-horizon",
    "href": "weeks/week_07_sched.html#on-the-horizon",
    "title": "Week 7",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "weeks/week_07_sched.html#class-exit-tickets",
    "href": "weeks/week_07_sched.html#class-exit-tickets",
    "title": "Week 7",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/13)\nQuiz starts on Wednesday (5/15)"
  },
  {
    "objectID": "weeks/week_07_sched.html#announcements",
    "href": "weeks/week_07_sched.html#announcements",
    "title": "Week 7",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/13\n\nI will review our interaction notes from last time\nQuiz 2 opens on Wednesday at 2pm!!\n\nWill cover Lessons 5-9!!\nMostly on Lessons 5-8, with one question about concepts covered in Lesson 9 (like types of missing data…)\n\nThink about how we fit logistic regression and GLMs\nThink about link functions!\nThink about our tests (Wald, Score, LRT)\nThink about the different transformations between Y, probability, and logit!\nThink about what our predictions mean\nThink about interpretations within logistic regression.\n\n\nLab 2\n\nI noticed that a lot of you did not comment on the trends from the bivariate analysis\n\nThis is why I didn’t have us use ggpairs() last quarter. It’s too easy to just blow past this.\nGetting to know your data and the trends you see in the sample is incredibly important!!\nThe best way to identify issues with your model is to have a good understanding of your data and their trends\n\nMany people noticed small cell counts for income levels\n\nWe will address this issues in lecture this week!\n\nUnless you are removing food insecurity from any hardship or multiple hardships, do NOT use these variables!"
  },
  {
    "objectID": "weeks/week_07_sched.html#muddiest-points",
    "href": "weeks/week_07_sched.html#muddiest-points",
    "title": "Week 7",
    "section": "Muddiest Points",
    "text": "Muddiest Points\n\n1. How do we determine the number of covariate patterns in R?\nTheoretically, all you need to do is count the number of groups in each categorical covariates. To find the total number of covariate patterns, you multiple those numbers by each other.\nIn R, we can take a dataframe with only the predictors in your model. You can use the distinct() function to create unique rows. The number of rows outputted will be the number of covariate patterns."
  },
  {
    "objectID": "weeks/week_08_sched.html#resources",
    "href": "weeks/week_08_sched.html#resources",
    "title": "Week 8",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n14\nModel Diagnostics\n\n\n\n\n\n15\nModel Building"
  },
  {
    "objectID": "weeks/week_08_sched.html#on-the-horizon",
    "href": "weeks/week_08_sched.html#on-the-horizon",
    "title": "Week 8",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "weeks/week_08_sched.html#announcements",
    "href": "weeks/week_08_sched.html#announcements",
    "title": "Week 8",
    "section": "Announcements",
    "text": "Announcements\n\nMonday 5/20\n\nHW 4 part d UPDATED!!\n\nHelp from Monday class on Part e\n\nHomework 3\n\nRemember to include the indicator function for different categories of your variables!!\nLOC has three levels: there should be two indicator functions and two coefficients for this variable!!\n\n\n\n\n\nWednesday 5/22\n\nLab 3\n\nWhen interpreting ORs…\n\nYou all are correct by including as much detail about the covariate as possible\n\nFor example: If I was using UNMETCARE_Y and I wrote “The estimated odds of food insecurity for individuals who needed medical care in the last 12 months but could not get it because they could not afford it…”\n\nThis is correct!\nBUT within our longer written report, we should define “unmet care” earlier on. Thus, once we get to interpreting ORs, we can just say “unmet care.”\n\n\nAlso, correct for including a list of the variables that you are adjusting for!\n\nBut again, we hopefully defined our final model and specifically mentioned the variables that we are adjusting for\nThus, once we get to our interpretation, we can say something more like “adjusting for the previously listed variables in our model”\n\n\nFor output of `tbl_regression()` make sure to edit the variable names into more common language\n\nset_variable_labels() from tibbleOne package might help!"
  },
  {
    "objectID": "weeks/week_08_sched.html#class-exit-tickets",
    "href": "weeks/week_08_sched.html#class-exit-tickets",
    "title": "Week 8",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Monday (5/20)\n Wednesday (5/22)"
  },
  {
    "objectID": "weeks/week_08_sched.html#muddiest-points",
    "href": "weeks/week_08_sched.html#muddiest-points",
    "title": "Week 8",
    "section": "Muddiest Points",
    "text": "Muddiest Points"
  },
  {
    "objectID": "weeks/week_09_sched.html#resources",
    "href": "weeks/week_09_sched.html#resources",
    "title": "Week 9",
    "section": "Resources",
    "text": "Resources\n\n\n\nLesson\nTopic\nSlides\nAnnotated Slides\nRecording\n\n\n\n\n16\nPoisson Regression"
  },
  {
    "objectID": "weeks/week_09_sched.html#on-the-horizon",
    "href": "weeks/week_09_sched.html#on-the-horizon",
    "title": "Week 9",
    "section": "On the Horizon",
    "text": "On the Horizon"
  },
  {
    "objectID": "weeks/week_09_sched.html#announcements",
    "href": "weeks/week_09_sched.html#announcements",
    "title": "Week 9",
    "section": "Announcements",
    "text": "Announcements\n\nWednesday 5/29\n\nSome student events coming up\n\n#1 “The Final Stretch: How to Find Motivation & Prioritize Tasks”\n\nJoin ONLINE on Wednesday, May 29th from 6PM-7PM to learn time-management tips and tricks.  \nRegister here and a Zoom link will be sent to you prior to the event: https://forms.gle/DkYYhdFXN7XtQ3YF8 \n\n#2 “Refuel for Finals”\n\nStudent Success Center, 6th floor Vanport\nJune 3rd-4th & 6th-7th  9am-5pm\n\n#3 “The Last Bounce: Bunnies, Burritos, and DIY Bath Salts”\n\nWednesday, June 5th Noon-2pm\nStudent Success Center, 6th floor Vanport\n\n\nHW 5 posted\nProject Report instructions finalized!!\nReview of Quiz 2\nQuiz 3 will open on Monday 6/3!!\n\nIt will cover Lesson 10-15"
  },
  {
    "objectID": "weeks/week_09_sched.html#class-exit-tickets",
    "href": "weeks/week_09_sched.html#class-exit-tickets",
    "title": "Week 9",
    "section": "Class Exit Tickets",
    "text": "Class Exit Tickets\n Wednesday (5/29)"
  },
  {
    "objectID": "weeks/week_09_sched.html#muddiest-points",
    "href": "weeks/week_09_sched.html#muddiest-points",
    "title": "Week 9",
    "section": "Muddiest Points",
    "text": "Muddiest Points\nThis will be filled in with your Exit Ticket responses."
  },
  {
    "objectID": "weeks/week_10_sched.html",
    "href": "weeks/week_10_sched.html",
    "title": "Week 10",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nOn Monday, 6/3, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1\nMake sure to remember your answer!! We’ll use this on Wednesday!"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association",
    "text": "Review of Test of Association\n\nLast week: learned some tests of association for contingency tables\n\n \n\nFor studies with two independent samples\n\nGeneral association\n\nChi-squared test\nFisher’s Exact test\n\nTest of trends\n\nCochran-Armitage test\nMantel-Haenszel test\n\n\n\n \n\nTest of association does not provide an effective measure of association"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#test-of-association-does-not-measure-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Test of association does not measure association",
    "text": "Test of association does not measure association\n\nTest of association does not provide an effective measure of association. The p-value alone is not enough\n\n\\(\\text{p-value} &lt; 0.05\\) suggests there is a statistically significant association between the group and outcome\n\\(\\text{p-value} = 0.00001\\) vs. \\(\\text{p-value} = 0.01\\) does not mean the magnistude of association is different\n\n\n \n\nBut it does not tell how different the risks are between the two groups\n\n \n\nWe want to find out one or more measurements for quantifying the risks across the groups."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measures-of-association",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measures of Association",
    "text": "Measures of Association\n\nWhen we have a 2x2 contingency table and independent samples, we have an option of three measures of association:\n \n\nRisk difference (RD)\n\n \n\nRelative risk (RR)\n\n \n\nOdds ratio (OR)\n\n\n \nEach measures association by comparing the proportion of successes/failures from each categorical group of our explanatory variable."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#before-we-discuss-each-further",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Before we discuss each further…",
    "text": "Before we discuss each further…\n \nLet’s define the cells within a 2x2 contingency table:\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThen we can define risk: the proportion of “successes”\n\nWith \\(\\text{Risk}_1 = \\dfrac{n_{11}}{n_1}\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Risk Difference",
    "text": "Risk Difference\n\nRisk difference computes the absolute difference in risk for the two groups (from the explanatory variable)\nPoint estimate: \\[\\widehat{RD} = \\widehat{p}_1 - \\widehat{p}_1\\]\n\nWith range of point estimate from \\([-1, 1]\\)\n\nApproximate standard error:\n\n\\[ SE_{\\widehat{RD}} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n95% Wald confidence interval for \\(\\widehat{RD}\\):\n\n\\[\\widehat{RD} \\pm 1.96 \\cdot SE_{\\widehat{RD}}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-example",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-example",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Risk Difference Example",
    "text": "Risk Difference Example"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#when-is-the-risk-difference-misleading",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "When is the risk difference misleading?",
    "text": "When is the risk difference misleading?\n\nThe same risk differences can have very different clinical meanings depending on the risk for each group\n\n \n \n\nExample: for two treatments A and B, we know the risk difference (RD) is 0.009. Is it a meaningful difference?\n\nIf the risk is 0.01 for Trt A and 0.001 for Trt B?\nIf the risk is 0.41 for Trt A and 0.401 for Trt B?\n\n\n \n\nUsing the RD alone to summarize the difference in risks for comparing the two groups can be misleading\n\nThe ratio of risk can provide an informative descriptive measure of the “relative risk”"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nRelative risk computes the ratio of each group’s proportions of “success”\n\nAlso called risk ratio    \n\nPoint estimate: \\[\\widehat{RR}=\\dfrac{\\hat{p}_1}{\\hat{p}_2} = \\dfrac{n_{11}/n_1}{n_{21}/n_2}\\]\n\nRange: \\([0, \\infty]\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-rr",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of RR",
    "text": "Log-transformation of RR\n\nSampling distribution of the relative risk is highly skewed unless sample sizes are quite large\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed RR\nThen we convert back to the RR\n\nWe take the log (natural log) of RR: \\(\\ln(\\widehat{RR})\\) or \\(log(\\widehat{RR})\\)\n\nWhenever I say “log” I mean natural log (very common in statistics)\n\n\n\n\n\nThen we need to find approximate standard error for \\(\\ln(\\widehat{RR})\\) \\[SE_{\\ln(\\widehat{RR})}=\\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\]\n95% confidence interval for \\(\\ln(\\widehat{RR})\\): \\[\\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-rr-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the RR scale?",
    "text": "How do we get back to the RR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{RR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}, \\ \\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}}, \\ e^{\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big(\\ln(\\widehat{RR}) - 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big), \\ \\exp\\big(\\ln(\\widehat{RR}) + 1.96 \\times SE_{\\ln(\\widehat{RR})}\\big)\\bigg)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR)",
    "text": "Relative Risk (RR)\n\nCan you compute the estimated RRs for the previous example?\n\nIf the risk for Trt A is 0.01 and Trt B is 0.001? \\(\\widehat{RR}= 10\\)\nIf the risk for Trt A is 0.41 and Trt B is 0.401? \\(\\widehat{RR}= 1.02\\)\n\nWhen \\(\\widehat{RR}= 1\\) …\n\nRisk is the same for the two groups\nIn other words, the group and the outcome are independent\n\nWhen computing \\(\\widehat{RR}\\) it is important to identify which variable is the response variable and which is explanatory variable\n\nWe may say “risk for Trt A” but this translates to the risk (or probability) of outcome success for those receiving Trt A"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-example",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relative-risk-rr-example",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relative Risk (RR) Example",
    "text": "Relative Risk (RR) Example\n\nlibrary(pubh)\ncontingency(case ~ glucimp, data = SHS)\n\n          Outcome\nPredictor     1    0\n  Normal    128 1004\n  Impaired  198  334\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          128         1004       1132      11.31 (9.52 to 13.30)\nExposed -          198          334        532     37.22 (33.10 to 41.48)\nTotal              326         1338       1664     19.59 (17.71 to 21.58)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 0.30 (0.25, 0.37)\nInc odds ratio                                 0.22 (0.17, 0.28)\nAttrib risk in the exposed *                   -25.91 (-30.41, -21.41)\nAttrib fraction in the exposed (%)            -229.15 (-300.81, -170.30)\nAttrib risk in the population *                -17.63 (-22.16, -13.10)\nAttrib fraction in the population (%)         -89.97 (-106.38, -74.87)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-building-up-to-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds (building up to Odds Ratio)",
    "text": "Odds (building up to Odds Ratio)\n\nFor a probability of success \\(p\\) (or sometimes referred to as \\(\\pi\\)), the odds of success is: \\[\\text{odds}=\\frac{p}{1-p}=\\frac{\\pi}{1-\\pi}\\]\n\nExample: if \\(\\pi=0.75\\), then odds of success \\(= \\dfrac{0.75}{0.25}=3\\)\n\nIf odds &gt; 1, it implies a success is more likely than a failure\n\nExample: for \\(odds = 3\\), we expect to observe three times as many successes as failures\n\nIf odds is known, the probability of success can be computed \\[\\pi = \\dfrac{\\text{odds}}{\\text{odds}+1}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nOdds ratio is the ratio of two odds:\\[\\widehat{OR}=\\frac{odds_1}{odds_2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}\\]\n\nRange: \\([0, \\infty]\\)\nInterpretation: The odds of success for “group 1” is “\\(\\widehat{OR}\\)” times the odds of success for “group 2”\n\n\n \n\nWhat do values of odds ratios mean?\n\n\n\n\n\n\n\nOdds Ratio\nClinical Meaning\n\n\n\n\n\\(\\widehat{OR} &lt; 1\\)\nOdds of success is smaller in group 1 than in group 2\n\n\n\\(\\widehat{OR} = 1\\)\nExplanatory and response variables are independent\n\n\n\\(\\widehat{OR} &gt; 1\\)\nOdds of success is greater in group 1 than in group 2"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-3",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR)",
    "text": "Odds Ratio (OR)\n\nValues of OR farther from 1.0 in a given direction represent stronger association\n\nAn OR = 4 is farther from independence than an OR = 2\nAn OR = 0.25 is farther from independence than an OR = 0.5\nFor OR = 4 and OR = 0.25, they are equally away from independence (because ¼ = 0.25)\n\n\n \n\nWe take the inverse of the OR for success of group 1 compared to group 2 to get…\n\nOR for failure of group 1 compared to group 2\nOR for success of group 2 compared to group 1"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#log-transformation-of-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Log-transformation of OR",
    "text": "Log-transformation of OR\n\nLike RR, sampling distribution of the odds ratio is highly skewed\n\nLog transformation results in approximately normal distribution\nThus, compute confidence interval using normally distributed, log-transformed OR\n\n\n\n\n\nApproximate standard error for \\(\\ln (\\widehat{OR})\\): \\[SE_{\\ln(\\widehat{OR})}=\\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\]\n95% confidence interval for \\(\\ln(\\widehat{OR})\\): \\[\\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#how-do-we-get-back-to-the-or-scale",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "How do we get back to the OR scale?",
    "text": "How do we get back to the OR scale?\n\nWe computed confidence interval using normally distributed, log-transformed RR (\\(\\ln(\\widehat{OR})\\)):\n\n\\[ \\bigg(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}, \\ \\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\bigg)\\]\n\nNow we need to exponentiate the CI to get back to interpretable values\n\nTake exponential of lower and upper bounds\n\n95% confidence interval for RR: two ways to display equation\n\n\\[ \\bigg(e^{\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}}, \\ e^{\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}}\\bigg)\\] \\[ \\bigg(\\exp\\big(\\ln(\\widehat{OR}) - 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big), \\ \\exp\\big(\\ln(\\widehat{OR}) + 1.96 \\times SE_{\\ln(\\widehat{OR})}\\big)\\bigg)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-example",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#odds-ratio-or-example",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Odds Ratio (OR) Example",
    "text": "Odds Ratio (OR) Example"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-4",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relationship-between-rr-and-or",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relationship-between-rr-and-or",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relationship Between RR and OR",
    "text": "Relationship Between RR and OR\n\nNotice that odds ratio is not equivalent to relative risk (or risk ratio)\nHowever, when the probability of “success” is small (e.g., rare disease), \\(\\widehat{OR}\\) is a nice approximation of \\(\\widehat{RR}\\) \\[\\widehat{OR}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}=\\widehat{RR}\\cdot \\frac{1-\\widehat{p_2}}{1-\\widehat{p_1}}\\]\n\nThe fraction in the last term of the above expression approximately equals to 1.0 if \\(\\widehat{p}_1\\) and \\(\\widehat{p}_2\\) BOTH quite small (&lt; 0.1)\n\nThe \\(\\widehat{OR}\\) and \\(\\widehat{RR}\\) are not very close to each other in SHS: diabetes not a rare disease\n\n\\(\\widehat{OR} = 4.65\\)\n\\(\\widehat{RR} = 3.29\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#notes-for-odds-ratios",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#notes-for-odds-ratios",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Notes for Odds Ratios",
    "text": "Notes for Odds Ratios\n\nThe OR is valid for\n\nCase-control studies (where the RR is not appropriate)\nProspective cohort studies\nCross-sectional studies\n\nIt can be interpreted either as…\n\nOdds of event for exposed vs. unexposed individuals, or\nOdds of exposure for individuals with vs. without the event of interest\n\nPay attention to the numerator and denominator for the OR"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#rr-in-retrospective-case-control-study",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#rr-in-retrospective-case-control-study",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "RR in retrospective case-control study",
    "text": "RR in retrospective case-control study\n\nIn retrospective, case-control studies, we often identify cases (patients with the outcome), then select a number of controls (patients without the outcome)\n\nCase-control study to require much smaller sample size than do equivalent cohort studies\n\nHowever, the proportion of cases in the sample does not represent the proportion of cases in the population\n\nRR compares probability of the outcome (case) for exposed and unexposed groups\nNumber of outcomes has been artificially inflated for retrospective study"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#or-in-retrospective-case-control-study",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#or-in-retrospective-case-control-study",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "OR in retrospective case-control study",
    "text": "OR in retrospective case-control study\n\nWhile we cannot estimate RR from a case-control study, we can still estimate OR for case-control study\n\nOR does not require us to distinguish between the outcome variable and explanatory variable in the contingency table\n\nAKA: Odds ratio of disease comparing exposed to not exposed is same as odds ratio of being exposed comparing diseased and not diseased\n\n\nFor case-control study where the probability of having outcome is small, the \\(\\widehat{OR}\\) is a nice approximation to \\(\\widehat{RR}\\)\n\nFor the 1:2 case-control table: \\(\\widehat{OR}=\\frac{40\\cdot160}{40\\cdot60} = 2.667\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#which-measurement-should-one-use",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#which-measurement-should-one-use",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Which measurement should one use?",
    "text": "Which measurement should one use?\n\n\n\n\n\n\n\nMeasurement\nPros and Cons\n\n\n\n\nRisk difference\n\nCan provide additional information, but can be misleading on its own\nNot the preferred measurement\n\n\n\nRisk ratio\n\nEasy to interpret because is a ratio of probabilities\nCannot use in retrospective, case-control studies\n\n\n\nOdds ratio\n\nAdequate for all studies\nGood estimate of RR for rare diseases\nMost preferred by statisticians because integrated into logistic regression"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-5",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-5",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measuring-agreement",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measuring-agreement",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measuring Agreement",
    "text": "Measuring Agreement\n\nStill within the realm of contingency tables\nWhat if we are NOT looking at the association between two variables?\nWhat if we want to look at the agreement between two things?\n\nAnswers of same subjects for same survey taken at different times\nTwo different radiologists’ assessment of the same X-ray\n\nCohen’s Kappa statistics: widely used as a measure of agreement\n\nExample: Reliability studies, interobserver agreement"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-beef-consumption-in-survey",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-beef-consumption-in-survey",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Example: Beef Consumption in Survey",
    "text": "Example: Beef Consumption in Survey\nA diet questionnaire was mailed to 537 female American nurses on two separate occasions several months apart. The questions asked included the quantities eaten of more than 100 separate food items. The data from the two surveys for the amount of beef consumption are presented in the below table. How can reproducibility of response for the beef-consumption data be quantified?"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measuring-agreement-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measuring-agreement-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measuring Agreement",
    "text": "Measuring Agreement\n\nIf perfect agreement among the two raters/surveys, we would expect nonzero entries only in the diagonal cells of the table\n\n \n\n\\(p_o\\) is the observed proportion of complete agreement (concordance)\n\\(p_E\\) is the expected proportion of complete agreement if the agreement is just due to chance\nIf the \\(p_o\\) is much greater than \\(p_E\\), then the agreement level is high. Otherwise, the agreement level is low\n\n \n\nCohen’s Kappa is based on the difference between \\(p_o\\) and \\(p_E\\): \\[\\hat{\\kappa}=\\frac{p_o-p_E}{1-p_E}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-beef-consumption-in-survey-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-beef-consumption-in-survey-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Example: Beef Consumption in Survey",
    "text": "Example: Beef Consumption in Survey"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-6",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-6",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 6",
    "text": "Poll Everywhere Question 6"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measuring-agreement-between-two-raters",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measuring-agreement-between-two-raters",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measuring Agreement Between Two Raters",
    "text": "Measuring Agreement Between Two Raters"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#testing-measuring-agreement",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#testing-measuring-agreement",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Testing Measuring Agreement",
    "text": "Testing Measuring Agreement"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-beef-consumption",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-beef-consumption",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Example: Beef Consumption",
    "text": "Example: Beef Consumption"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measurement-of-association-so-far",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measurement-of-association-so-far",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measurement of Association So Far",
    "text": "Measurement of Association So Far\n\nUsed contingency tables to test and measure association between two variables\n\nCategorical outcome variable (Y)\nOne categorical explanatory variable (X)\n\nSuch an association is called crude association\n\nNo adjustment for possible confounding factors\nAlso called marginal association\n\nBut we cannot expand analysis based on contingency tables past 3 variables\n\nWe can get into stratified contingency tables to bring in a 3rd variable\nBut I don’t think it’s worth it because regression can bring in (adjust for) many variables"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#confounding",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#confounding",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Confounding",
    "text": "Confounding"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measurement-of-association-for-three-way-tables",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measurement-of-association-for-three-way-tables",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measurement of Association for Three-way Tables",
    "text": "Measurement of Association for Three-way Tables"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-lung-cancer-and-drinking-status",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-lung-cancer-and-drinking-status",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Example: Lung cancer and drinking status",
    "text": "Example: Lung cancer and drinking status"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#types-of-confounders",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#types-of-confounders",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Types of confounders",
    "text": "Types of confounders"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-7",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-7",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 7",
    "text": "Poll Everywhere Question 7"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-scottish-health-heart-study",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-scottish-health-heart-study",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Example: Scottish Health Heart Study",
    "text": "Example: Scottish Health Heart Study"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#confounding-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#confounding-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Confounding",
    "text": "Confounding"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#adjusted-or-mantel-haenszel-method",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#adjusted-or-mantel-haenszel-method",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Adjusted OR: Mantel-Haenszel Method",
    "text": "Adjusted OR: Mantel-Haenszel Method"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#stratified-tables",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#stratified-tables",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Stratified tables",
    "text": "Stratified tables"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#adjusted-or-mantel-haenszel-method-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#adjusted-or-mantel-haenszel-method-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Adjusted OR: Mantel-Haenszel Method",
    "text": "Adjusted OR: Mantel-Haenszel Method"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-scottish-health-heart-study-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-scottish-health-heart-study-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Example: Scottish Health Heart Study",
    "text": "Example: Scottish Health Heart Study"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-7-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#poll-everywhere-question-7-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Poll Everywhere Question 7",
    "text": "Poll Everywhere Question 7"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#cochran-mantel-haenszel-cmh-test",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#cochran-mantel-haenszel-cmh-test",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Cochran-Mantel-Haenszel (CMH) Test",
    "text": "Cochran-Mantel-Haenszel (CMH) Test"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-scottish-health-heart-study-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#example-scottish-health-heart-study-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Example: Scottish Health Heart Study",
    "text": "Example: Scottish Health Heart Study"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#tests-from-last-class",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#tests-from-last-class",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Tests from last class",
    "text": "Tests from last class"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#section",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#section",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "1",
    "text": "1\n\nlibrary(pubh)\nct = mtcars |&gt;\n  contingency(as.factor(vs) ~ as.factor(am))\n\n         Outcome\nPredictor  1  0\n        1  7  6\n        0  7 12\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +            7            6         13     53.85 (25.13 to 80.78)\nExposed -            7           12         19     36.84 (16.29 to 61.64)\nTotal               14           18         32     43.75 (26.36 to 62.34)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 1.46 (0.67, 3.17)\nInc odds ratio                                 2.00 (0.48, 8.40)\nAttrib risk in the exposed *                   17.00 (-17.71, 51.71)\nAttrib fraction in the exposed (%)            31.58 (-48.44, 68.46)\nAttrib risk in the population *                6.91 (-20.77, 34.58)\nAttrib fraction in the population (%)         15.79 (-24.73, 43.15)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 0.907 Pr&gt;chi2 = 0.341\nFisher exact test that OR = 1: Pr&gt;chi2 = 0.473\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 0.34754, df = 1, p-value = 0.5555\n\nct$observed\n\n         Outcome\nPredictor  1  0\n        1  7  6\n        0  7 12"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#section-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#section-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "2",
    "text": "2\n\nmtcars\n\n\n\n\nmpgcyldisphpdratwtqsecvsamgearcarb\n\n21  6160  1103.9 2.6216.50144\n\n21  6160  1103.9 2.8817  0144\n\n22.84108  933.852.3218.61141\n\n21.46258  1103.083.2119.41031\n\n18.78360  1753.153.4417  0032\n\n18.16225  1052.763.4620.21031\n\n14.38360  2453.213.5715.80034\n\n24.44147  623.693.1920  1042\n\n22.84141  953.923.1522.91042\n\n19.26168  1233.923.4418.31044\n\n17.86168  1233.923.4418.91044\n\n16.48276  1803.074.0717.40033\n\n17.38276  1803.073.7317.60033\n\n15.28276  1803.073.7818  0033\n\n10.48472  2052.935.2518  0034\n\n10.48460  2153   5.4217.80034\n\n14.78440  2303.235.3417.40034\n\n32.4478.7664.082.2 19.51141\n\n30.4475.7524.931.6118.51142\n\n33.9471.1654.221.8319.91141\n\n21.54120  973.7 2.4620  1031\n\n15.58318  1502.763.5216.90032\n\n15.28304  1503.153.4417.30032\n\n13.38350  2453.733.8415.40034\n\n19.28400  1753.083.8517.10032\n\n27.3479  664.081.9418.91141\n\n26  4120  914.432.1416.70152\n\n30.4495.11133.771.5116.91152\n\n15.88351  2644.223.1714.50154\n\n19.76145  1753.622.7715.50156\n\n15  8301  3353.543.5714.60158\n\n21.44121  1094.112.7818.61142"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#recall-the-strong-heart-study",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Recall the Strong Heart Study",
    "text": "Recall the Strong Heart Study\n\n\nThe Strong Heart Study is an ongoing study of American Indians residing in 13 tribal communities in three geographic areas (AZ, OK, and SD/ND). We will look at data from this study examining the incidence of diabetes at a follow-up visit and impaired glucose tolerance (ITG) at baseline (4 years apart).\n \n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference",
    "text": "SHS Example: Risk Difference\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\nNeeded steps:\n\nCompute the risk difference\nCompute 95% confidence interval\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#risk-difference-rd",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Risk Difference (RD)",
    "text": "Risk Difference (RD)\n\nRisk difference computes the absolute difference in risk for the two groups (from the explanatory variable)\nPoint estimate: \\[\\widehat{RD} = \\widehat{p}_1 - \\widehat{p}_1 = \\dfrac{n_{11}}{n_1} - \\dfrac{n_{21}}{n_2}\\]\n\nWith range of point estimate from \\([-1, 1]\\)\n\nApproximate standard error:\n\n\\[ SE_{\\widehat{RD}} = \\sqrt{\\frac{\\hat{p}_1\\cdot(1-\\hat{p}_1)}{n_1} + \\frac{\\hat{p}_2\\cdot(1-\\hat{p}_2)}{n_2}}\\]\n\n95% Wald confidence interval for \\(\\widehat{RD}\\):\n\n\\[\\widehat{RD} \\pm 1.96 \\cdot SE_{\\widehat{RD}}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference",
    "text": "SHS Example: Risk Difference\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute 95% confidence interval\n\n\\[\\begin{aligned} &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times SE_{\\widehat{RD}} \\\\\n= &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{{\\hat{p}}_1\\ (1-{\\hat{p}}_1)}{n_1}+\\frac{{\\hat{p}}_2(1-{\\hat{p}}_2)}{n_2}\\ }\\\\\n=  & -0.025 \\pm 1.96\\times \\sqrt{\\frac{0.056(1-0.056)}{733}+\\frac{0.081(1-0.081)}{742}\\ }\\\\\n=  & (-0.0506,\\ 0.0008)\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-13",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-13",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (1/3)",
    "text": "SHS Example: Risk Difference (1/3)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute the risk difference \\[\\widehat{RD}={\\hat{p}}_1 - {\\hat{p}}_2=\\frac{n_{11}}{n_1}-\\frac{n_{21}}{n_2}=\\ \\frac{198}{532}\\ - \\frac{128}{1132}=0.3722−0.1131=0.2591\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-33",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-33",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (3/3)",
    "text": "SHS Example: Risk Difference (3/3)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n1/2. Compute risk difference and 95% confidence interval\n\nfmsb::riskdifference(198, 128, 532, 1132)\n\n                 Cases People at risk         Risk\nExposed    198.0000000    532.0000000    0.3721805\nUnexposed  128.0000000   1132.0000000    0.1130742\nTotal      326.0000000   1664.0000000    0.1959135\n\n\n\n    Risk difference and its significance probability (H0: The difference\n    equals to zero)\n\ndata:  198 128 532 1132\np-value &lt; 2.2e-16\n95 percent confidence interval:\n 0.2140779 0.3041346\nsample estimates:\n[1] 0.2591062"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (2/)",
    "text": "SHS Example: Risk Difference (2/)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute 95% confidence interval\n\n\\[\\begin{aligned} &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times SE_{\\widehat{RD}} \\\\\n= &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{{\\hat{p}}_1\\ (1-{\\hat{p}}_1)}{n_1}+\\frac{{\\hat{p}}_2(1-{\\hat{p}}_2)}{n_2}\\ }\\\\\n=  & 0.2591 \\pm 1.96\\times \\sqrt{\\frac{0.3722(1-0.3722)}{532}+\\frac{0.1131(1-0.1131)}{1132}\\ }\\\\\n=  & (0.2141,\\ 0.3041 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-33-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-33-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (3/3)",
    "text": "SHS Example: Risk Difference (3/3)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nInterpret the estimate\n\nThe diabetes diagnosis risk difference between impaired and normal glucose tolerance is 0.2591 (95% CI: 0.2141, 0.3041). Since the 95% confidence interval contains 0, we do not have sufficient evidence that the risk of diabetes diagnosis between impaired and normal glucose tolerance is different."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (1/4)",
    "text": "SHS Example: Risk Difference (1/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute the risk difference \\[\\widehat{RD}={\\hat{p}}_1 - {\\hat{p}}_2=\\frac{n_{11}}{n_1}-\\frac{n_{21}}{n_2}=\\ \\frac{198}{532}\\ - \\frac{128}{1132}=0.3722−0.1131=0.2591\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (2/4)",
    "text": "SHS Example: Risk Difference (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute 95% confidence interval\n\n\\[\\begin{aligned} &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times SE_{\\widehat{RD}} \\\\\n= &\\widehat{RD}\\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{{\\hat{p}}_1\\ (1-{\\hat{p}}_1)}{n_1}+\\frac{{\\hat{p}}_2(1-{\\hat{p}}_2)}{n_2}\\ }\\\\\n=  & 0.2591 \\pm 1.96\\times \\sqrt{\\frac{0.3722(1-0.3722)}{532}+\\frac{0.1131(1-0.1131)}{1132}\\ }\\\\\n=  & (0.2141,\\ 0.3041 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (3/4)",
    "text": "SHS Example: Risk Difference (3/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n1/2. Compute risk difference and 95% confidence interval\n\nfmsb::riskdifference(198, 128, 532, 1132)\n\n                 Cases People at risk         Risk\nExposed    198.0000000    532.0000000    0.3721805\nUnexposed  128.0000000   1132.0000000    0.1130742\nTotal      326.0000000   1664.0000000    0.1959135\n\n\n\n    Risk difference and its significance probability (H0: The difference\n    equals to zero)\n\ndata:  198 128 532 1132\np-value &lt; 2.2e-16\n95 percent confidence interval:\n 0.2140779 0.3041346\nsample estimates:\n[1] 0.2591062"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (4/4)",
    "text": "SHS Example: Risk Difference (4/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nInterpret the estimate\n\nThe diabetes diagnosis risk difference between impaired and normal glucose tolerance is 0.2591 (95% CI: 0.2141, 0.3041). Since the 95% confidence interval contains 0, we do not have sufficient evidence that the risk of diabetes diagnosis between impaired and normal glucose tolerance is different."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk",
    "text": "SHS Example: Relative Risk\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\nNeeded steps:\n\nCompute the relative risk\nFind confidence interval of log RR\nConvert back to RR\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-14-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (1/4)",
    "text": "SHS Example: Risk Difference (1/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute the relative risk \\[\\widehat{RR}=\\dfrac{{\\hat{p}}_1}{{\\hat{p}}_2}=\\dfrac{n_{11}/{n_1}}{n_{21}/{n_2}}=\\ \\frac{ 198/532}{128/1132}=\\dfrac{0.3722}{0.1131}=3.2915\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (2/4)",
    "text": "SHS Example: Risk Difference (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nFind confidence interval of log RR then convert back to RR\n\n\\[\\begin{aligned} & ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})} \\\\\n= &ln(\\widehat{RR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\\\\n=  & 1.1913 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ -\\frac{1}{532}+\\frac{1}{128}-\\frac{1}{1132}}\\\\\n=  & (0.2,\\ 0.3 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-34-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (3/4)",
    "text": "SHS Example: Risk Difference (3/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n1/2. Compute risk difference and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\nriskratio(x = SHS_ct, rev = \"rows\")$measure\n\n          risk ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 3.291471 2.702998 4.008061"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-44-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (4/4)",
    "text": "SHS Example: Risk Difference (4/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nInterpret the estimate\n\nThe diabetes diagnosis risk difference between impaired and normal glucose tolerance is 0.2591 (95% CI: 0.2141, 0.3041). Since the 95% confidence interval contains 0, we do not have sufficient evidence that the risk of diabetes diagnosis between impaired and normal glucose tolerance is different."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-difference-24-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Difference (2/4)",
    "text": "SHS Example: Risk Difference (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nFind confidence interval of log RR then convert back to RR\n\n\\[\\begin{aligned} & ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})} \\\\\n= &ln(\\widehat{RR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\\\\n=  & 1.1913 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ -\\frac{1}{532}+\\frac{1}{128}-\\frac{1}{1132}}\\\\\n=  & (0.2141,\\ 0.3041 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option",
    "text": "Pause: other option\n\nlibrary(pubh)\nSHS = SHS %&gt;% mutate(glucimp = as.factor(glucimp) %&gt;% relevel(ref = \"Normal\"))\ncontingency(case ~ glucimp, data = SHS)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532     37.22 (33.10 to 41.48)\nExposed -          128         1004       1132      11.31 (9.52 to 13.30)\nTotal              326         1338       1664     19.59 (17.71 to 21.58)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.29 (2.70, 4.01)\nInc odds ratio                                 4.65 (3.61, 6.00)\nAttrib risk in the exposed *                   25.91 (21.41, 30.41)\nAttrib fraction in the exposed (%)            69.62 (63.00, 75.05)\nAttrib risk in the population *                8.28 (5.63, 10.94)\nAttrib fraction in the population (%)         42.28 (34.71, 48.98)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-14",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-14",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Ratio (1/4)",
    "text": "SHS Example: Risk Ratio (1/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute the relative risk \\[\\widehat{RR}=\\dfrac{{\\hat{p}}_1}{{\\hat{p}}_2}=\\dfrac{n_{11}/{n_1}}{n_{21}/{n_2}}=\\ \\frac{ 198/532}{128/1132}=\\dfrac{0.3722}{0.1131}=3.2915\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-24",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-24",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Ratio (2/4)",
    "text": "SHS Example: Risk Ratio (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nFind confidence interval of log RR\n\n\\[\\begin{aligned} & ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})} \\\\\n= &ln(\\widehat{RR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\\\\n=  & 1.1913 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ -\\frac{1}{532}+\\frac{1}{128}-\\frac{1}{1132}}\\\\\n=  & (0.9944,\\ 1.3883 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-34",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-34",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Ratio (3/4)",
    "text": "SHS Example: Risk Ratio (3/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n1/2/3. Compute risk ratio and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\nriskratio(x = SHS_ct, rev = \"rows\")$measure\n\n          risk ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 3.291471 2.702998 4.008061"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\nSHS = SHS %&gt;% mutate(glucimp = as.factor(glucimp) %&gt;% relevel(ref = \"Normal\"))\ncontingency(case ~ glucimp, data = SHS)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532     37.22 (33.10 to 41.48)\nExposed -          128         1004       1132      11.31 (9.52 to 13.30)\nTotal              326         1338       1664     19.59 (17.71 to 21.58)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.29 (2.70, 4.01)\nInc odds ratio                                 4.65 (3.61, 6.00)\nAttrib risk in the exposed *                   25.91 (21.41, 30.41)\nAttrib fraction in the exposed (%)            69.62 (63.00, 75.05)\nAttrib risk in the population *                8.28 (5.63, 10.94)\nAttrib fraction in the population (%)         42.28 (34.71, 48.98)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-24-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-24-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Ratio (2/4)",
    "text": "SHS Example: Risk Ratio (2/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nConvert back to RR\n\n\\[\\begin{aligned} & (\\exp(0.9944),\\ \\exp(1.3883 )) \\\\\n= & (2.703,\\ 4.0081 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-44",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-risk-ratio-44",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Risk Ratio (4/4)",
    "text": "SHS Example: Risk Ratio (4/4)\n\n\n\n\nRisk difference\n\n\nCompute the point estimate and 95% confidence interval for the diabetes risk difference between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated risk of diabetes is 3.29 times greater for American Indians who had impaired glucose tolerance at baseline compared to those who had normal glucose tolerance (95% CI: 2.70, 4.01).\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the (population) relative risk is between 2.70 and 4.01.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the risk of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (1/6)",
    "text": "SHS Example: Relative Risk (1/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\nNeeded steps:\n\nCompute the relative risk\nFind confidence interval of log RR\nConvert back to RR\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (2/6)",
    "text": "SHS Example: Relative Risk (2/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute the relative risk \\[\\widehat{RR}=\\dfrac{{\\hat{p}}_1}{{\\hat{p}}_2}=\\dfrac{n_{11}/{n_1}}{n_{21}/{n_2}}=\\ \\frac{ 198/532}{128/1132}=\\dfrac{0.3722}{0.1131}=3.2915\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (3/6)",
    "text": "SHS Example: Relative Risk (3/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nFind confidence interval of log RR\n\n\\[\\begin{aligned} & \\ln(\\widehat{RR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{RR})} \\\\\n= &\\ln(\\widehat{RR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ -\\frac{1}{n_1}+\\frac{1}{n_{21}}-\\frac{1}{n_2}}\\\\\n=  & 1.1913 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ -\\frac{1}{532}+\\frac{1}{128}-\\frac{1}{1132}}\\\\\n=  & (0.9944,\\ 1.3883 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (4/6)",
    "text": "SHS Example: Relative Risk (4/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nConvert back to RR\n\n\\[\\begin{aligned} & (\\exp(0.9944),\\ \\exp(1.3883 )) \\\\\n= & (2.703,\\ 4.0081 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (5/6)",
    "text": "SHS Example: Relative Risk (5/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n1/2/3. Compute risk ratio and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\nriskratio(x = SHS_ct, rev = \"rows\")$measure\n\n          risk ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 3.291471 2.702998 4.008061"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-relative-risk-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Relative Risk (6/6)",
    "text": "SHS Example: Relative Risk (6/6)\n\n\n\n\nRelative risk\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Relative risk between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated risk of diabetes is 3.29 times greater for American Indians who had impaired glucose tolerance at baseline compared to those who had normal glucose tolerance (95% CI: 2.70, 4.01).\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the (population) relative risk is between 2.70 and 4.01.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the risk of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-16",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (1/6)",
    "text": "SHS Example: Odds Ratio (1/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\nNeeded steps:\n\nCompute the odds ratio\nFind confidence interval of log OR\nConvert back to OR\nInterpret the estimate"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-26",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (2/6)",
    "text": "SHS Example: Odds Ratio (2/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nCompute the odds ratio\n\n\\(\\widehat{p}_1 = 198/532 = 0.3722\\), \\(\\widehat{p}_2 = 128/1132 = 0.1131\\) \\[\\widehat{OR}=\\frac{\\widehat{p_1}/(1-\\widehat{p_1})}{\\widehat{p_2}/(1-\\widehat{p_2})}= \\dfrac{0.3722/(1-0.3722)}{0.1131/(1-0.1131)}= 4.6499\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-36",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (3/6)",
    "text": "SHS Example: Odds Ratio (3/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nFind confidence interval of log OR\n\n\\[\\begin{aligned} & \\ln(\\widehat{OR}) \\pm 1.96 \\times SE_{\\ln(\\widehat{OR})} \\\\\n= &\\ln(\\widehat{OR}) \\pm z_{\\left(1-\\frac{\\alpha}{2}\\right)}^\\ast \\times \\sqrt{\\frac{1}{n_{11}}\\ +\\frac{1}{n_{12}}+\\frac{1}{n_{21}}+\\frac{1}{n_{22}}}\\\\\n=  & 1.5368 \\pm 1.96\\times \\sqrt{\\frac{1}{198}\\ +\\frac{1}{334}+\\frac{1}{128}+\\frac{1}{1004}}\\\\\n=  & (1.2824,\\ 1.7913 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-46",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (4/6)",
    "text": "SHS Example: Odds Ratio (4/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nConvert back to OR\n\n\\[\\begin{aligned} & (\\exp(1.2824),\\ \\exp(1.7913 )) \\\\\n= & (3.6053,\\ 5.9971 )\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-56",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (5/6)",
    "text": "SHS Example: Odds Ratio (5/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n1/2/3. Compute OR and 95% confidence interval\n\nlibrary(epitools)\nSHS_ct = table(SHS$glucimp, SHS$case)\n# no `rev` needed below bc we set the reference level in slide 32\noddsratio(x = SHS_ct, method = \"wald\")$measure \n\n          odds ratio with 95% C.I.\n           estimate    lower    upper\n  Normal   1.000000       NA       NA\n  Impaired 4.649888 3.605289 5.997148"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pause-other-option-in-pubh-package-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Pause: other option in pubh package",
    "text": "Pause: other option in pubh package\n\ncontingency(case ~ glucimp, data = SHS, digits = 3)\n\n          Outcome\nPredictor     1    0\n  Impaired  198  334\n  Normal    128 1004\n\n             Outcome +    Outcome -      Total                 Inc risk *\nExposed +          198          334        532  37.218 (33.097 to 41.482)\nExposed -          128         1004       1132   11.307 (9.521 to 13.298)\nTotal              326         1338       1664  19.591 (17.709 to 21.581)\n\nPoint estimates and 95% CIs:\n-------------------------------------------------------------------\nInc risk ratio                                 3.291 (2.703, 4.008)\nInc odds ratio                                 4.650 (3.605, 5.997)\nAttrib risk in the exposed *                   25.911 (21.408, 30.413)\nAttrib fraction in the exposed (%)            69.618 (63.004, 75.050)\nAttrib risk in the population *                8.284 (5.631, 10.937)\nAttrib fraction in the population (%)         42.284 (34.713, 48.976)\n-------------------------------------------------------------------\nUncorrected chi2 test that OR = 1: chi2(1) = 154.239 Pr&gt;chi2 = &lt;0.001\nFisher exact test that OR = 1: Pr&gt;chi2 = &lt;0.001\n Wald confidence limits\n CI: confidence interval\n * Outcomes per 100 population units \n\n\n    Pearson's Chi-squared test with Yates' continuity correction\n\ndata:  dat\nX-squared = 152.6, df = 1, p-value &lt; 2.2e-16"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-odds-ratio-66",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Odds Ratio (6/6)",
    "text": "SHS Example: Odds Ratio (6/6)\n\n\n\n\nOdds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes Odds ratio between impaired and normal glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with impaired glucose tolerance at baseline is 4.65 times the odds for American Indians with normal glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 3.61 and 6.00.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#inversing-an-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Inversing an Odds Ratio",
    "text": "Inversing an Odds Ratio\n\nSome clinicians may prefer interpretations of OR &gt; 1 instead of an OR &lt; 1\nThe transformation can easily be done by inverse\n\nRemember we discussed that OR = 4 is an equivalent a strong association as OR = 0.25 (1/4)\n\nOR comparing group 1 to group 2 = inverse of OR comparing group 2 to group 1\n\n\\[ OR_{1v2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}=\\frac{1}{\\frac{{\\hat{p}}_2/(1-{\\hat{p}}_2)}{{\\hat{p}}_1/(1-{\\hat{p}}_1)}}=\\frac{1}{OR_{2v1}}\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\\[\\widehat{OR}=\\frac{1}{4.6499}=0.2151\\] The 95% Confidence interval is then\n\\[ \\left(\\frac{1}{5.9971}, \\frac{1}{3.6053}\\right)\\ =\\ (0.1667, 0.2774)\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\nNeeded steps:\n\nInverse point estimate and confidence interval\n\n\nlibrary(epitools)\noddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")$measure \n\n          odds ratio with 95% C.I.\n           estimate     lower     upper\n  Impaired 1.000000        NA        NA\n  Normal   0.215059 0.1667459 0.2773702"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "",
    "text": "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nAttaching package: 'janitor'\n\n\nThe following objects are masked from 'package:stats':\n\n    chisq.test, fisher.test\n\n\n\nAttaching package: 'rstatix'\n\n\nThe following object is masked from 'package:janitor':\n\n    make_clean_names\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nhere() starts at /Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/S2024_BSTA_513_613/S2024_BSTA_513"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relationship-between-rr-and-or-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relationship-between-rr-and-or-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relationship Between RR and OR",
    "text": "Relationship Between RR and OR\n\nAn example where a disease rare over the whole sample (~1%), but …\n\n\\(\\widehat{OR}\\) is not a good estimate of \\(\\widehat{RR}\\) in “rare” disease\n\n\n\n\n\\(\\widhat{p}_1\\) is 0.5: thus \\(\\widehat{OR}\\) and \\(\\widehat{RR}\\) are very different\n\n\\[\\widehat{RR}=\\frac{0.5}{0.00102}=490 \\text{ and } \\widehat{OR} = \\frac{0.5(1-0.5)}{0.00102(1-0.00102)}=981\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relationship-between-rr-and-or-12",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relationship-between-rr-and-or-12",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relationship Between RR and OR (1/2)",
    "text": "Relationship Between RR and OR (1/2)\n\nNotice that odds ratio is not equivalent to relative risk (or risk ratio)\nHowever, when the probability of “success” is small (e.g., rare disease), \\(\\widehat{OR}\\) is a nice approximation of \\(\\widehat{RR}\\) \\[\\widehat{OR}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}=\\widehat{RR}\\cdot \\frac{1-\\widehat{p_2}}{1-\\widehat{p_1}}\\]\n\nThe fraction in the last term of the above expression approximately equals to 1.0 if \\(\\widehat{p}_1\\) and \\(\\widehat{p}_2\\) BOTH quite small (&lt; 0.1)\n\nThe \\(\\widehat{OR}\\) and \\(\\widehat{RR}\\) are not very close to each other in SHS: diabetes not a rare disease\n\n\\(\\widehat{OR} = 4.65\\)\n\\(\\widehat{RR} = 3.29\\)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relationship-between-rr-and-or-22",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#relationship-between-rr-and-or-22",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Relationship Between RR and OR (2/2)",
    "text": "Relationship Between RR and OR (2/2)\n\nAn example where a disease rare over the whole sample (~1%), but …\n\n\\(\\widehat{OR}\\) is not a good estimate of \\(\\widehat{RR}\\) in “rare” disease\n\n\n\n\n\\(\\widehat{p}_1\\) is 0.5: thus \\(\\widehat{OR}\\) and \\(\\widehat{RR}\\) are very different\n\n\\[\\widehat{RR}=\\frac{0.5}{0.00102}=490 \\text{ and } \\widehat{OR} = \\frac{0.5(1-0.5)}{0.00102(1-0.00102)}=981\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#which-measurement-should-one-use-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#which-measurement-should-one-use-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Which measurement should one use?",
    "text": "Which measurement should one use?\n\n\n\n\n\n\n\nMeasurement\nPros and Cons\n\n\n\n\nRisk difference\n\nCan provide additional information, but can be misleading on its own\nNot the preferred measurement\n\n\n\nRisk ratio\n\nEasy to interpret because is a ratio of probabilities\nCannot use in retrospective, case-control studies\n\n\n\nOdds ratio\n\nAdequate for all studies\nGood estimate of RR for rare diseases\nMost preferred by statisticians because integrated into logistic regression"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#rr-in-retrospective-case-control-study-1",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#rr-in-retrospective-case-control-study-1",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "RR in retrospective case-control study",
    "text": "RR in retrospective case-control study\n\nAssume a 1:2 case-control study summarized in below table:\n\n\n\nAssume we compute the RR as if it is from a cohort study:\n\n\\[\\widehat{RR}=\\frac{\\widehat{p_1}}{\\widehat{p_2}}=\\frac{n_{11}/n_{1+}}{n_{21}/n_{2+}}=\\frac{40/80}{60/220}=1.8333\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#rr-in-retrospective-case-control-study-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#rr-in-retrospective-case-control-study-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "RR in retrospective case-control study",
    "text": "RR in retrospective case-control study\n\nIn real world, the proportion of controls (not diseased) is typically much higher. Assume the table below shows the proportion in the population in a cohort study\n\n\n\nThe estimated RR for the patient population is:\n\n\\[\\widehat{RR}=\\frac{\\widehat{p_1}}{\\widehat{p_2}}=\\frac{400/4400}{600/16600}=2.5152\\]"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measuring-agreement-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#measuring-agreement-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Measuring Agreement",
    "text": "Measuring Agreement\nCohen’s Kappa: \\[\\hat{\\kappa}=\\frac{p_o-p_E}{1-p_E}\\]\n\n\\(p_o=\\ \\frac{\\sum_{i}\\ n_{ii}}{n}\\) (sum of diagonals divided by total)\n\\(p_E=\\sum_{i}{a_ib_i}\\)\n\n \n\nWhat’s \\(\\sum_{i}{a_ib_i}\\)?\n\nFor \\(i\\) responses (row/columns), \\(a_i\\) is proportion of \\(i\\) response category in first survey and \\(b_i\\) is proportion of \\(i\\) response category in second survey (we’ll show this in the example)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#to-give-a-taste-of-regression-for-a-categorical-outcome-we-will-come-back-to-this",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#to-give-a-taste-of-regression-for-a-categorical-outcome-we-will-come-back-to-this",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "To give a taste of regression for a categorical outcome (we will come back to this!!)",
    "text": "To give a taste of regression for a categorical outcome (we will come back to this!!)\n\nlogreg = glm(case ~ glucimp, data = SHS, family = binomial)\n\n\n\n\nsummary(logreg)\n\n\nCall:\nglm(formula = case ~ glucimp, family = binomial, data = SHS)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -2.05972    0.09385  -21.95   &lt;2e-16 ***\nglucimpImpaired  1.53684    0.12982   11.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1646.3  on 1663  degrees of freedom\nResidual deviance: 1501.3  on 1662  degrees of freedom\nAIC: 1505.3\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nlibrary(epiDisplay)\nlogistic.display(logreg)\n\n\nLogistic regression predicting case \n \n                            OR(95%CI)      P(Wald's test) P(LR-test)\nglucimp: Impaired vs Normal 4.65 (3.61,6)  &lt; 0.001        &lt; 0.001   \n                                                                    \nLog-likelihood = -750.6533\nNo. of observations = 1664\nAIC value = 1505.3066\n\n\n\nlibrary(oddsratio)\nor_glm(data = SHS, model = logreg)"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#a-taste-of-regression-for-a-binary-outcome-we-will-come-back-to-this",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#a-taste-of-regression-for-a-binary-outcome-we-will-come-back-to-this",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "A taste of regression for a binary outcome (we will come back to this!!)",
    "text": "A taste of regression for a binary outcome (we will come back to this!!)\n\nlogreg = glm(case ~ glucimp, data = SHS, family = binomial)\n\n\n\n\nsummary(logreg)\n\n\nCall:\nglm(formula = case ~ glucimp, family = binomial, data = SHS)\n\nCoefficients:\n                Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -2.05972    0.09385  -21.95   &lt;2e-16 ***\nglucimpImpaired  1.53684    0.12982   11.84   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1646.3  on 1663  degrees of freedom\nResidual deviance: 1501.3  on 1662  degrees of freedom\nAIC: 1505.3\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nlibrary(epiDisplay)\nlogistic.display(logreg)\n\n\nLogistic regression predicting case \n \n                            OR(95%CI)      P(Wald's test) P(LR-test)\nglucimp: Impaired vs Normal 4.65 (3.61,6)  &lt; 0.001        &lt; 0.001   \n                                                                    \nLog-likelihood = -750.6533\nNo. of observations = 1664\nAIC value = 1505.3066"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#shs-example-inversing-odds-ratio-2",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "SHS Example: Inversing Odds Ratio",
    "text": "SHS Example: Inversing Odds Ratio\n\n\n\n\nInversing odds ratio\n\n\nCompute the point estimate and 95% confidence interval for the diabetes odds ratio between normal and impaired glucose tolerance.\n\n\n\n\n\n\n\n\n\n  \n    \n    \n      Glucose tolerance\n      \n        Diabetes\n      \n      Total\n    \n    \n      No\n      Yes\n    \n  \n  \n    Impaired\n334\n198\n532\n    Normal\n1004\n128\n1132\n    Total\n1338\n326\n1664\n  \n  \n  \n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe estimated odds of diabetes for American Indians with normal glucose tolerance at baseline is 0.22 times the odds for American Indians with impaired glucose tolerance at baseline.\n \nAdditional interpretation of 95% CI (not needed): We are 95% confident that the odds ratio is between 0.17 and 0.28.\n \nSince the 95% confidence interval does not include 1, there is sufficient evidence that the odds of diabetes differs significantly between impaired and normal glucose tolerance at baseline."
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#pubh-vs.-epitools",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "pubh vs. epitools",
    "text": "pubh vs. epitools\n\nIn pubh with contingency()\n\nGet all the info at once\nReally nice to double check how the code is interpreting your input\n\nIn epitools with riskratio() or oddsratio()\n\nMuch easier to grab the numbers!\nIn Quarto you can take R code and directly put it in your text\n\ng = oddsratio(x = SHS_ct, method = \"wald\", rev = \"rows\")\ng$measure[2,1]\n\n[1] 0.215059\n\n\n\nI can write {r eval=\"false\" echo=\"true\"} round(g$measure[2,1], 3) to print the number 0.215\n\n\n\n\n\nLesson 3: Measurement of Association for Contingency Tables"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-12",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (1/2)",
    "text": "Review of Test of Association (1/2)\n\nLast week: learned some tests of association for contingency tables\n\n \n\nFor studies with two independent samples\n\nGeneral association\n\nChi-squared test\nFisher’s Exact test\n\nTest of trends\n\nCochran-Armitage test\nMantel-Haenszel test"
  },
  {
    "objectID": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "href": "lectures/03_Meas_Assoc_CT/03_Meas_Assoc_CT.html#review-of-test-of-association-22",
    "title": "Lesson 3: Measurement of Association for Contingency Tables",
    "section": "Review of Test of Association (2/2)",
    "text": "Review of Test of Association (2/2)"
  },
  {
    "objectID": "project/Lab_01_instructions.html",
    "href": "project/Lab_01_instructions.html",
    "title": "Lab 1 Instructions",
    "section": "",
    "text": "Caution\n\n\n\nThis project includes analysis on food insecurity."
  },
  {
    "objectID": "project/Lab_01_instructions.html#directions",
    "href": "project/Lab_01_instructions.html#directions",
    "title": "Lab 1 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Grading\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n1.1.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/Lab_01_instructions.html#lab-activities",
    "href": "project/Lab_01_instructions.html#lab-activities",
    "title": "Lab 1 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Reading and listening activities\nI will not check that you have read or listened to any of these, but it is a good starting point for understanding the context of our data: food insecurity in the United States. I haven’t fully read them all yet, but I will be reading and sharing throughout the quarter.\nHere are some articles:\n\nNPR: Millions of American families struggle to get food on the table, report finds\n\nWith option to listen to article\n\nIdentification of factors related to food insecurity and the implications for social determinants of health screenings\nNIMHD’s page on Food Accessibility, Insecurity and Health Outcomes\nFood Insecurity among American Indians and Alaska Natives: A National Profile using the Current Population Survey–Food Security Supplement\nA Framework for Evaluating Social Determinants of Health Screening and Referrals for Assistance\n\n\n\n2.2 Familiarize yourself with the Well-Being and Basic Needs Survey\nPlease read the Urban Institute’s page on the Well-Being and Basic Needs Survey (WBNS) and more of their published information of the survey (at least the first 4 pages). You can also read about the overarching From Safety Net to Solid Ground Initiative that started the survey. Answer the following questions:\n\nWhat is the motivation for this study?\nHow could an analysis that looks at associations with food insecurity help facilitate change in policy?\nWhy is it important to study food insecurity?\n\n\n\n\n\n\n\nTask\n\n\n\nAnswer the following questions using information on WBNS:\n\nWhat is the motivation for this study?\nHow could an analysis that looks at associations with food insecurity help facilitate change in policy?\nWhy is it important to study food insecurity?\n\n\n\n\n\n2.3 File organization\nBefore downloading the data, go back to Lesson 2 and follow the file setup for our project. This includes making an .Rproj file within the main folder. Make sure you are working with the project by using the here() function to display your working directory.\n\n\n\n\n\n\nTask\n\n\n\nDisplay your working directory using the here package and here() function.\n\n\n\n\n2.4 Access and download the data\n\nGo to the Health and Medical Care Archive page for the Well-Being and Basic Needs Survey\nGo to the Data & Documentation tab \nDownload the R version of the Public use data \nRead and agree to the Terms of Use. After this, you will be redirected to a new page.\nLog into ICPSR by clicking “Access through your institution”. You should be taken to a new page where you need to select “Oregon Health & Science University” \nLogin using standard OHSU login. Then the download should begin!\nMake sure to move this into your project folder under the Data folder.\nTake a look at the folders/files you just downloaded. Make sure to locate and understand the difference between the Codebook, Questionnaire, and User Guide. (Note that the website also contains the codebook if you go to the variable tab. I think the online one has an easier user interface than the pdf.)\n\n\n\n\n\n\n\nTask\n\n\n\nNo task to report back on. Just make sure you have the data!\n\n\n\n\n2.5 Decide on list of variables to focus on\nFrom the codebook, I want you to explore the variables and create a list of 10 predictors that you would like to focus on. Our outcome is FOOD_INSEC so we cannot use this as a predictor. Feel free to take a look at the Urban Institute’s list of publications to get ideas of variables and relationships.\nThere are a few requirements for your predictors:\n\n1 variable must be a numeric (i.e. PPAGE)\n1 variable must be binary\n1 variable must be multi-level categorical (categorical with more than 2 groups)\nYou must choose at least 10 predictors (does not include the outcome)\n\nThere is a good online version of the codebook with information about the variables. I have linked you to the ID varaible, but you can take a look at all the other variables using the left hand side navigator:\n\nYou can look under survey questions to get a better sense of how questions were asked, but please stick to variables under Demographic Variables, Family Income, Insurance Status, and Material Hardship. Do not choose variables from the Administrative levels, Survey Questions, nor School Enrollment or Child Care variables. May leave in the ID variable for easier tracking on individuals, but it does not count towards the 10 predictors.\n\n\n\n\n\n\nTask\n\n\n\nList the 10 predictors that you plan to use in your analysis. Note which variables are numeric, binary, or multi-level categorical.\n\n\n\n\n2.6 Get a sense of how you would like to analyze the data\nFor our project, we will examine the association between the food insecurity and one other variable (our main explanatory variable). From the above readings, survey information, and your list of predictors in Section 2.5, which association are you most interested in analyzing?\nPlease write this in the form of a research question statement. Feel free to copy this sentence and insert your chosen predictor: We will investigate the association between food insecurity and ____.\n\n\n\n\n\n\nTask\n\n\n\nComplete the following statement to identify your research question:\nWe will investigate the association between food insecurity and ____.\n\n\n\n\n2.7 Save data for processing with .Rda\nWithin this document, or in a separate document, use R to save a copy of the dataset so that you can process it without changing the raw data. Recall the file organization that we discussed to set up proper folders. Include a screenshot showing the new .Rda file within your Data folder.\n\n\n\n\n\n\nTask\n\n\n\nInclude a screenshot showing the new .Rda file within your Data folder.\n\n\n\n\n2.8 Getting data in working format\nYou can start by selecting only the variables you will use in your analysis. Again, you can keep ID in addition to your outcome and predictors for easy tracking.\nUse the following code (with your dataset’s name) to remove the parentheses with values that are in front of the category names. Make sure to change old_df and new_df.\n\nnew_df = data.frame(lapply(old_df, function(x) {gsub(\".*) \", \"\", x)}))\n\n\n\n\n\n\n\nTask\n\n\n\n\nSelect the variables that will be used in your analysis and make a new dataset. Include the code that you used.\nRemove the parentheses with values that are in front of the category names.\n\n\n\n\n\n2.9 Explore the outcome and predictors\nThe codebook online gives some nice plots of each variable. Please take a look at the codebook online to see the spread of each variable. Make note of any categorical variables that have less than 100 observations in a group. This may cause issues in our analysis later.\n\n\n\n\n\n\nTask\n\n\n\n\nTo check that you have looked that the variables, please report the percent of respondents that were food insecure in the past 12 months.\nList any categorical variables that have less than 100 observations in a group\n\n\n\n\n\n2.10 Compile above work into an introduction\nPlease check out this source for what a research article introduction includes and how to organize it. The only thing I would add is mentioning the Well-Being and Basic Needs Survey.\n\n\n\n\n\n\nTask\n\n\n\nWrite an introduction to the analysis."
  },
  {
    "objectID": "project.html#information-and-resources-on-food-insecurity",
    "href": "project.html#information-and-resources-on-food-insecurity",
    "title": "Project Central",
    "section": "Information and Resources on Food Insecurity",
    "text": "Information and Resources on Food Insecurity\nThis project will discuss food insecurity and unmet basic needs. If you have experienced or are experiencing food insecurity, and this project impacts your mental health or ability to work, please let me know. We can work on an alternative analysis with a different dataset.\nIf you are currently experiencing food insecurity or not meeting your basic needs, here are some resources for Oregon residents:\n\nOregon One elegibility page\n\nYou can apply for benefits including medical, food, cash, or child care assistance\nThis is one way to apply to multiple benefits\n\nOregon Health Plan\nThe Oregon Food Bank has a food finder\n\nIn addition to these statewide resources, as students, there are other resources available to you:\n\nFood Assistance and Basic Needs for OHSU students\n\nIncludes information on SNAP program\nIncludes information on free groceries available to students at the Food Resource Center\n\nYou can even order it online and pick it up!\n\n\nYou can email basicneeds@ohsu.edu for help getting assistance or connecting you to the following services:\n\nManage your finances\nAccess emergency funds\nFind legal services\nApply for SNAP\nFind child care\nFind housing"
  },
  {
    "objectID": "project.html#reading-and-listening-sources",
    "href": "project.html#reading-and-listening-sources",
    "title": "Project Central",
    "section": "Reading and listening sources",
    "text": "Reading and listening sources"
  },
  {
    "objectID": "lectures/Project_prez/Project_intro.html#original-motivation",
    "href": "lectures/Project_prez/Project_intro.html#original-motivation",
    "title": "Project Introduction",
    "section": "Original motivation",
    "text": "Original motivation\n\nMixed-Status Immigrant Families Disproportionately Experienced Material Hardships in 2021\n\n\n\nProject Introduction"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#relationship-between-rr-and-or-12",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#relationship-between-rr-and-or-12",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Relationship Between RR and OR (1/2)",
    "text": "Relationship Between RR and OR (1/2)\n\nNotice that odds ratio is not equivalent to relative risk (or risk ratio)\n\n \n\nHowever, when the probability of “success” is small (e.g., rare disease), \\(\\widehat{OR}\\) is a nice approximation of \\(\\widehat{RR}\\) \\[\\widehat{OR}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}=\\widehat{RR}\\cdot \\frac{1-\\widehat{p_2}}{1-\\widehat{p_1}}\\]\n\nThe fraction in the last term of the above expression approximately equals to 1.0 if \\(\\widehat{p}_1\\) and \\(\\widehat{p}_2\\) BOTH quite small (&lt; 0.1)\n\n\n \n\nThe \\(\\widehat{OR}\\) and \\(\\widehat{RR}\\) are not very close to each other in SHS: diabetes not a rare disease\n\n\\(\\widehat{OR} = 4.65\\)\n\\(\\widehat{RR} = 3.29\\)"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#relationship-between-rr-and-or-22",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#relationship-between-rr-and-or-22",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Relationship Between RR and OR (2/2)",
    "text": "Relationship Between RR and OR (2/2)\n\nAn example where a disease rare over the whole sample (~1%), but …\n\n\\(\\widehat{OR}\\) is not a good estimate of \\(\\widehat{RR}\\) in “rare” disease\n\n\n\n\n\n\n\n\n\\(\\widehat{p}_1\\) is 0.5: thus \\(\\widehat{OR}\\) and \\(\\widehat{RR}\\) are very different\n\n\\[\\widehat{RR}=\\frac{0.5}{0.00102}=490 \\text{ and } \\widehat{OR} = \\frac{0.5(1-0.5)}{0.00102(1-0.00102)}=981\\]"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#notes-for-odds-ratios",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#notes-for-odds-ratios",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Notes for Odds Ratios",
    "text": "Notes for Odds Ratios\n\nThe OR is valid for\n\nCase-control studies (where the RR is not appropriate)\nProspective cohort studies\nCross-sectional studies\n\n\n \n\nIt can be interpreted either as…\n\nOdds of event for exposed vs. unexposed individuals, or\nOdds of exposure for individuals with vs. without the event of interest\n\n\n \n\nPay attention to the numerator and denominator for the OR"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#which-measurement-should-one-use",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#which-measurement-should-one-use",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Which measurement should one use?",
    "text": "Which measurement should one use?"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "RR in retrospective case-control study",
    "text": "RR in retrospective case-control study\n\nIn retrospective, case-control studies, we identify cases (patients with the outcome), then select a number of controls (patients without the outcome)\n\nCase-control study to require much smaller sample size than equivalent cohort studies\nSo we pick out the cases and controls first, then see if there is exposure\nBecause the odds can be interpreted as \\(OR\\) of exposure and \\(OR\\) of case, we can use the \\(OR\\) to assess the case\n\\(RR\\) relative risk does not have this property\n\nHowever, the proportion of cases in the sample does not represent the proportion of cases in the population\n\nRR compares probability of the outcome (case) for exposed and unexposed groups\nNumber of outcomes has been artificially inflated for retrospective study"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-1",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-1",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "RR in retrospective case-control study",
    "text": "RR in retrospective case-control study\n\nAssume a 1:2 case-control study summarized in below table:\n\n\n\nAssume we compute the RR as if it is from a cohort study:\n\n\\[\\widehat{RR}=\\frac{\\widehat{p_1}}{\\widehat{p_2}}=\\frac{n_{11}/n_{1+}}{n_{21}/n_{2+}}=\\frac{40/80}{60/220}=1.8333\\]"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-2",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-2",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "RR in retrospective case-control study",
    "text": "RR in retrospective case-control study\n\nIn real world, the proportion of controls (not diseased) is typically much higher. Assume the table below shows the proportion in the population in a cohort study\n\n\n\nThe estimated RR for the patient population is:\n\n\\[\\widehat{RR}=\\frac{\\widehat{p_1}}{\\widehat{p_2}}=\\frac{400/4400}{600/16600}=2.5152\\]"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#or-in-retrospective-case-control-study",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#or-in-retrospective-case-control-study",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "OR in retrospective case-control study",
    "text": "OR in retrospective case-control study\n\nWhile we cannot estimate RR from a case-control study, we can still estimate OR for case-control study\n\nOR does not require us to distinguish between the outcome variable and explanatory variable in the contingency table\n\nAKA: Odds ratio of disease comparing exposed to not exposed is same as odds ratio of being exposed comparing diseased and not diseased\n\n\n\n \n\n\n\nFor case-control study where the probability of having outcome is small, the \\(\\widehat{OR}\\) is a nice approximation to \\(\\widehat{RR}\\)\n\nFor the 1:2 case-control table: \\(\\widehat{OR}=\\frac{40\\cdot160}{40\\cdot60} = 2.667\\)\nPopulation cohort study: \\(\\widehat{RR}=2.5152\\)"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-5",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-5",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Measuring Agreement",
    "text": "Measuring Agreement\n\nStill within the realm of contingency tables\nWhat if we are NOT looking at the association between two variables?\n\n \n\nWhat if we want to look at the agreement between two things?\n\nAnswers of same subjects for same survey taken at different times\nTwo different radiologists’ assessment of the same X-ray\n\n\n \n\nCohen’s Kappa statistics: widely used as a measure of agreement\n\nExample: Reliability studies, interobserver agreement"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Beef Consumption in Survey",
    "text": "Example: Beef Consumption in Survey\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between beef consumption surveys. Similar to question: Are results reproducible for the beef-consumption in the survey?\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the kappa statistic\nFind confidence interval of kappa\nInterpret the estimate"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-1",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-1",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Measuring Agreement",
    "text": "Measuring Agreement\n\nIf perfect agreement among the two raters/surveys:\n\nWe would expect nonzero entries only in the diagonal cells of the table\n\n\n \n\n\\(p_o\\) is the observed proportion of complete agreement (concordance)\n\\(p_E\\) is the expected proportion of complete agreement if the agreement is just due to chance\nIf the \\(p_o\\) is much greater than \\(p_E\\), then the agreement level is high.\n\nOtherwise, the agreement level is low\n\n\n \n\n\n\nCohen’s Kappa is based on the difference between \\(p_o\\) and \\(p_E\\): \\[\\hat{\\kappa}=\\frac{p_o-p_E}{1-p_E}\\]\n\n\n\n\n\n\\(\\hat{\\kappa} = 0\\): No agreement between surveys/raters other than what would be expected by chance\n\\(\\hat{\\kappa} = 1\\): Complete agreement"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-2",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-2",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Measuring Agreement",
    "text": "Measuring Agreement\nCohen’s Kappa: \\[\\hat{\\kappa}=\\frac{p_o-p_E}{1-p_E}\\]\n\n\\(p_o=\\ \\frac{\\sum_{i}\\ n_{ii}}{n}\\) (sum of diagonals divided by total)\n\\(p_E=\\sum_{i}{a_ib_i}\\)\n\n \n\nWhat’s \\(\\sum_{i}{a_ib_i}\\)?\n\nFor \\(i\\) responses (row/columns), \\(a_i\\) is proportion of \\(i\\) response category in first survey and \\(b_i\\) is proportion of \\(i\\) response category in second survey (we’ll show this in the example)"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-1",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-1",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Beef Consumption in Survey",
    "text": "Example: Beef Consumption in Survey\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between beef consumption surveys. Similar to question: Are results reproducible for the beef-consumption in the survey?\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n1/2. Compute the kappa statistic and find confidence interval of kappa\n\nlibrary(epiR)\nbeef = matrix(c(136, 92, 69, 240), nrow = 2, byrow = T)\nepi.kappa(beef, method = \"cohen\")$kappa \n\n        est         se     lower     upper\n1 0.3781906 0.04100635 0.2978196 0.4585616"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-6",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-6",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Poll Everywhere Question 6",
    "text": "Poll Everywhere Question 6"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-between-two-raters",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-between-two-raters",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Measuring Agreement Between Two Raters",
    "text": "Measuring Agreement Between Two Raters"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#testing-measuring-agreement",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#testing-measuring-agreement",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Testing Measuring Agreement",
    "text": "Testing Measuring Agreement"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Beef Consumption",
    "text": "Example: Beef Consumption"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measurement-of-association-so-far",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measurement-of-association-so-far",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Measurement of Association So Far",
    "text": "Measurement of Association So Far\n\nUsed contingency tables to test and measure association between two variables\n\nCategorical outcome variable (Y)\nOne categorical explanatory variable (X)\n\nWe looked at risk difference, risk ratio, and odds ratio to measure association\nSuch an association is called crude association\n\nNo adjustment for possible confounding factors\nAlso called marginal association\n\nBut we cannot expand analysis based on contingency tables past 3 variables\n\nWe can get into stratified contingency tables to bring in a 3rd variable\nBut I don’t think it’s worth it because regression can bring in (adjust for) many variables"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#a-taste-of-regression-for-a-binary-outcome-we-will-come-back-to-this",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#a-taste-of-regression-for-a-binary-outcome-we-will-come-back-to-this",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "A taste of regression for a binary outcome (we will come back to this!!)",
    "text": "A taste of regression for a binary outcome (we will come back to this!!)\n\nlogreg = glm(case ~ glucimp, data = SHS, family = binomial)\n\n\n\n\nsummary(logreg)\n\n\nCall:\nglm(formula = case ~ glucimp, family = binomial, data = SHS)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -0.52287    0.08969   -5.83 5.55e-09 ***\nglucimpNormal -1.53684    0.12982  -11.84  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1646.3  on 1663  degrees of freedom\nResidual deviance: 1501.3  on 1662  degrees of freedom\nAIC: 1505.3\n\nNumber of Fisher Scoring iterations: 4\n\n\n\n\nlibrary(epiDisplay)\n\nLoading required package: foreign\n\n\nLoading required package: survival\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:gtsummary':\n\n    select\n\n\nThe following object is masked from 'package:rstatix':\n\n    select\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: nnet\n\n\n\nAttaching package: 'epiDisplay'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    alpha\n\nlogistic.display(logreg)\n\n\nLogistic regression predicting case \n \n                     OR(95%CI)         P(Wald's test) P(LR-test)\nglucimp (cont. var.) 0.22 (0.17,0.28)  &lt; 0.001        &lt; 0.001   \n                                                                \nLog-likelihood = -750.6533\nNo. of observations = 1664\nAIC value = 1505.3066"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#our-example",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#our-example",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Our example!",
    "text": "Our example!"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-1",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-1",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-2",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-2",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#lets-get-this-data-down",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#lets-get-this-data-down",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Let’s get this data down!",
    "text": "Let’s get this data down!"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#just-in-case-our-data-doesnt-work-out-beef-consumption-in-survey",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#just-in-case-our-data-doesnt-work-out-beef-consumption-in-survey",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Just in case our data doesn’t work out: Beef Consumption in Survey",
    "text": "Just in case our data doesn’t work out: Beef Consumption in Survey\nA diet questionnaire was mailed to 537 female American nurses on two separate occasions several months apart. The questions asked included the quantities eaten of more than 100 separate food items. The data from the two surveys for the amount of beef consumption are presented in the below table. How can reproducibility of response for the beef-consumption data be quantified?"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#lets-get-our-mood-data-down",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#lets-get-our-mood-data-down",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Let’s get our mood data down!",
    "text": "Let’s get our mood data down!"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Our moods",
    "text": "Example: Our moods\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between our Monday and Wednesday moods.\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the kappa statistic\nFind confidence interval of kappa\nInterpret the estimate"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-oberved-kappas",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-oberved-kappas",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Measuring Agreement: Oberved Kappas",
    "text": "Measuring Agreement: Oberved Kappas\n\nGuidelines for evaluating Kappa (Rosner TB)\n \n\nExcellent agreement if \\(\\hat\\kappa \\geq 0.75\\)\n\n \n\nFair to good agreement if \\(0.4 &lt; \\hat\\kappa &lt; 0.75\\)\n\n \n\nPoor agreement if \\(\\hat\\kappa \\leq 0.4\\)\n\n\n \nIf \\(\\hat\\kappa&lt;0\\), suggest agreement less than by chance"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-cohens-kappa",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#measuring-agreement-cohens-kappa",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Measuring Agreement: Cohen’s Kappa",
    "text": "Measuring Agreement: Cohen’s Kappa\n\n\n\nPoint estimate: \\[\\hat{\\kappa}=\\frac{p_o-p_E}{1-p_E}\\]\n\nWith \\(p_o=\\ \\frac{\\sum_{i}\\ n_{ii}}{n}\\) (sum of diagonals divided by total)\nWith \\(p_E=\\sum_{i}{a_ib_i}\\)\nWith range of point estimate from \\([-1, 1]\\)\n\n\n\n\n\n\n\nWhat’s \\(\\sum_{i}{a_ib_i}\\)?\n\n\nFor \\(i\\) responses (row/columns), \\(a_i\\) is proportion of \\(i\\) response category in first survey and \\(b_i\\) is proportion of \\(i\\) response category in second survey (we’ll show this in the example)\n\n\n\n\n\nApproximate standard error:\n\n\\[ SE_{\\widehat{\\kappa}} = \\sqrt{\\frac{1}{{n\\left(1-p_e\\right)}^2}\\left\\{p_e^2+p_e-\\sum_{i}\\left[a_ib_i\\left(a_i+b_i\\right)\\right]\\right\\}}\\]\n\n95% Wald confidence interval for \\(\\widehat{\\kappa}\\):\n\n\\[\\widehat{\\kappa} \\pm 1.96 \\cdot SE_{\\widehat{\\kappa}}\\]"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-1",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-1",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Our moods",
    "text": "Example: Our moods\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between our Monday and Wednesday moods.\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n1/2. Compute the kappa statistic and find confidence interval of kappa\n\nlibrary(epiR)\nmoods = matrix(c(13, 9, 6, 24), nrow = 2, byrow = T)\nepi.kappa(moods, method = \"cohen\")$kappa \n\n        est       se     lower     upper\n1 0.3981481 0.131082 0.1412321 0.6550642"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#last-class",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#last-class",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Last class",
    "text": "Last class\n\nUsed contingency tables to test and measure association between two variables\n\nCategorical outcome variable (Y)\nOne categorical explanatory variable (X)\n\nWe looked at risk difference, risk ratio, and odds ratio to measure association\n\n\n\n\n\n\n\n\nMeasure\nEstimate\n\n\n\n\nRisk difference\n\\[\\widehat{RD} = \\widehat{p}_1 - \\widehat{p}_1 = \\dfrac{n_{11}}{n_1} - \\dfrac{n_{21}}{n_2}\\]\n\n\nRelative risk / risk ratio\n\\[\\widehat{RR}=\\dfrac{\\hat{p}_1}{\\hat{p}_2} = \\dfrac{n_{11}/n_1}{n_{21}/n_2}\\]\n\n\nOdds ratio\n\\[\\widehat{OR}=\\frac{odds_1}{odds_2}=\\frac{{\\hat{p}}_1/(1-{\\hat{p}}_1)}{{\\hat{p}}_2/(1-{\\hat{p}}_2)}\\]\n\n\n\n\nDiscussed how OR will be an important measurement in logistic regression"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-2",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-2",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Our moods",
    "text": "Example: Our moods\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between our Monday and Wednesday moods.\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe kappa statistic is ____ (95% CI: _____, _____), indicating ______ agreement.\nSince the 95% confidence interval does/does not contain 0, we have/do not have sufficient evidence that there is _________ agreement between our mood on Monday and our mood on Wednesday."
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-3",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-3",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Our moods",
    "text": "Example: Our moods\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between our Monday and Wednesday moods.\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n1/2. Compute the kappa statistic and find confidence interval of kappa\n\nlibrary(epiR)\nbeef = matrix(c(136, 92, 69, 240), nrow = 2, byrow = T)\nepi.kappa(beef, method = \"cohen\")$kappa \n\n        est         se     lower     upper\n1 0.3781906 0.04100635 0.2978196 0.4585616\n\n\n\n\n        est         se     lower     upper\n1 0.3781906 0.04100635 0.2978196 0.4585616"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-4",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-4",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Our moods",
    "text": "Example: Our moods\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between our Monday and Wednesday moods.\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe kappa statistic is 0.378 (95% CI: 0.298, 0.459), indicating fair agreement.\nSince the 95% confidence interval does not contain 0, we have sufficient evidence that there is fair agreement between the surveys for beef consumption. I would say this survey is not reliably reproducible since we did not achieve excellent agreement.\n\n\nLesson 4: Measurements of Association and Agreement"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-2",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-2",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Beef Consumption in Survey",
    "text": "Example: Beef Consumption in Survey\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between beef consumption surveys. Similar to question: Are results reproducible for the beef-consumption in the survey?\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe kappa statistic is 0.378 (95% CI: 0.298, 0.459), indicating fair agreement.\nSince the 95% confidence interval does not contain 0, we have sufficient evidence that there is fair agreement between the surveys for beef consumption. The survey is not reliably reproducible since we did not achieve excellent agreement.\n\n\nLesson 4: Measurements of Association and Agreement"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-13",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-13",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Our moods (1/3)",
    "text": "Example: Our moods (1/3)\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between our Monday and Wednesday moods.\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the kappa statistic\nFind confidence interval of kappa\nInterpret the estimate"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-23",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-23",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Our moods (2/3)",
    "text": "Example: Our moods (2/3)\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between our Monday and Wednesday moods.\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n1/2. Compute the kappa statistic and find confidence interval of kappa\n\nlibrary(epiR)\n\nPackage epiR 2.0.63 is loaded\n\n\nType help(epi.about) for summary information\n\n\nType browseVignettes(package = 'epiR') to learn how to use epiR for applied epidemiological analyses\n\n\n\n\nmoods = matrix(c(100, 40, 10, 30), nrow = 2, byrow = T)\nmoods\n\n     [,1] [,2]\n[1,]  100   40\n[2,]   10   30\n\nepi.kappa(moods, method = \"cohen\")$kappa \n\n        est         se     lower     upper\n1 0.3661972 0.07617362 0.2168996 0.5154947"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-33",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-our-moods-33",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Our moods (3/3)",
    "text": "Example: Our moods (3/3)\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between our Monday and Wednesday moods.\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe kappa statistic is ____ (95% CI: _____, _____), indicating ______ agreement.\nSince the 95% confidence interval does/does not contain 0, we have/do not have sufficient evidence that there is _________ agreement between our mood on Monday and our mood on Wednesday."
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-13",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-13",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Beef Consumption in Survey (1/3)",
    "text": "Example: Beef Consumption in Survey (1/3)\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between beef consumption surveys. Similar to question: Are results reproducible for the beef-consumption in the survey?\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nCompute the kappa statistic\nFind confidence interval of kappa\nInterpret the estimate"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-23",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-23",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Beef Consumption in Survey (2/3)",
    "text": "Example: Beef Consumption in Survey (2/3)\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between beef consumption surveys. Similar to question: Are results reproducible for the beef-consumption in the survey?\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n1/2. Compute the kappa statistic and find confidence interval of kappa\n\nlibrary(epiR)\nbeef = matrix(c(136, 92, 69, 240), nrow = 2, byrow = T)\nepi.kappa(beef, method = \"cohen\")$kappa \n\n        est         se     lower     upper\n1 0.3781906 0.04100635 0.2978196 0.4585616"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-33",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#example-beef-consumption-in-survey-33",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Example: Beef Consumption in Survey (3/3)",
    "text": "Example: Beef Consumption in Survey (3/3)\n\n\n\n\nAgreement of surveys\n\n\nCompute the point estimate and 95% confidence interval for the agreement between beef consumption surveys. Similar to question: Are results reproducible for the beef-consumption in the survey?\n\n\n\n\n\n\n\n\n\n\nNeeded steps:\n\nInterpret the estimate\n\nThe kappa statistic is 0.378 (95% CI: 0.298, 0.459), indicating fair agreement.\nSince the 95% confidence interval does not contain 0, we have sufficient evidence that there is fair agreement between the surveys for beef consumption. The survey is not reliably reproducible since we did not achieve excellent agreement."
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_01.html",
    "href": "project/LastName_FirstInit_Lab_01.html",
    "title": "Lab 1",
    "section": "",
    "text": "This lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_01.html#lab-activities",
    "href": "project/LastName_FirstInit_Lab_01.html#lab-activities",
    "title": "Lab 1",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n2.1 Reading and listening activities\n\n\n2.2 Familiarize yourself with the Well-Being and Basic Needs Survey\n\n\n\n\n\n\nTask\n\n\n\nAnswer the following questions using information on WBNS:\n\nWhat is the motivation for this study?\nHow could an analysis that looks at associations with food insecurity help facilitate change in policy?\nWhy is it important to study food insecurity?\n\n\n\n\n\n2.3 File organization\n\n\n\n\n\n\nTask\n\n\n\nDisplay your working directory using the here package and here() function.\n\n\n\n\n2.4 Access and download the data\n\n\n\n\n\n\nTask\n\n\n\nNo task to report back on. Just make sure you have the data!\n\n\n\n\n2.5 Decide on list of variables to focus on\n\n\n\n\n\n\nTask\n\n\n\nList the 10 predictors that you plan to use in your analysis. Note which variables are numeric, binary, or multi-level categorical.\n\n\n\n\n2.6 Get a sense of how you would like to analyze the data\n\n\n\n\n\n\nTask\n\n\n\nComplete the following statement to identify your research question:\nWe will investigate the association between food insecurity and ____.\n\n\n\n\n2.7 Save data for processing with .Rda\n\n\n\n\n\n\nTask\n\n\n\nInclude a screenshot showing the new .Rda file within your Data folder.\n\n\n\n\n2.8 Getting data in working format\n\n\n\n\n\n\nTask\n\n\n\n\nSelect the variables that will be used in your analysis and make a new dataset. Include the code that you used.\nRemove the parentheses with values that are in front of the category names.\n\n\n\n\n\n2.9 Explore the outcome and predictors\n\n\n\n\n\n\nTask\n\n\n\n\nTo check that you have looked that the variables, please report the percent of respondents that were food insecure in the past 12 months.\nList any categorical variables that have less than 100 observations in a group\n\n\n\n\n\n2.10 Compile above work into an introduction\n\n\n\n\n\n\nTask\n\n\n\nWrite an introduction to the analysis."
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_01.html#directions",
    "href": "project/LastName_FirstInit_Lab_01.html#directions",
    "title": "Lab 1",
    "section": "",
    "text": "This lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-3",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-3",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-4",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#poll-everywhere-question-4",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-13",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-13",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "RR in retrospective case-control study (1/3)",
    "text": "RR in retrospective case-control study (1/3)\n\nIn retrospective case-control studies: we identify cases (patients with the outcome), then select a number of controls (patients without the outcome)\n\nCase-control study to require much smaller sample size than equivalent cohort studies\nSo we pick out the cases and controls first, then see if there is exposure\n\n\n \n\nHowever, the proportion of cases in the sample does not represent the proportion of cases in the population\n\nRR compares probability of the outcome (case) for exposed and unexposed groups\nNumber of outcomes has been artificially inflated for case-control study"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-23",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-23",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "RR in retrospective case-control study (2/3)",
    "text": "RR in retrospective case-control study (2/3)\n\nAssume a 1:2 case-control study summarized in below table:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssume we compute the RR as if it is from a cohort study:\n\n\\[\\widehat{RR}=\\frac{\\widehat{p_1}}{\\widehat{p_2}}=\\frac{n_{11}/n_{1+}}{n_{21}/n_{2+}}=\\frac{40/80}{60/220}=1.8333\\]"
  },
  {
    "objectID": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-33",
    "href": "lectures/04_Meas_Assoc_Agree/04_Meas_Assoc_Agree.html#rr-in-retrospective-case-control-study-33",
    "title": "Lesson 4: Measurements of Association and Agreement",
    "section": "RR in retrospective case-control study (3/3)",
    "text": "RR in retrospective case-control study (3/3)\n\nIn real world, the proportion of controls (not diseased) is typically much higher. Assume the table below shows the proportion in the population in a cohort study\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe estimated RR for the patient population is:\n\n\\[\\widehat{RR}=\\frac{\\widehat{p_1}}{\\widehat{p_2}}=\\frac{400/4400}{600/16600}=2.5152\\]"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#health-disparities-in-breast-cancer-diagnosis-working-example",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#health-disparities-in-breast-cancer-diagnosis-working-example",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Health disparities in breast cancer diagnosis: working example",
    "text": "Health disparities in breast cancer diagnosis: working example\n\nQuestion: Is race/ethnicity and/or age associated with an individual’s diagnosed stage of breast cancer?\n\nFor now, consider each covariate separately\n\n\n \n\nPopulation: individuals who are assigned female at birth who have been diagnosed with breast cancer in the United States\n\n \n\nData from the Surveillance, Epidemiology, and End Results (SEER) Program (2014-2018)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#please-note-that-this-question-has-been-answered",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#please-note-that-this-question-has-been-answered",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Please note that this question has been answered",
    "text": "Please note that this question has been answered\n\nYou can take a look at the Breast Cancer Research Foundation’s page: Understanding Breast Cancer Racial Disparities\nBig contributors to racial disparities include:\n\nUnderrepresentation in clinical trials\nAccess to healthcare\nMore aggressive cancers more likely in people of Native American, African, Hispanic, and Latin American descent\n\nOur analysis will not be new, but this kind of work has shed light on the importance of focused research on people of color to better serve people of color who develop breast cancer\n\nDr. Davis focuses research on genomics and tumor microenvironment in African and African American patients\nDr. Ambrosone focuses research on how immune cells differ between patients. Specifically on the DARC gene, which is an evolved gene that helps fight malaria, that is found at a higher rate in people with African descent."
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-health-disparities-in-breast-cancer-diagnosis-12",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-health-disparities-in-breast-cancer-diagnosis-12",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Example: Health disparities in breast cancer diagnosis (1/2)",
    "text": "Example: Health disparities in breast cancer diagnosis (1/2)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-health-disparities-in-breast-cancer-diagnosis-22",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-health-disparities-in-breast-cancer-diagnosis-22",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Example: Health disparities in breast cancer diagnosis (2/2)",
    "text": "Example: Health disparities in breast cancer diagnosis (2/2)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-1",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-1",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-determine-differences-in-diagnosis-i",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-determine-differences-in-diagnosis-i",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we determine differences in diagnosis? (I)",
    "text": "How do we determine differences in diagnosis? (I)\n\n\nLesson 5: Simple Logistic Regression"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-determine-differences-in-diagnosis-12",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-determine-differences-in-diagnosis-12",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we determine differences in diagnosis? (1/2)",
    "text": "How do we determine differences in diagnosis? (1/2)\n\nBreast cancer diagnosis study: two variables that are categorical\nWe could use a contingency table (or two-way table)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-determine-differences-in-diagnosis-22",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-determine-differences-in-diagnosis-22",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we determine differences in diagnosis? (2/2)",
    "text": "How do we determine differences in diagnosis? (2/2)\n\n\n\nContingency table does not work for…\n\nContinuous covariates\nMultiple covariates\n\n\n \n\nLogistic regression models can handle multiple covariates that are continuous or categorical"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-determine-differences-in-diagnosis-22-1",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-determine-differences-in-diagnosis-22-1",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we determine differences in diagnosis? (2/2)",
    "text": "How do we determine differences in diagnosis? (2/2)\n\n\n\nContingency table does not work for…\n\nContinuous covariates\nMultiple covariates\n\n\n \n\nLogistic regression models can handle multiple covariates that are continuous or categorical"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#reference-for-individual-overview",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#reference-for-individual-overview",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Reference for individual overview",
    "text": "Reference for individual overview"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#building-towards-simple-logistic-regression",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#building-towards-simple-logistic-regression",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Building towards simple logistic regression",
    "text": "Building towards simple logistic regression\n\nGoal: model the probability of our outcome (\\(\\pi(X)\\)) with the covariate (\\(X_1\\))\nIn simple linear regression, we use the model in its various forms: \\[\\begin{aligned} Y&=\\beta_0+\\beta_1X_1+\\epsilon \\\\ E[Y|X] &= \\beta_0 + \\beta_1X_1 \\\\ \\widehat{Y} &= \\beta_0 + \\beta_1X_1 \\end{aligned}\\]\nPotential problem? Probabilities can only take values from 0 to 1"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#simple-logistic-regression-model-components",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#simple-logistic-regression-model-components",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Simple Logistic Regression Model: Components",
    "text": "Simple Logistic Regression Model: Components\n\n\n\nOutcome: \\(Y\\) - binary (two-level) categorical variable\n\n\\(Y=1\\)\n\\(Y=0\\)\n\n\n\n\nCovariate: \\(X_1\\)\n\nFor today: simple logistic regression with one covariate\n\\(X_1\\) can be continuous or categorical\n\n\n\n\n\nProbability of outcome for individual with observed covariates\n\n\\(P\\left(Y=1|X\\right)=\\pi\\left(X\\right)\\)\n\\(P\\left(Y=0|X\\right)=1-\\pi(X)\\)\n\nBecause the expected value is a weighted average, we can say: \\[\\begin{aligned} E(Y|X) & = P(Y=1|X) \\cdot 1 + P(Y=1|X) \\cdot 0 \\\\ & = P(Y=1|X) \\cdot 1 \\\\ & = P(Y=1|X) \\\\ & = \\pi(X) \\end{aligned}\\]\n\nFor categorical outcomes, \\(\\pi(X)\\) (or \\(\\pi\\) for shorthand), is more widely used than \\(E(Y|X)\\)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#can-we-apply-olr-to-our-binary-outcome",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#can-we-apply-olr-to-our-binary-outcome",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Can we apply OLR to our binary outcome?",
    "text": "Can we apply OLR to our binary outcome?"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-ordinary-linear-regression-olr-i",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-ordinary-linear-regression-olr-i",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Review of Ordinary Linear Regression (OLR) (I)",
    "text": "Review of Ordinary Linear Regression (OLR) (I)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#violated-linearity",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#violated-linearity",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Violated: Linearity",
    "text": "Violated: Linearity\n\n\n\nThe relationship between the variables is linear (a straight line):\n\n\\(E[Y|X]\\) or \\(\\pi(X)\\), is a straight-line function of \\(X\\)\n\nThe independent variable \\(X\\) can take any value, while \\(\\pi(X)\\) is a probability that should be bounded by [0,1]\n\nWe cannot use linear mapping to translate \\(X\\) to \\(\\pi(X)\\)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#violated-normality",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#violated-normality",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Violated: Normality",
    "text": "Violated: Normality\n\n\n\nIn linear regression, \\(\\epsilon\\) is distributed normally\n\n \n\nRecall that \\(Y\\) can take only one of the two values: 0 or 1\nAnd the fitted \\(Y\\), \\(\\widehat{Y}\\) can also only take values 0 or 1\nThus, \\(\\epsilon = Y - \\widehat{Y}\\) can only take values -1, 0, or 1\n\n \n\nThen \\(\\epsilon\\) cannot follow a normal distribution, which would require \\(\\epsilon\\) to have a continuum of values and no upper or lower bound"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#violated-homoscedasticity",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#violated-homoscedasticity",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Violated: Homoscedasticity",
    "text": "Violated: Homoscedasticity\n\nIn linear regression, \\(\\text{var}(\\epsilon) = \\sigma^2\\)\n\nVariance does not depend on \\(X\\)\n\n\n \n\nWhen Y is a binary outcome \\[\\begin{aligned} \\text{var}\\left(Y\\right) & =\\pi\\left(1-\\pi\\right)\\\\ & = \\left(\\beta_0+\\beta_1X\\right)\\left(1-\\beta_0-\\beta_1X\\right) \\end{aligned}\\]\n\nVariance depends on \\(X\\)\n\n\n \n\nBecause variance depends on \\(X\\): no homoscedasticity\n\nVariance will not be equal across X-values"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#what-happens-if-we-use-olr-for-categorical-responses",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#what-happens-if-we-use-olr-for-categorical-responses",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "What happens if we use OLR for categorical responses?",
    "text": "What happens if we use OLR for categorical responses?"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#simple-logistic-regression-model-components-1",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#simple-logistic-regression-model-components-1",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Simple Logistic Regression Model: Components",
    "text": "Simple Logistic Regression Model: Components"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-2",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-2",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-fix-these-violations",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-fix-these-violations",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we fix these violations?",
    "text": "How do we fix these violations?\n\nQuestion: How do we manipulate our response variable so that we fix these violations?\n\n \n\nAnswer: We need to transform the outcome so we can map differences in covariates to the two levels\n\nWill discuss in a few slides: called link function"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-transform-our-outcome",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-transform-our-outcome",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we transform our outcome?",
    "text": "How do we transform our outcome?"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#simple-logistic-regression-model",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#simple-logistic-regression-model",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Simple Logistic Regression Model",
    "text": "Simple Logistic Regression Model\nThe (population) regression model is denoted by:\n \n\n\n\n\n\n\\[ \\text{logit} (\\pi) =  \\beta_0 + \\beta_1X\\]\n\n\n\n\n\n \nComponents\n\n\n\n\\(\\pi\\)\nprobability that the outcome occurs (\\(Y=1\\)) given \\(X\\)\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#generalized-linear-models-glm-i",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#generalized-linear-models-glm-i",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Generalized Linear Models (GLM) (I)",
    "text": "Generalized Linear Models (GLM) (I)\n\n\n\nGeneralized Linear Models are a class of models that includes regression models for continuous and categorical responses\n\nResponses follow exponential family distribution\n\n\n \n\nHere we will focus on the GLMs for categorical/count data\n\nLogistic regression is just a one type of GLM\nPoisson regression – for counts\nLog-binomial can be used to focus on risk ratio"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-3",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-3",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#glm-random-component",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#glm-random-component",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "GLM: Random Component",
    "text": "GLM: Random Component\n\nThe random component specifies the response variable \\(Y\\) and selects a probability distribution for it\n\n \n \n\nBasically, we are just identifying the distribution for our outcome\n\nIf Y is binary: assumes a binomial distribution of Y\nIf Y is count: assumes Poisson or negative binomial distribution of Y\nIf Y is continuous: assumea Normal distribution of Y"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#glm-systematic-component",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#glm-systematic-component",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "GLM: Systematic Component",
    "text": "GLM: Systematic Component\n\nThe systematic component specifies the explanatory variables, which enter linearly as predictors \\[\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\]\n\n \n\nAbove equation includes:\n\nCentered variables\nInteractions\nTransformations of variables (like squares)\n\n\n \n\nSystematic component is the same as what we learned in Linear Models"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#glm-link-function",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#glm-link-function",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "GLM: Link Function",
    "text": "GLM: Link Function\n\nIf \\(\\mu = E(Y)\\), then the link function specifies a function \\(g(.)\\) that relates \\(\\mu\\) to the linear predictor as: \\[g\\left(\\mu\\right)=\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\]\n\n\\(g\\left(\\mu\\right)\\) is the transformation we make to \\(E(Y)\\) (aka \\(\\mu\\)) so that the linear predictors (right side of equation) can be linked to the outcome\n\nThe link function connects the random component with the systematic component\nCan also think of this as: \\[\\mu=g^{-1}\\left(\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\right)\\]"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#simple-logistic-regression-model-1",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#simple-logistic-regression-model-1",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Simple Logistic Regression Model",
    "text": "Simple Logistic Regression Model"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#estimation-for-logistic-regression-model",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#estimation-for-logistic-regression-model",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Estimation for Logistic Regression Model",
    "text": "Estimation for Logistic Regression Model\n\nSame as linear regression model: we need to estimate the values of \\(\\beta_0\\) and \\(\\beta_1\\)\n\n \n\nMaximum likelihood: yields values for the unknown parameters that maximize the probability of obtaining observed set of data\n\nIn linear regression, this leads to least squares estimation\nMaximum likelihood estimators (MLE): values of parameters that maximize likelihood\n\n\n \n\nLikelihood function: expresses the probability of the observed data as a function of the unknown parameters"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-5",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-5",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-to-find-maximum-likelihood-estimator-mle",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-to-find-maximum-likelihood-estimator-mle",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How to find Maximum Likelihood Estimator (MLE)?",
    "text": "How to find Maximum Likelihood Estimator (MLE)?\n\nConstruct a likelihood function for an individual\n\n \n\nConstruct the likelihood function across the sample\n\n \n\nConvert to log-likelihood\n\n \n\nFind parameter values that maximize log-likelihood (MLEs)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#construct-a-likelihood-function-for-an-individual",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#construct-a-likelihood-function-for-an-individual",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "1. Construct a likelihood function for an individual",
    "text": "1. Construct a likelihood function for an individual\n\n\n\nWithin a dataset with n subjects, for the \\(i\\)th subject:\n\nif \\(Y_i=1\\), the contribution to the likelihood function is \\(\\pi\\left(X_i\\right)\\)\nif \\(Y_i=0\\), the contribution to the likelihood function is \\(1-\\pi\\left(X_i\\right)\\)\n\n\n \n\nThe contribution from the \\(i\\)th subject to the likelihood function can be expressed as: \\[\\pi\\left(X_i\\right)^{Y_i}\\left[1-\\pi\\left(X_i\\right)\\right]^{1-Y_i}\\]\n\n\n\n\nRecall\n\n\n\n\\(Y_i\\): Response variable of the \\(i\\)th subject\n\\(X_i\\): Independent variable for the \\(i\\)th subject\n\\(\\pi\\left(X_i\\right)=\\Pr{\\left(Y_i=1\\middle|\\ X_i\\right)}\\)\n\\(1-\\pi\\left(X_i\\right)=\\Pr(Y_i=0|X_i)\\)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#construct-the-likelihood-function-across-the-sample",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#construct-the-likelihood-function-across-the-sample",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "2. Construct the likelihood function across the sample",
    "text": "2. Construct the likelihood function across the sample\n\n\n\nSince there are \\(n\\) subjects in the data, and each subject is considered independent of each other, the likelihood function for the whole data can be expressed as:\n\n\\[l(\\beta_0, \\beta_1) = \\prod_{i=1}^{n}{\\pi(X_i)^{Y_i} (1-\\pi(X_i)) ^ {1-Y_i}}\\]\n\n\n\nRecall\n\n\n\n\\(Y_i\\): Response variable of the \\(i\\)th subject\n\\(X_i\\): Independent variable for the \\(i\\)th subject\n\\(\\pi\\left(X_i\\right)=\\Pr{\\left(Y_i=1\\middle|\\ X_i\\right)}\\)\n\\(1-\\pi\\left(X_i\\right)=\\Pr(Y_i=0|X_i)\\)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#convert-to-log-likelihood",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#convert-to-log-likelihood",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "3. Convert to log-likelihood",
    "text": "3. Convert to log-likelihood\n\n\n\nMathematically, it is easier to work with the log likelihood function for maximization\nThe log likelihood function is: \\[\\begin{aligned}L\\left(\\beta_0,\\beta_1\\right) &=\\ln{\\left(l\\left(\\beta_0,\\beta_1\\right)\\right)} \\\\ & =\n\\sum_{i=1}^{n}\\bigg[Y_i\\cdot\\text{ln}[\\pi(X_i)] + (1-Y_i)\\cdot\\text{ln}[1-\\pi(X_i)] \\bigg]\n\\end{aligned}\\]\n\n\n\n\nRecall\n\n\n\n\\(Y_i\\): Response variable of the \\(i\\)th subject\n\\(X_i\\): Independent variable for the \\(i\\)th subject\n\\(\\pi\\left(X_i\\right)=\\Pr{\\left(Y_i=1\\middle|\\ X_i\\right)}\\)\n\\(1-\\pi\\left(X_i\\right)=\\Pr(Y_i=0|X_i)\\)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#find-mles-that-maximize-log-likelihood",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#find-mles-that-maximize-log-likelihood",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "4. Find MLEs that maximize log-likelihood",
    "text": "4. Find MLEs that maximize log-likelihood\n\nTo find \\(\\beta_0\\) and \\(\\beta_1\\) that maximizes \\(L\\left(\\beta_0,\\beta_1\\right)\\):\n\nWe differentiate \\(L\\left(\\beta_0,\\beta_1\\right)\\) with respect to \\(\\beta_0\\) and \\(\\beta_1\\)…\nAnd set the resulting expression to zero\n\n \nSuch equations are called likelihood equations.\n\n\\(\\sum\\left[Y_i-\\pi\\left(X_i\\right)\\right]=0\\)\n\\(\\sum\\ X_i\\left[Y_i-\\pi\\left(X_i\\right)\\right]=0\\)\n\n \nIn logistic regression, there is no “closed form” solution to the above equations\nNeed to use iterative algorithm, such as iteratively reweighted least squares (IRLS) algorithm, should be used to find the MLEs for logistic regression"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-do-this-in-r",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-do-this-in-r",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we do this in R?",
    "text": "How do we do this in R?\n\nglm() function automatically does MLE for you\nYou can explore other algorithms (other than IWLS) to maximize the likelihood\n\nPretty good Cross Validated post on algorithms in glm()"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#can-we-apply-ols-to-our-binary-outcome",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#can-we-apply-ols-to-our-binary-outcome",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Can we apply OLS to our binary outcome?",
    "text": "Can we apply OLS to our binary outcome?\n\nLet’s see if we can apply OLS/linear regression to our binary outcome\nWhat assumptions do our data need to meet in order to use OLR?\nLet’s review OLR assumptions!"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-simple-linear-regression-i",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-simple-linear-regression-i",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Review of simple linear regression (I)",
    "text": "Review of simple linear regression (I)\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-simple-linear-regression-12",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-simple-linear-regression-12",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Review of simple linear regression (1/2)",
    "text": "Review of simple linear regression (1/2)\nThe (population) regression model is denoted by:\n \n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-simple-linear-regression-22",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-simple-linear-regression-22",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Review of simple linear regression (2/2)",
    "text": "Review of simple linear regression (2/2)\n\nAssumptions of the linear regression model:\n\nIndependence: observations are independent\nLinearity: linear relationship between \\(E[Y|X]\\) and \\(X\\) \\[E[Y|X] = \\beta_0 + \\beta_1 \\cdot X\\]\nNormality and homoscedasticity assumption for residuals (\\(\\epsilon\\)):\n\nNormality: residuals are normally distributed\nHomoscedasticity (equal variance): Variance of \\(Y\\) given \\(X\\) (\\(\\sigma_{Y|X}^2\\)), is the same for any \\(X\\)\n\n\n\n \n\nWhich assumptions are violated if dependent variable is categorical?\n\nThink in terms of binary dependent variable"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-simple-linear-regression12",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#review-of-simple-linear-regression12",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Review of simple linear regression(1/2)",
    "text": "Review of simple linear regression(1/2)\nThe (population) regression model is denoted by:\n \n\n\n\n\n\n\\[Y =  \\beta_0 + \\beta_1X + \\epsilon\\]\n\n\n\n\n\n \nComponents\n\n\n\n\\(Y\\)\nresponse, outcome, dependent variable\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable\n\n\n\\(\\epsilon\\)\nresiduals, error term"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#generalized-linear-models-glms-12",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#generalized-linear-models-glms-12",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Generalized Linear Models (GLMs) (1/2)",
    "text": "Generalized Linear Models (GLMs) (1/2)\n\n\n\nGeneralized Linear Models are a class of models that includes regression models for continuous and categorical responses\n\nResponses follow exponential family distribution\nHelps us set up other types of regressions using each outcome’s needed transformations\n\n\n \n\nHere we will focus on the GLMs for categorical/count data\n\nLogistic regression is just a one type of GLM\nPoisson regression – for counts\nLog-binomial can be used to focus on risk ratio"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#generalized-linear-models-glms-22",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#generalized-linear-models-glms-22",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Generalized Linear Models (GLMs) (2/2)",
    "text": "Generalized Linear Models (GLMs) (2/2)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#glm-link-function-1",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#glm-link-function-1",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "GLM: Link Function",
    "text": "GLM: Link Function"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-4",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#poll-everywhere-question-4",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "homework/HW2.html#purpose",
    "href": "homework/HW2.html#purpose",
    "title": "Homework 2",
    "section": "",
    "text": "This homework is designed to help you practice the following important skills and knowledge that we covered in Lessons 3-6:\n\nCalculate and interpret the estimated risk difference, relative risk, and odds ratios, and their confidence intervals\nExpand work on contingency tables to evaluate the agreement or reproducibility using Cohen’s Kappa\nUnderstand important differences between linear regression and logistic regression\nConstruct a simple logistic regression model\nTest a covariate for significance using the Wald test and LRT"
  },
  {
    "objectID": "weeks/week_08_sched.html",
    "href": "weeks/week_08_sched.html",
    "title": "Week 8",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nOn Wednesday, 5/22, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "weeks/week_11_sched.html",
    "href": "weeks/week_11_sched.html",
    "title": "Week 11",
    "section": "",
    "text": "Room Locations for the week\n\n\n\nIf we meet this week, we will be in RPV Room A (1217)"
  },
  {
    "objectID": "homework/HW3.html#question-2",
    "href": "homework/HW3.html#question-2",
    "title": "Homework 3",
    "section": "Question 2",
    "text": "Question 2\nThis question is adapted from the Hosmer and Lemeshow textbook, page 47. (Note that the parts are sited differently in the textbook.)\nUse the Myopia Study data described in Section 1.6.6 and use MYOPIC as the outcome and as possible variables for a model: AGE, GENDER, family history of myopia (MOMMY and DADMY), number of hours playing sports (SPORTHR) and number of hours watching television (TVHR).\n\nPart a\nWrite down the equation for the logistic regression model of MYOPIC on AGE, GENDER, MOMMY, DADMY, SPORTHR, and TVHR. How many parameters does this model contain?\n\n\nPart b\nUsing glm(), obtain the maximum likelihood estimates of the parameters of the logistic regression model. Using these estimates write down the equation for the fitted values (i.e., the estimated logistic probabilities).\n\n\nPart c\nAssess the significance of the coefficient corresponding to number of hours playing sports. You may use the Wald test or the likelihood ratio test. Please interpret the odds ratio for the coefficient (in this part, you do not need to calculate the confidence interval.)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-transform-our-outcome-12",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-transform-our-outcome-12",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we transform our outcome? (1/2)",
    "text": "How do we transform our outcome? (1/2)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-transform-our-outcome-22",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-transform-our-outcome-22",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we transform our outcome? (2/2)",
    "text": "How do we transform our outcome? (2/2)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#translate-the-results-back-to-an-equation",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#translate-the-results-back-to-an-equation",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Translate the results back to an equation!",
    "text": "Translate the results back to an equation!\n\nJust going to pull the coefficients so I have a reference as I create the fitted regression model:\n\n\nsummary(bc_reg)$coefficients\n\n              Estimate Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -0.9894225  0.0232055 -42.63742 0.000000e+00\nAge_c        0.0569645  0.0032039  17.77974 1.014557e-70\n\n\n\nFitted logistic regression model: \\[\\text{logit}(\\pi(Age)) = -0.989 + 0.057 \\cdot Age\\]"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-do-this-in-r-12",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-do-this-in-r-12",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we do this in R? (1/2)",
    "text": "How do we do this in R? (1/2)\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-do-this-in-r-12-1",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-we-do-this-in-r-12-1",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we do this in R? (1/2)",
    "text": "How do we do this in R? (1/2)\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#reminder-simple-logistic-regression-model",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#reminder-simple-logistic-regression-model",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Reminder: Simple Logistic Regression Model",
    "text": "Reminder: Simple Logistic Regression Model\nThe (population) regression model is denoted by:\n \n\n\n\n\n\n\\[ \\text{logit} (\\pi) =  \\beta_0 + \\beta_1X\\]\n\n\n\n\n\n \nComponents\n\n\n\n\\(\\pi\\)\nprobability that the outcome occurs (\\(Y=1\\)) given \\(X\\)\n\n\n\\(\\beta_0\\)\nintercept\n\n\n\\(\\beta_1\\)\nslope\n\n\n\\(X\\)\npredictor, covariate, independent variable"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-perform-mle-in-r",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#how-do-perform-mle-in-r",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do perform MLE in R?",
    "text": "How do perform MLE in R?\n\nglm() function automatically does MLE for you\n\nFor logistic regression with a binary outcome, we need to set the family within glm() to “binomial” which will automatically set the logit link\n\nYou can explore other algorithms (other than IRLS) to maximize the likelihood\n\nPretty good Cross Validated post on algorithms in glm()"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-12",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-12",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Example: Breast cancer diagnosis (1/2)",
    "text": "Example: Breast cancer diagnosis (1/2)\n\nLet’s start with simple logistic regression with late stage breast cancer diagnosis as the outcome and age as our independent variable\nWe want to fit: \\[\\text{logit}(\\pi(Age)) = \\beta_0 + \\beta_1 \\cdot Age\\]\n\nDon’t forget: \\(\\pi(Age) = P(Y=1 | Age) = P(\\text{Late stage BC diagnosis}| Age)\\)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-13",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-13",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Example: Breast cancer diagnosis (1/3)",
    "text": "Example: Breast cancer diagnosis (1/3)\n\nLet’s start with simple logistic regression with late stage breast cancer diagnosis as the outcome and age as our independent variable\n\n \n\nWe want to fit: \\[\\text{logit}(\\pi(Age)) = \\beta_0 + \\beta_1 \\cdot Age\\]\n\nDon’t forget: \\(\\pi(Age) = P(Y=1 | Age) = P(\\text{Late stage BC diagnosis}| Age)\\)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-23",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-23",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Example: Breast cancer diagnosis (2/3)",
    "text": "Example: Breast cancer diagnosis (2/3)\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-33",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-33",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Example: Breast cancer diagnosis (3/3)",
    "text": "Example: Breast cancer diagnosis (3/3)\n\nTranslate the results back to an equation!\nJust going to pull the coefficients so I have a reference as I create the fitted regression model:\n\n\n\n\nsummary(bc_reg)$coefficients\n\n              Estimate Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -0.9894225  0.0232055 -42.63742 0.000000e+00\nAge_c        0.0569645  0.0032039  17.77974 1.014557e-70\n\n\n\nFitted logistic regression model: \\[\\text{logit}(\\widehat{\\pi}(Age)) = -0.989 + 0.057 \\cdot Age\\]\n\nWe will need to reverse the transformation process in slide 24-25 to find the odds ratios"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#slide-transform",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#slide-transform",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "How do we transform our outcome? (1/2)",
    "text": "How do we transform our outcome? (1/2)"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-33-1",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#example-breast-cancer-diagnosis-33-1",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Example: Breast cancer diagnosis (3/3)",
    "text": "Example: Breast cancer diagnosis (3/3)\n\nTranslate the results back to an equation!\nJust going to pull the coefficients so I have a reference as I create the fitted regression model:\n\n\n\n\nsummary(bc_reg)$coefficients\n\n              Estimate Std. Error   z value     Pr(&gt;|z|)\n(Intercept) -0.9894225  0.0232055 -42.63742 0.000000e+00\nAge_c        0.0569645  0.0032039  17.77974 1.014557e-70\n\n\n\nFitted logistic regression model: \\[\\text{logit}(\\widehat{\\pi}(Age)) = -0.989 + 0.057 \\cdot Age\\]\n\nWe will need to reverse the transformation process in slide 24-25 to find the odds ratios\n\nWill do in next week’s lessons\n\n\n\nThis is the fitted line:"
  },
  {
    "objectID": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#reference-individual-components",
    "href": "lectures/05_Simple_logistic_reg/05_Simple_logistic_reg.html#reference-individual-components",
    "title": "Lesson 5: Simple Logistic Regression",
    "section": "Reference: individual components",
    "text": "Reference: individual components"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/Tests_GLMs.html#connection-between-tests-in-linear-models-and-glms",
    "href": "lectures/06_Tests_GLMs/Tests_GLMs.html#connection-between-tests-in-linear-models-and-glms",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Connection between tests in linear models and GLMs",
    "text": "Connection between tests in linear models and GLMs\n\nBecause we used OLS to find the estimate in our linear model class, we could use the following tests:\n\nt-test for single coefficients\nF-test for single coefficients or groups of coefficients\n\nThese tests hinge on the Mean Squared Error (MSE) which we minimized in OLS\n\nThink back to our ANOVA table and how we compared the MSE of one model to another to determine if a covariate explains enough variation of our outcome\n\nIn GLMs, when we use maximum likelihood estimation (MLE), we cannot use t-tests or F-tests\n\nBecause we are now using likelihood to find our estimates (not OLS)\n\nBut we have parallel tests!!\n\nt-test –&gt; Wald test\n\nThese are asymptotically equivalent (as our sample goes to infinity)\n\nF-test –&gt; Likelihood ratio test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/Tests_GLMs.html#reference-hypothesis-testing-using-confidence-intervals",
    "href": "lectures/06_Tests_GLMs/Tests_GLMs.html#reference-hypothesis-testing-using-confidence-intervals",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Reference: hypothesis testing using confidence intervals",
    "text": "Reference: hypothesis testing using confidence intervals"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/Tests_GLMs.html#tests-and-what-theyre-used-for",
    "href": "lectures/06_Tests_GLMs/Tests_GLMs.html#tests-and-what-theyre-used-for",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Tests and what they’re used for",
    "text": "Tests and what they’re used for\n\n\n\n\n\n\n\n\n\n\nWald test\nScore test\nLRT\n\n\n\n\nUsed for single coefficient\n\n\n\n\n\nCan be used to report confidence interval for a single coefficient\n\n\n\n\n\nConfidence interval reported by R for a single coefficient (and most commonly used)\n\n\n\n\n\nUse for multi-level categorical covariate\n\n\n\n\n\nUsed for comparing two models with different (but nested) covariates\n\n\n\n\n\n\n\n\nLesson 6: Tests for Generalized Linear Models"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/Tests_GLMs.html#series-of-poll-everywhere-questions-to-make-the-following-table",
    "href": "lectures/06_Tests_GLMs/Tests_GLMs.html#series-of-poll-everywhere-questions-to-make-the-following-table",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Series of Poll Everywhere questions to make the following table",
    "text": "Series of Poll Everywhere questions to make the following table"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/Tests_GLMs.html#introduction-to-three-tests-in-glm",
    "href": "lectures/06_Tests_GLMs/Tests_GLMs.html#introduction-to-three-tests-in-glm",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Introduction to three tests in GLM",
    "text": "Introduction to three tests in GLM\n\nTo introduce these three tests, we will work on the hypothesis test for a single coefficient\n\nTo be clear: the Likelihood ratio test can be extended to more coefficients\n\nLet’s say we fit a GLM using MLE\n\nWe will continue to use logistic regression as our working example\n\nNow we want to test the significant of a coefficient:\nHypothesis test for an individual coefficient \\(j\\):\n\n\\(H_0: \\beta_j = 0\\)\n\\(H_1: \\beta_j \\neq 0\\)\n\nThree potential tests to use:\n\nWald test\nScore test\nLikelihood ratio test (LRT)"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#connection-between-tests-in-linear-models-and-glms",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#connection-between-tests-in-linear-models-and-glms",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Connection between tests in linear models and GLMs",
    "text": "Connection between tests in linear models and GLMs\n\nIn linear regression, we used ordinary least squares (OLS) to find the best fit model, so we could use the following tests:\n\nt-test for single coefficients\nF-test for single coefficients or groups of coefficients\n\nThese tests hinge on the Mean Squared Error (MSE) which we minimized in OLS and the LINE assumptions\n\n \n\nIn GLMs, when we use maximum likelihood estimation (MLE), we cannot use t-tests or F-tests\n\nBecause we are now using likelihood to find our estimates (not OLS)\n\nBut we have parallel tests in MLE!!\n\nt-test ⟶ Wald test\nF-test ⟶ Likelihood ratio test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#introduction-to-three-tests-in-glm",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#introduction-to-three-tests-in-glm",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Introduction to three tests in GLM",
    "text": "Introduction to three tests in GLM\n\nTo introduce these three tests, we will work on a single coefficient\n\nTo be clear: the Likelihood ratio test can be extended to more coefficients\n\nLet’s say we fit a GLM using MLE\n\nWe will continue to use logistic regression as our working example\n\n\n \n\nNow we want to run a hypothesis test for an individual coefficient \\(j\\):\n\n\\(H_0: \\beta_j = 0\\)\n\\(H_1: \\beta_j \\neq 0\\)\n\nThree potential tests that we use with a Likelihood function are:\n\nWald test\nScore test\nLikelihood ratio test (LRT)"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#glm-random-component",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#glm-random-component",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "GLM: Random Component",
    "text": "GLM: Random Component"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#reference-hypothesis-testing-using-confidence-intervals",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#reference-hypothesis-testing-using-confidence-intervals",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Reference: hypothesis testing using confidence intervals",
    "text": "Reference: hypothesis testing using confidence intervals\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nWhat is the estimate and its confidence interval?\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#series-of-poll-everywhere-questions-to-make-the-following-table",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#series-of-poll-everywhere-questions-to-make-the-following-table",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Series of Poll Everywhere questions to make the following table",
    "text": "Series of Poll Everywhere questions to make the following table"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#tests-and-what-theyre-used-for",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#tests-and-what-theyre-used-for",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Tests and what they’re used for",
    "text": "Tests and what they’re used for\n\n\n\n\n\n\n\n\n\n\nWald test\nScore test\nLRT\n\n\n\n\nUsed to test significance of single coefficient\n\n\n\n\n\nCan be used to report confidence interval for a single coefficient\n\n\n\n\n\nConfidence interval reported by R for a single coefficient (and most commonly used)\n\n\n\n\n\nUse to test significance/contribution to outcome prediction of multi-level categorical covariate\n\n\n\n\n\nUsed for comparing two models with different (but nested) covariates"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Wald test",
    "text": "Wald test\n\nVery similar to a t-test!\n\nBut slightly different because it based in our likelihood function\n\nAssumes test statistic W follows a standard normal distribution under the null hypothesis\nTest statistic: \\[W=\\frac{{\\hat{\\beta}}_1}{se({\\hat{\\beta}}_1)}\\sim N(0,1)\\]\n\nwhere \\(\\widehat{\\beta}_1\\) is a MLE\n\nThe Wald test is a routine output in R (summary() of glm() output)"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#score-test",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#score-test",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Score test",
    "text": "Score test\n\nScore test does not require the computation of MLE for \\(\\beta_1\\), while both likelihood test and Wald test does\n\nOnly need to know \\(\\beta_1\\) under the null\n\nScore test is based on the first and second derivatives of the log-likelihood under the null hypothesis: \\[S=\\frac{\\sum_{i=1}^{n}{x_i(y_i-\\bar{y})}}{\\sqrt{\\bar{y}(1-\\bar{y})\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)^2}} \\sim N(0,1)\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#revisit-the-likelihood-function",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#revisit-the-likelihood-function",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Revisit the likelihood function",
    "text": "Revisit the likelihood function\n\n\n\nLikelihood function: expresses the probability of the observed data as a function of the unknown parameters\n\nFunction that enumerates the likelihood (similar to probability) that we observe the data across the range of potential values of our coefficients\n\nWe often compare likelihoods to see what estimates are more likely given our data\nPlot to right is a simplistic view of likelihood\n\nI have flattened the likelihood that would be a function of \\(\\beta_0\\) and \\(\\beta_1\\) into a 2D plot (instead of 3D: \\(\\beta_0\\) vs. \\(\\beta_1\\) vs. \\(L(\\beta_0, \\beta_1)\\))\n\nI use \\(L\\) to represent the log-likelihood and \\(l\\) to represent the likelihood"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lets-say-we-want-to-look-at-our-previous-model-with-age-and-late-stage-bc-diagnosis",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lets-say-we-want-to-look-at-our-previous-model-with-age-and-late-stage-bc-diagnosis",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Let’s say we want to look at our previous model with age and late stage BC diagnosis",
    "text": "Let’s say we want to look at our previous model with age and late stage BC diagnosis\n\nLet’s start with simple logistic regression with late stage breast cancer diagnosis as the outcome and age as our independent variable\nWe want to fit: \\[\\text{logit}(\\pi(Age)) = \\beta_0 + \\beta_1 \\cdot Age\\]\n\nDon’t forget: \\(\\pi(Age) = P(Y=1 | Age) = P(\\text{Late stage BC diagnosis}| Age)\\)\n\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-1",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-1",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Wald test",
    "text": "Wald test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-2",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-2",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Wald test",
    "text": "Wald test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-1",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-1",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-in-r",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-in-r",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Wald test in R",
    "text": "Wald test in R\n\nlibrary(epiDisplay)\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4\n\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#score-test-1",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#score-test-1",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Score test",
    "text": "Score test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-1",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-1",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test\n\nTo assess the significance of the covariate’s coefficient in the simple logistic regression, we compare the deviance (D) with and without the covariate \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]\nFor a continuous or binary variable, this is equivalent to test: \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\)\nTest statistic for LRT: \\[G=-2ln\\left[\\frac{\\text{likelihood without } x}{\\text{likelihood with } x}\\right]=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-2",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-2",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test\n\nTo assess the significance of the covariate’s coefficient in the simple logistic regression, we compare the deviance (D) with and without the covariate \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]\nFor a continuous or binary variable, this is equivalent to test: \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\)\nTest statistic for LRT: \\[G=-2ln\\left[\\frac{\\text{likelihood without } x}{\\text{likelihood with } x}\\right]=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-3",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-3",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-4",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-4",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-5",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-5",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lrt-what-is-deviance",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lrt-what-is-deviance",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "LRT: what is Deviance?",
    "text": "LRT: what is Deviance?\n\nDeviance: quantifies the difference in likelihoods between a fitted and saturated model\n\nFitted model:\n\nYour proposed fitted model\n\nSaturated model:\n\nA model that contains as many parameters as there are data points = perfect fit\n\nBasically every individual has their own covariate\n\nPerfect fit = maximum possible likelihood\n\n\n\n \n\nAll fitted models will have likelihood less than saturated model"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lrt-what-is-deviance-1",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lrt-what-is-deviance-1",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "LRT: what is Deviance?",
    "text": "LRT: what is Deviance?\n\nThe deviance is mathematically defined as: \\[D=-2[L_{\\text{fitted}}-L_{\\text{saturated}}]\\]\nAn alternative way to write it is: \\[D=-2ln\\left[\\frac{\\text{likelihood of the fitted model}}{\\text{likelihood of the saturated model}}\\right]\\]\nUsing ‘-2’ is to make the deviance follow a chi-square distribution"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#deviance-to-likelihood-ratio-test",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#deviance-to-likelihood-ratio-test",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Deviance to Likelihood Ratio Test",
    "text": "Deviance to Likelihood Ratio Test\n\nIn the LRT, we are NOT comparing the likelihood of saturated model to the fitted model\n\n \n\nWe ARE comparing the Deviance of the model with x and the model without x\n\nWe just use the saturated model to calculate Deviance\nBoth are considered fitted models with their own respective Deviance\n\n\n \n\nSo our LRT is: \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#for-reference",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#for-reference",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "For reference",
    "text": "For reference\n\\[ \\begin{aligned}\nG&=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with }x\\right) \\\\\nG&=-2ln\\left[\\frac{\\text{likelihood of model without } x}{\\text{likelihood of saturated model}}\\right]-\\left(-2ln\\left[\\frac{\\text{likelihood of model with } x}{\\text{likelihood of saturated model}}\\right]\\right) \\\\\nG&=-2ln\\left[\\frac{\\text{likelihood of model without } x}{\\text{likelihood of saturated model}}\\times\\frac{\\text{likelihood of saturated model}}{\\text{likelihood of model with } x}\\right] \\\\\nG&=-2ln\\left[\\frac{\\text{likelihood of model without } x}{\\text{likelihood of model with }}\\right] \\\\\nG&=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-4",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-4",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lrt-in-r",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lrt-in-r",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "LRT in R",
    "text": "LRT in R\n\nlibrary(lmtest)\nbc_age = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nbc_int = glm(Late_stage_diag ~ 1, data = bc, family = binomial)\nlmtest::lrtest(bc_age, bc_int)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Age_c\nModel 2: Late_stage_diag ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   2 -5754.8                         \n2   1 -5930.5 -1 351.27  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#all-three-tests-together",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#all-three-tests-together",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "All three tests together",
    "text": "All three tests together\n\nUCLA FAQ on Tests"
  },
  {
    "objectID": "quizzes.html",
    "href": "quizzes.html",
    "title": "Quizzes",
    "section": "",
    "text": "Quiz\nAnswer key\n\n\n\n\n1\n\n\n\n2\n\n\n\n3\n\n\n\n\n\nQuiz Instructions\n\nI have written a “30 minute” quiz, but there is no time limit. You will have from Monday at 2pm to Wednesday at 1pm to finish the quiz. The quiz will be administered through Sakai under “Tests & Quizzes.”\nThe quiz is open book and open notes. You may use books other than the class textbook, you may use anything on our course webpage, and you may use reference websites (like Wikipedia, Googling expected value of specific distribution, etc.).\nNo cheating will be tolerated. Cheating includes:\n\nUsing ChatGPT\nUsing question and answer threads typically seen on sites like StackExchange, WikiHow, Quora, Reddit, StackOverflow, Chegg, etc.\nAsking other students in the room or looking at other students’ quiz work.\n\nEach multiple choice question is worth 3 points. The free response questions are labelled with their point value.\nIf taking the quiz in class, you may use headphones.\n\n\n\nQuiz 1 Information\n\nWill cover the learning objectives in Lesson 1-4, with more emphasis on the new material presented in Lessons 3-4\n\nFrom Lesson 2 there will be a question about the Normal approximation of the binomial distribution and a question about which test to use for a set of variables"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#which-test-to-use",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#which-test-to-use",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Which test to use?",
    "text": "Which test to use?\n\nAll three tests are asymptotically equivalent\n\nAs sample approaches infinity\n\nFor testing significance of single covariate coefficient:\n\nLRT\n\nWald and score are only approximations of LRT\nFor smaller samples, LRT better\n\nWald test is very convenient\n\nAutomatically performed in R\nDoes not need to estimate two models (LRT does)\nGood for constructing confidence intervals of coefficients and odds ratios\n\nScore test\n\nDoes not need to estimate two models (LRT does)\nI don’t really see people use this…"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#revisit-our-previous-model-with-late-stage-bc-diagnosis-and-age",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#revisit-our-previous-model-with-late-stage-bc-diagnosis-and-age",
    "title": "Lesson 6: Tests for Generalized Linear Models",
    "section": "Revisit our previous model with late stage BC diagnosis and age",
    "text": "Revisit our previous model with late stage BC diagnosis and age\n\nSimple logistic regression with late stage breast cancer diagnosis as outcome and age as independent variable\n\n\n\n\\[\\text{logit}(\\pi(Age)) = \\beta_0 + \\beta_1 \\cdot Age\\]\n\nDon’t forget: \\(\\pi(Age) = P(Y=1 | Age)\\)\n\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#revisit-previous-model-with-late-stage-bc-diagnosis-and-age",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#revisit-previous-model-with-late-stage-bc-diagnosis-and-age",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Revisit previous model with late stage BC diagnosis and age",
    "text": "Revisit previous model with late stage BC diagnosis and age\n\n\n\nSimple logistic regression model: \\[\\text{logit}(\\pi(Age)) = \\beta_0 + \\beta_1 \\cdot Age\\]\n\n\n \nDon’t forget: \\(\\pi(Age) = P(Y=1 | Age)\\)\n\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-13",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-13",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Wald test (1/3)",
    "text": "Wald test (1/3)\n\nVery similar to a t-test!\n\nBut slightly different because it based in our likelihood function\n\nAssumes test statistic W follows a standard normal distribution under the null hypothesis\nTest statistic: \\[W=\\frac{{\\hat{\\beta}}_j}{se({\\hat{\\beta}}_j)}\\sim N(0,1)\\]\n\nwhere \\(\\widehat{\\beta}_j\\) is a MLE of coefficient \\(j\\)\n\n95% Wald confidence interval: \\[{\\hat{\\beta}}_1\\pm1.96 \\cdot SE_{{\\hat{\\beta}}_j}\\]\nThe Wald test is a routine output in R (summary() of glm() output)\n\nIncludes \\(SE_{{\\hat{\\beta}}_j}\\) and can easily find confidence interval with tidy()"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-23",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-23",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Wald test (2/3)",
    "text": "Wald test (2/3)"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-33",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-33",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Wald test (3/3)",
    "text": "Wald test (3/3)"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-procedure-with-confidence-intervals",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#wald-test-procedure-with-confidence-intervals",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Wald test procedure with confidence intervals",
    "text": "Wald test procedure with confidence intervals\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the confidence interval and determine if it overlaps with null\n\nOverlap with null (usually 0 for coefficient) = fail to reject null\nNo overlap with null (usually 0 for coefficient) = reject null\n\nWrite a conclusion to the hypothesis test\n\nWhat is the estimate and its confidence interval?\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#ive-never-explicitly-said-this",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#ive-never-explicitly-said-this",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "I’ve never explicitly said this…",
    "text": "I’ve never explicitly said this…\n\nBecause we are in a public health class, we are often analyzing data with sensitive outcomes\n\n \n\nIf you ever need a moment in class because of our topic, feel free to just step out or leave and privately view the lecture\n\n \n\nIf you need extra time on your assignments because you have an emotional response to lectures/homework/lab, just let me know! Extenuating circumstance!"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nWald test for age coefficient\n\n\nInterpret the coefficient for age in our model of late stage breast cancer diagnosis.\n\n\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the confidence interval and determine if it overlaps with null\nWrite a conclusion to the hypothesis test\n\n\n\n\nNote\n\n\nI don’t want us to get fixated on this interpretation. This is more to introduce the process, BUT it’s MUCH better to interpret the coefficient in terms of OR (next class)."
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-1",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-1",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nWald test for age coefficient\n\n\nInterpret the coefficient for age in our model of late stage breast cancer diagnosis.\n\n\n\nSet the level of significance \\(\\alpha\\) \\[\\alpha=0.05\\]\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\[\\begin{aligned}\nH_0 &: \\beta_{Age} = 0 \\\\\nH_1 &: \\beta_{Age} \\neq 0 \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-2",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-2",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nWald test for age coefficient\n\n\nInterpret the coefficient for age in our model of late stage breast cancer diagnosis.\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nlibrary(epiDisplay)\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-3",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-3",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nWald test for age coefficient\n\n\nInterpret the coefficient for age in our model of late stage breast cancer diagnosis.\n\n\n\n\n\nWrite a conclusion to the hypothesis test\n\nFor every one year increase in age, the log-odds of late stage breast cancer diagnosis increases 0.057 (95% CI: 0.051, 0.063).\nThere is sufficient evidence that age an breast cancer diagnosis are associated.\n\n\n\nNote\n\n\nI don’t want us to get fixated on this interpretation. This is more to introduce the process, BUT it’s MUCH better to interpret the coefficient in terms of OR (next class)."
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-4",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-4",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nLRT\n\n\nDetermine if the model including age is more likely than model without age. Aka: Is age associated with late stage breast cancer diagnosis?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-13",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-13",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Likelihood ratio test (1/3)",
    "text": "Likelihood ratio test (1/3)\n\nLikelihood ratio test answers the question:\n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate or the model omitting the covariate?\nAka: Which model is more likely given our data: model including the covariate or the model omitting the covariate?\n\n\n \n\nTest a single coefficient by comparing different models\n\nVery similar to the F-test\n\n\n \n\nImportant: LRT can be used conduct hypothesis tests for multiple coefficients\n\nJust like F-test, we can test a single coefficient, continuous/binary covariate, multi-level covariate, or multiple covariates"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-23",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-23",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Likelihood ratio test (2/3)",
    "text": "Likelihood ratio test (2/3)\n\nTo assess the significance of a continuous/binary covariate’s coefficient in the simple logistic regression, we compare the deviance (D) with and without the covariate \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]\n\n \n\nFor a continuous or binary variable, this is equivalent to test: \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\)\nTest statistic for LRT: \\[G=-2ln\\left[\\frac{\\text{likelihood without } x}{\\text{likelihood with } x}\\right]=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-23-1",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-23-1",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Likelihood ratio test (2/3)",
    "text": "Likelihood ratio test (2/3)\n\nTo assess the significance of a continuous/binary covariate’s coefficient in the simple logistic regression, we compare the deviance (D) with and without the covariate \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]\n\n \n\nFor a continuous or binary variable, this is equivalent to test: \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\)\nTest statistic for LRT: \\[G=-2ln\\left[\\frac{\\text{likelihood without } x}{\\text{likelihood with } x}\\right]=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-23-2",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-23-2",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Likelihood ratio test (2/3)",
    "text": "Likelihood ratio test (2/3)\n\nTo assess the significance of a continuous/binary covariate’s coefficient in the simple logistic regression, we compare the deviance (D) with and without the covariate \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]\n\n \n\nFor a continuous or binary variable, this is equivalent to test: \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\)\nTest statistic for LRT: \\[G=-2ln\\left[\\frac{\\text{likelihood without } x}{\\text{likelihood with } x}\\right]=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-33",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#likelihood-ratio-test-33",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nUnder the null hypothesis, with adequate sample size, LRT statistic follows a chi-square distribution: \\[G \\sim \\chi^2(df)\\]\n\n\\(df = (\\# \\text{coefficients in larger model}) − (\\# \\text{coefficients in smaller model})\\)\n\n\n \n\nIf we are testing a single coefficient, like age, then \\(df=1\\)"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lrt-procedure",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#lrt-procedure",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "LRT procedure",
    "text": "LRT procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-5",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-5",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nLRT\n\n\nDetermine if the model including age is more likely than model without age. Aka: Is age associated with late stage breast cancer diagnosis?\n\n\n\nSet the level of significance \\(\\alpha\\) \\[\\alpha=0.05\\]\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\[\\begin{aligned}\nH_0 &: \\beta_{Age} = 0 \\\\\nH_1 &: \\beta_{Age} \\neq 0 \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-6",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-6",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nLRT\n\n\nDetermine if the model including age is more likely than model without age. Aka: Is age associated with late stage breast cancer diagnosis?\n\n\n\nCalculate the test statistic and p-value\n\n\nlibrary(lmtest)\nbc_age = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nbc_int = glm(Late_stage_diag ~ 1, data = bc, family = binomial)\nlmtest::lrtest(bc_age, bc_int)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Age_c\nModel 2: Late_stage_diag ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   2 -5754.8                         \n2   1 -5930.5 -1 351.27  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-7",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#example-bc-diagnosis-and-age-7",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nLRT\n\n\nDetermine if the model including age is more likely than model without age. Aka: Is age associated with late stage breast cancer diagnosis?\n\n\n\nWrite a conclusion to the hypothesis test\n\nWe reject the null hypothesis that the coefficient corresponding to age is 0 (\\(p-value &lt;&lt; 0.05\\)). There is sufficient evidence that there is an association between age and late stage breast cancer diagnosis."
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#so-how-would-the-wald-test-and-lrt-show-up-in-research",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#so-how-would-the-wald-test-and-lrt-show-up-in-research",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "So how would the Wald test and LRT show up in research?",
    "text": "So how would the Wald test and LRT show up in research?\n\nWald test\n\nOften used when reporting estimates\nGenerally presented using a forest plot or table of ORs or RRs\n\nThen we highlight the specific variable of interest in text\nWill include the OR/RR estimate (not the coefficient like we saw today) with the 95% CI and proper interpretation of result\n\n\n\n \n\nLRT\n\nOften when performing model selection and comparing two models\n\nReporting model selection Cross Validated post\n\nOften does not show up explicitly in our reports, but is essential to get to our final model!!\n\n\n\n\nLesson 6: Tests for GLMs using Likelihood function"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-2",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-2",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-3",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-3",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-5",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-5",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-6",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-6",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Poll Everywhere Question 6",
    "text": "Poll Everywhere Question 6"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-7",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-7",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Poll Everywhere Question 7",
    "text": "Poll Everywhere Question 7"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-8",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#poll-everywhere-question-8",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Poll Everywhere Question 8",
    "text": "Poll Everywhere Question 8"
  },
  {
    "objectID": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#tests-and-what-theyre-used-for-filled",
    "href": "lectures/06_Tests_GLMs/06_Tests_GLMs.html#tests-and-what-theyre-used-for-filled",
    "title": "Lesson 6: Tests for GLMs using Likelihood function",
    "section": "Tests and what they’re used for (filled)",
    "text": "Tests and what they’re used for (filled)\n\n\n\n\n\n\n\n\n\n\nWald test\nScore test\nLRT\n\n\n\n\nUsed to test significance of single coefficient\n\n\n\n\n\nCan be used to report confidence interval for a single coefficient\n\n\n\n\n\nConfidence interval reported by R for a single coefficient (and most commonly used)\n\n\n\n\n\nUse to test significance/contribution to outcome prediction of multi-level categorical covariate\n\n\n\n\n\nUsed for comparing two models with different (but nested) covariates"
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_02.html",
    "href": "project/LastName_FirstInit_Lab_02.html",
    "title": "Lab 2",
    "section": "",
    "text": "This is your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n\nThe purpose of this lab is to explore our data further, set up the unadjusted odds ratio, and create code to later help us present our final model.\n\n\n\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback."
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_02.html#directions",
    "href": "project/LastName_FirstInit_Lab_02.html#directions",
    "title": "Lab 2",
    "section": "",
    "text": "This is your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n\nThe purpose of this lab is to explore our data further, set up the unadjusted odds ratio, and create code to later help us present our final model.\n\n\n\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback."
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_02.html#lab-activities",
    "href": "project/LastName_FirstInit_Lab_02.html#lab-activities",
    "title": "Lab 2",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format (1 sentence). You can change the wording if you’d like, but please make sure it is still clear. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Make sure variables are coded correctly\n\n\n\n\n\n\nTask\n\n\n\n\nUse class() to determine the class of each of the 11 variables you selected from Lab 1 (including the outcome).\nChange the variable type to the appropriate type.\n\n\n\n\n\n2.3 Consider potential confounders and effect modifiers\n\n\n\n\n\n\nTask\n\n\n\nFill in the below table (or any other way you wish to present the same information).\n\n\n\n\n\n\n\n\n\n\nVariable name\nConfounder, Effect modifier, or nothing?\nReasoning (1-2 sentences)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 Create contingency tables for categorical predictors\n\n\n\n\n\n\nTask\n\n\n\n\nCreate contingency tables for all categorical covariates with food insecurity.\nTake note of any cell counts that are less than 10\n\n\n\n\n\n2.5 Bivariate exploratory data analysis\n\n\n\n\n\n\nTask\n\n\n\n\nUse ggpairs() (introduced in BSTA 512 Lesson 13) to quickly look at the relationship between variables.\nList predictors with which there is a clear trend with food insecurity.\n\n\n\n\n\n2.6 Fit simple logistic regression\n\n\n\n\n\n\nTask\n\n\n\n\nUsing glm(), run a logistic regression with food insecurity and your main variable of interest.\nDisplay the unadjusted odds ratio of the regression. You can use logistic.display()\nInterpret the unadjusted odds ratio (with 95% confidence interval). If you’re main variable is multi-level, then you will need to interpret multiple odds ratios.\n\n\n\n\n\n2.7 Plot the predicted probability\n\n\n\n\n\n\nTask\n\n\n\nPlot or make a table of your predicted probabilities."
  },
  {
    "objectID": "project/Project_report_instructions.html",
    "href": "project/Project_report_instructions.html",
    "title": "Project Report Instructions",
    "section": "",
    "text": "Project template\n\n\n\nYou may use this project template to get started on the report. It is your responsibility to meet the formatting guidelines below!!\nDO NOT USE SITE PAGE (“Project Report Instructions”, current page) as your template!!\n\n\n\n\nProject reports serve as a great way to communicate the knowledge learned in our class and connect it to context within research. It is important that we can take a step back from the numbers and analysis to see what questions linear regression can help us answer.\nIt is really important for you to look back through your labs and connect the work!!\n\n\n\n\nThe report will be written in Quarto. Turn in both the qmd and html files\n\nNo code should appear in the html document\n\nThis means all R code chunks should have #| echo: false\nThis also means warnings and messages should be turned off\n\n\nThe report should be 10 - 14 paragraphs long\n\nIn 512, many lab reports were a little too long!\nRemember, I know mostly what you did in the labs! This report is meant to synthesize that work into a coherent message/story!\n\nThis often means details of our analysis are lost.\n\n\nTables and figures should NOT have variable names as they appear in the data frame\n\nVariable names should be understood by a reader\nVariable names should be written in full words\nInclude a title or caption for all figures\nFigure and tables appear on same page or close to same page where they are first referenced\nTables and figures are an appropriate size in the html - Nicky is able to read all words in figures and tables\n\nWriting, spelling, and grammar should be admissable\n\nThis means I can generally follow your thought/what you are trying to communicate\nSome spelling and grammar mistakes are allowed\n\nI will not take off points if there are a few sprinkled in\nIf every or close to every sentence has mistakes, then I will take off\n\n\nSectioning of the report\n\nMain sections that are required: Introduction, Statistical Methods, Results, Discussion, Conclusion, Reflection, and References\n\nYou may have an appendix to include additional figures!\n\nOther sections that might help group specific methods or results\n\nTitle information at the top of the html\n\nThis includes the title itself, your name, and the date\n\n\n\n\n\n\n\n\nThe project report is a separate file from the labs\n\n\n\nYou can save tables and figures from labs or separate files, then load them in the report\n\nSave R objects in analyses file:\n\nSuppose you named the Table 1 as table1\nsave(table1, file = \"table1.Rdata\")\n\nLoad R objects in report file: load(file = \"table1.Rdata\")\n\n\n\n\n\n\nThe following are examples of reports from BSTA 513 with the feedback that I gave them.\nPlease note that 513 uses a different type of outcome than our class. These examples are meant to help guide you with the formatting and some appropriate content.\nAlso note that these were converted to PDFs so I could write in feedback. Some of the tables and figure sizes were distorted. They need to be legible in the html.\n\nReport 1 with my feedback\nReport 2 with my feedback\n\nThe above reports have code showing in their html. Remember that I am asking you to hide all code, warnings, and messages.\n\n\n\n\nGradingRubricMore clarifications\n\n\nThe project report is out of 36 points. Note that the Statistical Methods and Results sections are graded on an 8-point scale, while all other components are graded on a 4-point scale.\n\n\n\n\n\n\nSome words on my grading process\n\n\n\nIn the final lab, I gave you the option to do LASSO regression and focus on prediction. I know this created some confusion since we mostly set up the project as a question of association in Lab 1-3. We can still interpret the odds ratios from LASSO regression. I will be fairly lenient if reports are confused between prediction and association aims. I will try to give feedback on it, but I will not penalize any minor confusions.\nAnother word: My process starts harsh. I want to give you as much feedback as possible, and this will also reflect in lower assigned scores. At this point, I put the report grades into Sakai. I check to see if anyone’s overall course letter grade goes down. If less than ~5 course grades go down, then I revisit their project reports. If their report fails to demonstrate the most important learning objectives from the course, then I will keep the lower grade. If they demonstrate an understanding of the most important learning objectives, then I will adjust their score to increase their course grade. If more than 5 grades go down, then I revisit everyone’s reports. I will make a class wide grade bump in all reports.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with very few grammatical or spelling errors. With little editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with some (around 2 per section) grammatical or spelling errors. With some editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but have many grammatical or spelling errors. With major editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but are very hard to follow due to grammar mistakes.\nLab not submitted on Sakai (or by email if late) with .html file. Report is not written with complete sentences. With major editing, the report can be distributed.\n\n\nFigures and work\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. For the most part, figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message. A few mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look semi-professional, are not so easily interpreted by the reader, and convey the intended message but after some work by the reader. Some mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables do not look professional, are not easily interpreted by the reader, and/or do not convey the intended message. Many mistakes in the figures are made.\nRequested output is not displayed, Missing one or more figures.\n\n\nIntroduction\nProvides a good background for the research question, includes motivation for the question, and references previous research that justifies this analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nDoes not provide a background that connects to the research question. Motivation and previous research are not mentioned.\nNo introduction included.\n\n\nMethods (8 points)\nDescribes statistical methods concisely and highlights pertinent information to the reader (listed Sections below). Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Some incorrect analyses included in the description.\nDescribes statistical methods, but lacks clarity. Demonstrates a lack of understanding about the overall process of regression analysis. Incorrect analyses included in the description.\nNo methods included.\n\n\nResults (8 points)\nCorrectly interprets coefficients for the explanatory variable and identifies any other interesting trends. Highlights pertinent results to the reader (listed Sections below).\nCorrectly interprets coefficients, but does correctly incorporate the interaction (if in the model). Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients. Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients.Omits pertinent results to the reader (listed Sections below).\nNo results included.\n\n\nDiscussion\nThoroughly and concisely discusses limitations and considerations of the results, and their consequences.\nDiscusses limitations and considerations of the results and their consequences, but misses some big considerations.\nDiscusses limitations and considerations of the results, but does not discuss the consequences.\nDiscusses limitations and considerations of the results, but misses many considerations and does not discuss consequences.\nNo discussion included.\n\n\nConclusion and References\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are mostly cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but focus is not on the research question) and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but not the focus at all) and statistical caveats are not described. References are not cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is not answered. Or references are not included at all.\n\n\nReflection\nDiscusses all the labs. Reflection demonstrates an understanding of each lab’s purpose and overarching connection from the labs to overall project.\nDiscusses 3 out of 4 labs.\nDiscusses 2 out of 4 labs.\nDiscusses 1 out of 4 labs.\nNo reflection included\n\n\n\n\n\n\nIn formatting, an example of a report with little editing needed is one that has zero to some grammar or spelling mistakes, no code chunks showing, and no output warnings nor messages showing.\nProfessional figures mean\n\nI can read the words and numbers in the html\n\nVariable names are converted from the data frame version to readable text\nFor example: iam_001 does not show up on axes, instead something like: Response to \"Currently, I am...\"\n\nColors are only used if conveying information\nIntended message of the figure is easily understood\n\nIf you are trying to show a trend of proportion of food insecurity vs. an ordered categorical variable, then the variable is ordered on the x-axis\n\n\nFor the references\n\nI will not be overly critical about the formatting\nBy consistency, I mean that you if you are citing things like (Last Name, Year) it doesn’t suddenly change to number citations.\nIf you would like to use Quarto’s citation tool, you can! I actually pair it with Zotero and it works beautifully! (But I would not embark on this if you haven’t used Zotero before)\n\n\n\n\n\n\n\n\n\nThe only figure that I require in the report is Table 1\nIf you are focusing on prediction (LASSO) than you may not need to present your odds ratios at all.\n\nI think it will be good to do this since LASSO does allow for interpretations and we framed our project as such\n\nIf you have a lot of predictors (i.e. you included categorical income or family size), please do not put a giant table of forest plot in the middle of your results\n\nEven if you did not include these variables, you can keep the full table/plot in the appendix\nPut this big table or plot in the appendix!\n\nWhen it comes to reporting the odds ratios: you can simply include the estimated odds ratio in text (as an interpretation)\n\nThis should ONLY be for your variable of interest!\nIf your variable has multiple categories, discuss the trend and then report an example interpretation to demonstrate how each category should be interpreted\n\nYou may wish to include a table showing all the odds ratios for each group\n\n\nAnother important table may include model assessment values like AUC if you are comparing models\n\nIf you only have one model, you may not need to make a table\n\n\n \n\nWhen deciding to create a table or figure, ask yourself: Does this help the reader understand what I am saying in text? Can it be conveyed just as well with words?\n\nThings that are conveyed better with tables/figures\n\nComparisons of the same measurement across different models\nMultiple calculated values for the same measurement\n\nLike estimated odds ratios for all variables in a model"
  },
  {
    "objectID": "project/Project_report_instructions.html#directions",
    "href": "project/Project_report_instructions.html#directions",
    "title": "Project Report Instructions",
    "section": "",
    "text": "Project template\n\n\n\nYou may use this project template to get started on the report. It is your responsibility to meet the formatting guidelines below!!\nDO NOT USE SITE PAGE (“Project Report Instructions”, current page) as your template!!\n\n\n\n\nProject reports serve as a great way to communicate the knowledge learned in our class and connect it to context within research. It is important that we can take a step back from the numbers and analysis to see what questions linear regression can help us answer.\nIt is really important for you to look back through your labs and connect the work!!\n\n\n\n\nThe report will be written in Quarto. Turn in both the qmd and html files\n\nNo code should appear in the html document\n\nThis means all R code chunks should have #| echo: false\nThis also means warnings and messages should be turned off\n\n\nThe report should be 10 - 14 paragraphs long\n\nIn 512, many lab reports were a little too long!\nRemember, I know mostly what you did in the labs! This report is meant to synthesize that work into a coherent message/story!\n\nThis often means details of our analysis are lost.\n\n\nTables and figures should NOT have variable names as they appear in the data frame\n\nVariable names should be understood by a reader\nVariable names should be written in full words\nInclude a title or caption for all figures\nFigure and tables appear on same page or close to same page where they are first referenced\nTables and figures are an appropriate size in the html - Nicky is able to read all words in figures and tables\n\nWriting, spelling, and grammar should be admissable\n\nThis means I can generally follow your thought/what you are trying to communicate\nSome spelling and grammar mistakes are allowed\n\nI will not take off points if there are a few sprinkled in\nIf every or close to every sentence has mistakes, then I will take off\n\n\nSectioning of the report\n\nMain sections that are required: Introduction, Statistical Methods, Results, Discussion, Conclusion, Reflection, and References\n\nYou may have an appendix to include additional figures!\n\nOther sections that might help group specific methods or results\n\nTitle information at the top of the html\n\nThis includes the title itself, your name, and the date\n\n\n\n\n\n\n\n\nThe project report is a separate file from the labs\n\n\n\nYou can save tables and figures from labs or separate files, then load them in the report\n\nSave R objects in analyses file:\n\nSuppose you named the Table 1 as table1\nsave(table1, file = \"table1.Rdata\")\n\nLoad R objects in report file: load(file = \"table1.Rdata\")\n\n\n\n\n\n\nThe following are examples of reports from BSTA 513 with the feedback that I gave them.\nPlease note that 513 uses a different type of outcome than our class. These examples are meant to help guide you with the formatting and some appropriate content.\nAlso note that these were converted to PDFs so I could write in feedback. Some of the tables and figure sizes were distorted. They need to be legible in the html.\n\nReport 1 with my feedback\nReport 2 with my feedback\n\nThe above reports have code showing in their html. Remember that I am asking you to hide all code, warnings, and messages.\n\n\n\n\nGradingRubricMore clarifications\n\n\nThe project report is out of 36 points. Note that the Statistical Methods and Results sections are graded on an 8-point scale, while all other components are graded on a 4-point scale.\n\n\n\n\n\n\nSome words on my grading process\n\n\n\nIn the final lab, I gave you the option to do LASSO regression and focus on prediction. I know this created some confusion since we mostly set up the project as a question of association in Lab 1-3. We can still interpret the odds ratios from LASSO regression. I will be fairly lenient if reports are confused between prediction and association aims. I will try to give feedback on it, but I will not penalize any minor confusions.\nAnother word: My process starts harsh. I want to give you as much feedback as possible, and this will also reflect in lower assigned scores. At this point, I put the report grades into Sakai. I check to see if anyone’s overall course letter grade goes down. If less than ~5 course grades go down, then I revisit their project reports. If their report fails to demonstrate the most important learning objectives from the course, then I will keep the lower grade. If they demonstrate an understanding of the most important learning objectives, then I will adjust their score to increase their course grade. If more than 5 grades go down, then I revisit everyone’s reports. I will make a class wide grade bump in all reports.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with very few grammatical or spelling errors. With little editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences with some (around 2 per section) grammatical or spelling errors. With some editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but have many grammatical or spelling errors. With major editing, the report can be distributed.\nLab submitted on Sakai (or by email if late) with .html file. Report is written in complete sentences, but are very hard to follow due to grammar mistakes.\nLab not submitted on Sakai (or by email if late) with .html file. Report is not written with complete sentences. With major editing, the report can be distributed.\n\n\nFigures and work\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. For the most part, figures and tables look professional, are easily interpreted by the reader, and easily convey the intended message. A few mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables look semi-professional, are not so easily interpreted by the reader, and convey the intended message but after some work by the reader. Some mistakes in the figures are made.\nAll requested output is displayed, including 2 required figures and tables, and at least one additional figure. Figures and tables do not look professional, are not easily interpreted by the reader, and/or do not convey the intended message. Many mistakes in the figures are made.\nRequested output is not displayed, Missing one or more figures.\n\n\nIntroduction\nProvides a good background for the research question, includes motivation for the question, and references previous research that justifies this analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nProvides a decent background for the research question and includes motivation for the question. Previous research is mentioned, but feels disconnected to the current analysis.\nDoes not provide a background that connects to the research question. Motivation and previous research are not mentioned.\nNo introduction included.\n\n\nMethods (8 points)\nDescribes statistical methods concisely and highlights pertinent information to the reader (listed Sections below). Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Demonstrates proper analyses were performed.\nDescribes statistical methods and highlights pertinent information to the reader (listed Sections below). Details were omitted or added that were not needed to explain the overarching methods. Some incorrect analyses included in the description.\nDescribes statistical methods, but lacks clarity. Demonstrates a lack of understanding about the overall process of regression analysis. Incorrect analyses included in the description.\nNo methods included.\n\n\nResults (8 points)\nCorrectly interprets coefficients for the explanatory variable and identifies any other interesting trends. Highlights pertinent results to the reader (listed Sections below).\nCorrectly interprets coefficients, but does correctly incorporate the interaction (if in the model). Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients. Highlights pertinent results to the reader (listed Sections below).\nIncorrectly interprets coefficients.Omits pertinent results to the reader (listed Sections below).\nNo results included.\n\n\nDiscussion\nThoroughly and concisely discusses limitations and considerations of the results, and their consequences.\nDiscusses limitations and considerations of the results and their consequences, but misses some big considerations.\nDiscusses limitations and considerations of the results, but does not discuss the consequences.\nDiscusses limitations and considerations of the results, but misses many considerations and does not discuss consequences.\nNo discussion included.\n\n\nConclusion and References\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are mostly cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is answered and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but focus is not on the research question) and statistical caveats described to non-technical person. References are sometimes cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is somewhat answered (but not the focus at all) and statistical caveats are not described. References are not cited consistently within the report, and in the Reference section. This includes the data source!\nFor the conclusion, main research question is not answered. Or references are not included at all.\n\n\nReflection\nDiscusses all the labs. Reflection demonstrates an understanding of each lab’s purpose and overarching connection from the labs to overall project.\nDiscusses 3 out of 4 labs.\nDiscusses 2 out of 4 labs.\nDiscusses 1 out of 4 labs.\nNo reflection included\n\n\n\n\n\n\nIn formatting, an example of a report with little editing needed is one that has zero to some grammar or spelling mistakes, no code chunks showing, and no output warnings nor messages showing.\nProfessional figures mean\n\nI can read the words and numbers in the html\n\nVariable names are converted from the data frame version to readable text\nFor example: iam_001 does not show up on axes, instead something like: Response to \"Currently, I am...\"\n\nColors are only used if conveying information\nIntended message of the figure is easily understood\n\nIf you are trying to show a trend of proportion of food insecurity vs. an ordered categorical variable, then the variable is ordered on the x-axis\n\n\nFor the references\n\nI will not be overly critical about the formatting\nBy consistency, I mean that you if you are citing things like (Last Name, Year) it doesn’t suddenly change to number citations.\nIf you would like to use Quarto’s citation tool, you can! I actually pair it with Zotero and it works beautifully! (But I would not embark on this if you haven’t used Zotero before)\n\n\n\n\n\n\n\n\n\nThe only figure that I require in the report is Table 1\nIf you are focusing on prediction (LASSO) than you may not need to present your odds ratios at all.\n\nI think it will be good to do this since LASSO does allow for interpretations and we framed our project as such\n\nIf you have a lot of predictors (i.e. you included categorical income or family size), please do not put a giant table of forest plot in the middle of your results\n\nEven if you did not include these variables, you can keep the full table/plot in the appendix\nPut this big table or plot in the appendix!\n\nWhen it comes to reporting the odds ratios: you can simply include the estimated odds ratio in text (as an interpretation)\n\nThis should ONLY be for your variable of interest!\nIf your variable has multiple categories, discuss the trend and then report an example interpretation to demonstrate how each category should be interpreted\n\nYou may wish to include a table showing all the odds ratios for each group\n\n\nAnother important table may include model assessment values like AUC if you are comparing models\n\nIf you only have one model, you may not need to make a table\n\n\n \n\nWhen deciding to create a table or figure, ask yourself: Does this help the reader understand what I am saying in text? Can it be conveyed just as well with words?\n\nThings that are conveyed better with tables/figures\n\nComparisons of the same measurement across different models\nMultiple calculated values for the same measurement\n\nLike estimated odds ratios for all variables in a model"
  },
  {
    "objectID": "project/Project_report_instructions.html#sections",
    "href": "project/Project_report_instructions.html#sections",
    "title": "Project Report Instructions",
    "section": "2 Sections",
    "text": "2 Sections\n\n2.1 Title\n\nPurpose: Create an identifiable name for your research project that includes the main research question’s variables and gives some context to the analysis or results\n\n\n\n2.2 Introduction\n\nLength: 1-2 paragraphs\nPurpose: Introduce the project motivation, data, and research question. It also includes any background information relevant for understanding the analysis and relevant previous work.\nThis section is non-technical.\n\nBy reading just the introduction, someone without a technical background should have an idea of what they study was about, and why it is important\n\nYou may start with the introduction written in Lab 1, but you should edit it and make sure it flows into your report well!\nShould contain some references\nShould include a sentence that states your research question (but NOT using a question). For example: “This study investigates the association between food insecurity and age.”\n\n\n\n2.3 Statistical Methods\n\nLength: 3-5 paragraphs\nPurpose: Describe the analyses that were conducted and methods used to select variables and check diagnostics\nImportant to keep in mind: methods typically describe your approach and process, not the results of that process\n\nFor example: I might say “We investigated the linearity of each continuous covariate visually. If continuous variables were not linear, then we divided the variable into categories using existing guidelines from &lt;insert reference here&gt; or creating quartiles.”\n\nIn the methods section, I would NOT say: “We investigated the linearity of each continuous covariate visually. We found that age was not linearly related to log-odds of food insecurity. Thus, we categorized age into the following groups: ___, ____, ____, ____, and ____.”\n\nThe last two sentences about age would be more appropriate in the Results section\n\n\n\nSome important methods to discuss (You may divide these into your sections, not necessarily with these names)\n\nGeneral approach to the dataset\n\n3-5 sentences\nDid you need to do any quality control?\nMissing data: complete case analysis or imputation based on Lab 3 choices\n\n1 sentence\nCan be included in the Exploratory data analysis section\n\n\nVariable transformations\n\nThis includes a description of analyses for Table 1 and what statistics were used to summarize the variables\n\nMore on creation of Table 1, not discussing the results of Table 1\n\nIncludes (not required)\n\nCategorizing a continuous variable (even if performed in model selection)\nDid you use family size/household size/number of children as categorical or continuous?\nDid you make any transformations to income?\nUsing scoring for an ordered categorical variable (that is not your explanatory variable)\n\n1 sentence per changed variable\n\nMaking a categorical variable a factor in our R code is NOT a change!\n\n\nModel building: we performed purposeful selection OR LASSO regression\n\n3 sentences max\nThis is where we discuss our process from Lab 4\nFor purposeful selection, includes:\n\nDescribe purposeful selection: combining existing literature, clinical significance, and analysis\nHow did you build the model? Describe the process in ONE SENTENCE\nDid you consider confounders and effect modifiers?\n\nFor LASSO:\n\nMention that you used LASSO regression with important facts like the penalty used, percent testing set, included interactions in the selection process\n\n\nModel diagnostics and model fit\n\n2-5 sentences\nThis is where we discuss our process from Lab 4\nIncludes\n\nProcess of investigating model diagnostics and fit\nIf assumptions were not met, what process did you use to fix it?\nAgain, we will NOT discuss the results of the model diagnostics\n\nFor example: “We investigate the change in Pearson residual for every observation. For observations with large changes (exceeding change of 4), we check the feasibility of their measurements.\n\n\n\n\n\n\n\n2.4 Results\n\nLength: ~2-3 paragraphs\nPurpose: Relay the results from our sample’s analysis typically focusing on the numbers and interpretations\n\nThe goal is not to interpret every single variable in the model but rather to show that you are proficient in using the model output to address the research question, using the interpretations to support your conclusions.\nFocus on the variables that help you answer the research question and that provide relevant context for the reader.\n\nSome important results to discuss (also could be sections)\n\nSample data set statistics (Table 1)\n\n3-5 sentences\nInclude a brief description of the sample’s characteristics\nTable 1 should be referenced and appear here!\n\nFinal model\n\n1-2 sentences\nDescribe final model (or models if comparing a few)\n\nWhat variables were included in your final model?\nWhat interactions with your explanatory variable did you include?\n\n\nInterpret the model coefficients in the context of the research question\n\n1 paragraph (maybe 2 in special cases)\n\nWhen in doubt, ask Nicky if your analysis is a special case\n\nInterpreting the explanatory variable’s relationship with food insecurity is the most important thing to report!!\n\nWhen doing this, make sure you account for ALL interactions: If your explanatory variable has multiple interactions and you are trying to interpret one, then what does that mean about the other variables involved in the other interactions? If this is confusing, please make an appointment with me!!\n\n\nResults of model diagnostics and model fit if there is anything worth noting\n\nTables & figures\n\nThe following are required tables or figures\n\nTable 1 summarizing participant characteristics both overall and stratified by your primary independent variable\n\nOther possible tables or figures\n\nTable or figure with regression results\n\nCan be a forest plot\nIf you have A LOT of coefficient estimates, the forest plot may not work well!\n\n\n1-3 figures that you think are helpful in understanding the results, for example\n\nDAG explaining connection between variables (if you did this)\nTable or figure to compare model fit statistics (if you did this)\nTable or figure for unadjusted relationship between outcome and explanatory variables\n\n\n\n\n\n2.5 Discussion\n\nLength: 2-3 paragraphs\nPurpose: Discuss the results and give them context outside of the sample and its analysis\nSome important things to include\n\nInclude a paragraph on the limitations of the results\n\nYou don’t need to hit all the limitations, but think about the big ones (generalizability? independence of samples? large sample size vs. clinical significance? the way we handled variables?)\n\nAfter limitations, discuss the positive parts of the results\n\nWhat can we do with these results? What impact can it have?\n\nAny overarching trends that are worth noting?\n\nShould contain some references\n\n\n\n2.6 Reflection\n\nHow did each Lab help you build towards your project report?\n\nLength: 1-2 sentences per lab\n\nDid you change anything from your labs?\n\nLength: 1-5 sentences per lab\n\n1 sentence to say no changes were made or up to 5 sentences describing the changes you made\n\nFor example, if I gave you feedback about changing a variable to a factor, you do not need to discuss that here.\nHowever, if you made a serious change to how you built your model from Lab 4 (after seeing my feedback) then put that here.\n\n\n\n\n2.7 References\n\nInclude your references here!\nYou introduction should have references, especially when discussing the social science behind the analysis\nYou must reference the WBNS data source!!\n\n\n\n2.8 Optional: Appendix\n\nAdditional figures, tables, and plots that you may want to include"
  },
  {
    "objectID": "project/Lab_02_instructions.html",
    "href": "project/Lab_02_instructions.html",
    "title": "Lab 2 Instructions",
    "section": "",
    "text": "You can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n\nThe purpose of this lab is to explore our data further, set up the unadjusted odds ratio, and create code to later help us present our final model.\n\n\n\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/Lab_02_instructions.html#directions",
    "href": "project/Lab_02_instructions.html#directions",
    "title": "Lab 2 Instructions",
    "section": "",
    "text": "You can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n\nThe purpose of this lab is to explore our data further, set up the unadjusted odds ratio, and create code to later help us present our final model.\n\n\n\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/Lab_02_instructions.html#lab-activities",
    "href": "project/Lab_02_instructions.html#lab-activities",
    "title": "Lab 2 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format (1 sentence). You can change the wording if you’d like, but please make sure it is still clear. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Make sure variables are coded correctly\nUse class() to determine the class of each of the 11 variables you selected from Lab 1 (including the outcome). A tidyverse equivalent to the apply() function that we learned last quarter is map(). Please take a look at the description of the map() function.\nMake sure the class that R recognizes is the class that you expect the variable to be. Categorical variables should be factors amd numeric variables should be numeric. It is very important that your outcome, food insecurity, is a factor with the reference level set to “No.” For example, if I am using age, but the class is character, I will need to convert age to a numeric variable. If I have a categorical covariate that is recognized as a character, I should convert it to a factor with a specific reference level.\n\ndf_name %&gt;% map(class)\n\n\n\n\n\n\n\nTask\n\n\n\n\nUse class() to determine the class of each of the 11 variables you selected from Lab 1 (including the outcome).\nChange the variable type to the appropriate type.\n\n\n\n\n\n2.3 Consider potential confounders and effect modifiers\nFor each of the 10 predictor variables, fill out the below table. Determine whether you think each variable will be a confounder, effect modifier, or nothing in relation to your main variable and food insecurity. This does not need to be extensive reasoning. If you would like to present this information in another way, you may.\n\n\n\n\n\n\nTask\n\n\n\nFill in the below table (or any other way you wish to present the same information).\n\n\n\n\n\n\n\n\n\n\nVariable name\nConfounder, Effect modifier, or nothing?\nReasoning (1-2 sentences)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.4 Create contingency tables for categorical predictors\nFor each categorical covariate, create a contingency table between it and food insecurity. You can create a data frame with only categorical covariattes, then use lapply() to make a table for each column. Take note of any cells that have less than 10 observations. No need to make these tables pretty.\n\n# You need to replace df_cat_only with your data frame that \n#     only has categorical predictors\nlapply(df_cat_only, function(x) table(df_cat_only$FOOD_INSEC, x))\n\n\n\n\n\n\n\nTask\n\n\n\n\nCreate contingency tables for all categorical covariates with food insecurity.\nTake note of any cell counts that are less than 10\n\n\n\n\n\n2.5 Bivariate exploratory data analysis\nUse ggpairs() (introduced in BSTA 512 Lesson 13) to quickly look at the relationship between variables. If you have trouble seeing or interpreting the individual plots, try recreating them in ggplot().\n\n\n\n\n\n\nTask\n\n\n\n\nUse ggpairs() (introduced in BSTA 512 Lesson 13) to quickly look at the relationship between variables.\nList predictors with which there is a clear trend with food insecurity.\n\n\n\n\n\n2.6 Fit simple logistic regression\n\n\n\n\n\n\nTask\n\n\n\n\nUsing glm(), run a logistic regression with food insecurity and your main variable of interest.\nDisplay the unadjusted odds ratio of the regression. You can use logistic.display()\nInterpret the unadjusted odds ratio (with 95% confidence interval). If you’re main variable is multi-level, then you will need to interpret multiple odds ratios.\n\n\n\n\n\n2.7 Plot the predicted probability\nI want us to plot the predicted probability across our main independent variable. If your main variable of interest (from your research question) is continuous, then you can follow the code from Lesson 7 to construct a plot of the predicted probability. If your main variable of interest in categorical, then you can try plotting the predicted probability in the same way as Lesson 7. You may prefer to present the predicted probabilities for each group as a table.\nThis plot will serve as a good foundation if we have any interactions in the model!\n\n\n\n\n\n\nTask\n\n\n\nPlot or make a table of your predicted probabilities."
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#ive-never-explicitly-said-this",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#ive-never-explicitly-said-this",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "I’ve never explicitly said this…",
    "text": "I’ve never explicitly said this…\n\nBecause we are in a public health class, we are often analyzing data with sensitive outcomes\n\n \n\nIf you ever need a moment in class because of our topic, feel free to just step out or leave and privately view the lecture\n\n \n\nIf you need extra time on your assignments because you have an emotional response to lectures/homework/lab, just let me know! Extenuating circumstance!"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#connection-between-tests-in-linear-models-and-glms",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#connection-between-tests-in-linear-models-and-glms",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Connection between tests in linear models and GLMs",
    "text": "Connection between tests in linear models and GLMs\n\nIn linear regression, we used ordinary least squares (OLS) to find the best fit model, so we could use the following tests:\n\nt-test for single coefficients\nF-test for single coefficients or groups of coefficients\n\nThese tests hinge on the Mean Squared Error (MSE) which we minimized in OLS and the LINE assumptions\n\n \n\nIn GLMs, when we use maximum likelihood estimation (MLE), we cannot use t-tests or F-tests\n\nBecause we are now using likelihood to find our estimates (not OLS)\n\nBut we have parallel tests in MLE!!\n\nt-test ⟶ Wald test\nF-test ⟶ Likelihood ratio test"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#revisit-the-likelihood-function",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#revisit-the-likelihood-function",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Revisit the likelihood function",
    "text": "Revisit the likelihood function\n\n\n\nLikelihood function: expresses the probability of the observed data as a function of the unknown parameters\n\nFunction that enumerates the likelihood (similar to probability) that we observe the data across the range of potential values of our coefficients\n\nWe often compare likelihoods to see what estimates are more likely given our data\nPlot to right is a simplistic view of likelihood\n\nI have flattened the likelihood that would be a function of \\(\\beta_0\\) and \\(\\beta_1\\) into a 2D plot (instead of 3D: \\(\\beta_0\\) vs. \\(\\beta_1\\) vs. \\(L(\\beta_0, \\beta_1)\\))\n\nI use \\(L\\) to represent the log-likelihood and \\(l\\) to represent the likelihood"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#introduction-to-three-tests-in-glm",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#introduction-to-three-tests-in-glm",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Introduction to three tests in GLM",
    "text": "Introduction to three tests in GLM\n\nTo introduce these three tests, we will work on a single coefficient\n\nTo be clear: the Likelihood ratio test can be extended to more coefficients\n\nLet’s say we fit a GLM using MLE\n\nWe will continue to use logistic regression as our working example\n\n\n \n\nNow we want to run a hypothesis test for an individual coefficient \\(j\\):\n\n\\(H_0: \\beta_j = 0\\)\n\\(H_1: \\beta_j \\neq 0\\)\n\nThree potential tests that we use with a Likelihood function are:\n\nWald test\nScore test\nLikelihood ratio test (LRT)"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-1",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-1",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#revisit-previous-model-with-late-stage-bc-diagnosis-and-age",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#revisit-previous-model-with-late-stage-bc-diagnosis-and-age",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Revisit previous model with late stage BC diagnosis and age",
    "text": "Revisit previous model with late stage BC diagnosis and age\n\n\n\nSimple logistic regression model: \\[\\text{logit}(\\pi(Age)) = \\beta_0 + \\beta_1 \\cdot Age\\]\n\n\n \nDon’t forget: \\(\\pi(Age) = P(Y=1 | Age)\\)\n\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#wald-test-13",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#wald-test-13",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Wald test (1/3)",
    "text": "Wald test (1/3)\n\nVery similar to a t-test!\n\nBut slightly different because it based in our likelihood function\n\nAssumes test statistic W follows a standard normal distribution under the null hypothesis\nTest statistic: \\[W=\\frac{{\\hat{\\beta}}_j}{se({\\hat{\\beta}}_j)}\\sim N(0,1)\\]\n\nwhere \\(\\widehat{\\beta}_j\\) is a MLE of coefficient \\(j\\)\n\n95% Wald confidence interval: \\[{\\hat{\\beta}}_1\\pm1.96 \\cdot SE_{{\\hat{\\beta}}_j}\\]\nThe Wald test is a routine output in R (summary() of glm() output)\n\nIncludes \\(SE_{{\\hat{\\beta}}_j}\\) and can easily find confidence interval with tidy()"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#wald-test-23",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#wald-test-23",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Wald test (2/3)",
    "text": "Wald test (2/3)"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#wald-test-33",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#wald-test-33",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Wald test (3/3)",
    "text": "Wald test (3/3)"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#wald-test-procedure-with-confidence-intervals",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#wald-test-procedure-with-confidence-intervals",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Wald test procedure with confidence intervals",
    "text": "Wald test procedure with confidence intervals\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the confidence interval and determine if it overlaps with null\n\nOverlap with null (usually 0 for coefficient) = fail to reject null\nNo overlap with null (usually 0 for coefficient) = reject null\n\nWrite a conclusion to the hypothesis test\n\nWhat is the estimate and its confidence interval?\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\nRemember this from last class?\n\n\nWald test for age coefficient\n\n\nInterpret the coefficient for age in our model of late stage breast cancer diagnosis.\n\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063\n  \n  \n  \n\n\n\n\n\nWrite a conclusion to the hypothesis test\n\nFor every one year increase in age, the log-odds of late stage breast cancer diagnosis increases 0.057 (95% CI: 0.051, 0.063)."
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-1",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-1",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nWald test for age coefficient\n\n\nInterpret the coefficient for age in our model of late stage breast cancer diagnosis.\n\n\n\nSet the level of significance \\(\\alpha\\) \\[\\alpha=0.05\\]\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\[\\begin{aligned}\nH_0 &: \\beta_{Age} = 0 \\\\\nH_1 &: \\beta_{Age} \\neq 0 \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-2",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-2",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nWald test for age coefficient\n\n\nInterpret the coefficient for age in our model of late stage breast cancer diagnosis.\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nlibrary(epiDisplay)\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-3",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-3",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nWald test for age coefficient\n\n\nInterpret the coefficient for age in our model of late stage breast cancer diagnosis.\n\n\n\n\n\nWrite a conclusion to the hypothesis test\n\nFor every one year increase in age, the log-odds of late stage breast cancer diagnosis increases 0.057 (95% CI: 0.051, 0.063).\nThere is sufficient evidence that age an breast cancer diagnosis are associated.\n\n\n\nNote\n\n\nI don’t want us to get fixated on this interpretation. This is more to introduce the process, BUT it’s MUCH better to interpret the coefficient in terms of OR (next class)."
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-2",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-2",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#score-test",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#score-test",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Score test",
    "text": "Score test\n\nScore test does not require the computation of MLE for \\(\\beta_1\\), while both likelihood test and Wald test does\n\nOnly need to know \\(\\beta_1\\) under the null\n\nScore test is based on the first and second derivatives of the log-likelihood under the null hypothesis: \\[S=\\frac{\\sum_{i=1}^{n}{x_i(y_i-\\bar{y})}}{\\sqrt{\\bar{y}(1-\\bar{y})\\sum_{i=1}^{n}\\left(x_i-\\bar{x}\\right)^2}} \\sim N(0,1)\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#score-test-1",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#score-test-1",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Score test",
    "text": "Score test"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-13",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-13",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Likelihood ratio test (1/3)",
    "text": "Likelihood ratio test (1/3)\n\nLikelihood ratio test answers the question:\n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate or the model omitting the covariate?\nAka: Which model is more likely given our data: model including the covariate or the model omitting the covariate?\n\n\n \n\nTest a single coefficient by comparing different models\n\nVery similar to the F-test\n\n\n \n\nImportant: LRT can be used conduct hypothesis tests for multiple coefficients\n\nJust like F-test, we can test a single coefficient, continuous/binary covariate, multi-level covariate, or multiple covariates"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-23",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-23",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Likelihood ratio test (2/3)",
    "text": "Likelihood ratio test (2/3)\n\nTo assess the significance of a continuous/binary covariate’s coefficient in the simple logistic regression, we compare the deviance (D) with and without the covariate \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]\n\n \n\nFor a continuous or binary variable, this is equivalent to test: \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\)\nTest statistic for LRT: \\[G=-2ln\\left[\\frac{\\text{likelihood without } x}{\\text{likelihood with } x}\\right]=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-23-1",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-23-1",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Likelihood ratio test (2/3)",
    "text": "Likelihood ratio test (2/3)\n\nTo assess the significance of a continuous/binary covariate’s coefficient in the simple logistic regression, we compare the deviance (D) with and without the covariate \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]\n\n \n\nFor a continuous or binary variable, this is equivalent to test: \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\)\nTest statistic for LRT: \\[G=-2ln\\left[\\frac{\\text{likelihood without } x}{\\text{likelihood with } x}\\right]=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#lrt-what-is-deviance",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#lrt-what-is-deviance",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "LRT: what is Deviance?",
    "text": "LRT: what is Deviance?\n\nDeviance: quantifies the difference in likelihoods between a fitted and saturated model\n\nFitted model:\n\nYour proposed fitted model\n\nSaturated model:\n\nA model that contains as many parameters as there are data points = perfect fit\n\nBasically every individual has their own covariate\n\nPerfect fit = maximum possible likelihood\n\n\n\n \n\nAll fitted models will have likelihood less than saturated model"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#lrt-what-is-deviance-1",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#lrt-what-is-deviance-1",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "LRT: what is Deviance?",
    "text": "LRT: what is Deviance?\n\nThe deviance is mathematically defined as: \\[D=-2[L_{\\text{fitted}}-L_{\\text{saturated}}]\\]\nAn alternative way to write it is: \\[D=-2ln\\left[\\frac{\\text{likelihood of the fitted model}}{\\text{likelihood of the saturated model}}\\right]\\]\nUsing ‘-2’ is to make the deviance follow a chi-square distribution"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#deviance-to-likelihood-ratio-test",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#deviance-to-likelihood-ratio-test",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Deviance to Likelihood Ratio Test",
    "text": "Deviance to Likelihood Ratio Test\n\nIn the LRT, we are NOT comparing the likelihood of saturated model to the fitted model\n\n \n\nWe ARE comparing the Deviance of the model with x and the model without x\n\nWe just use the saturated model to calculate Deviance\nBoth are considered fitted models with their own respective Deviance\n\n\n \n\nSo our LRT is: \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#for-reference",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#for-reference",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "For reference",
    "text": "For reference\n\\[ \\begin{aligned}\nG&=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with }x\\right) \\\\\nG&=-2ln\\left[\\frac{\\text{likelihood of model without } x}{\\text{likelihood of saturated model}}\\right]-\\left(-2ln\\left[\\frac{\\text{likelihood of model with } x}{\\text{likelihood of saturated model}}\\right]\\right) \\\\\nG&=-2ln\\left[\\frac{\\text{likelihood of model without } x}{\\text{likelihood of saturated model}}\\times\\frac{\\text{likelihood of saturated model}}{\\text{likelihood of model with } x}\\right] \\\\\nG&=-2ln\\left[\\frac{\\text{likelihood of model without } x}{\\text{likelihood of model with }}\\right] \\\\\nG&=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-3",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-3",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-23-2",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-23-2",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Likelihood ratio test (2/3)",
    "text": "Likelihood ratio test (2/3)\n\nTo assess the significance of a continuous/binary covariate’s coefficient in the simple logistic regression, we compare the deviance (D) with and without the covariate \\[G=D\\left(\\text{model without } x\\right)-D\\left(\\text{model with } x\\right)\\]\n\n \n\nFor a continuous or binary variable, this is equivalent to test: \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\)\nTest statistic for LRT: \\[G=-2ln\\left[\\frac{\\text{likelihood without } x}{\\text{likelihood with } x}\\right]=2ln\\left[\\frac{l\\left({\\hat{\\beta}}_0,{\\hat{\\beta}}_1\\right)}{l({\\hat{\\beta}}_0)}\\right]\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-33",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test-33",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nUnder the null hypothesis, with adequate sample size, LRT statistic follows a chi-square distribution: \\[G \\sim \\chi^2(df)\\]\n\n\\(df = (\\# \\text{coefficients in larger model}) − (\\# \\text{coefficients in smaller model})\\)\n\n\n \n\nIf we are testing a single coefficient, like age, then \\(df=1\\)"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#likelihood-ratio-test",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#lrt-procedure",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#lrt-procedure",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "LRT procedure",
    "text": "LRT procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-4",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-4",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nLRT\n\n\nDetermine if the model including age is more likely than model without age. Aka: Is age associated with late stage breast cancer diagnosis?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-5",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-5",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nLRT\n\n\nDetermine if the model including age is more likely than model without age. Aka: Is age associated with late stage breast cancer diagnosis?\n\n\n\nSet the level of significance \\(\\alpha\\) \\[\\alpha=0.05\\]\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\[\\begin{aligned}\nH_0 &: \\beta_{Age} = 0 \\\\\nH_1 &: \\beta_{Age} \\neq 0 \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-6",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-6",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nLRT\n\n\nDetermine if the model including age is more likely than model without age. Aka: Is age associated with late stage breast cancer diagnosis?\n\n\n\nCalculate the test statistic and p-value\n\n\nlibrary(lmtest)\nbc_age = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nbc_int = glm(Late_stage_diag ~ 1, data = bc, family = binomial)\nlmtest::lrtest(bc_age, bc_int)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Age_c\nModel 2: Late_stage_diag ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   2 -5754.8                         \n2   1 -5930.5 -1 351.27  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-7",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-bc-diagnosis-and-age-7",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: BC diagnosis and age",
    "text": "Example: BC diagnosis and age\n\n\nLRT\n\n\nDetermine if the model including age is more likely than model without age. Aka: Is age associated with late stage breast cancer diagnosis?\n\n\n\nWrite a conclusion to the hypothesis test\n\nWe reject the null hypothesis that the coefficient corresponding to age is 0 (\\(p-value &lt;&lt; 0.05\\)). There is sufficient evidence that there is an association between age and late stage breast cancer diagnosis."
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#all-three-tests-together",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#all-three-tests-together",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "All three tests together",
    "text": "All three tests together\n\nUCLA FAQ on Tests"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#which-test-to-use",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#which-test-to-use",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Which test to use?",
    "text": "Which test to use?\n\nAll three tests are asymptotically equivalent\n\nAs sample approaches infinity\n\nFor testing significance of single covariate coefficient:\n\nLRT\n\nWald and score are only approximations of LRT\nFor smaller samples, LRT better\n\nWald test is very convenient\n\nAutomatically performed in R\nDoes not need to estimate two models (LRT does)\nGood for constructing confidence intervals of coefficients and odds ratios\n\nScore test\n\nDoes not need to estimate two models (LRT does)\nI don’t really see people use this…"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-4",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-4",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-5",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-5",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-6",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-6",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Poll Everywhere Question 6",
    "text": "Poll Everywhere Question 6"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-7",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-7",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Poll Everywhere Question 7",
    "text": "Poll Everywhere Question 7"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-8",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#poll-everywhere-question-8",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Poll Everywhere Question 8",
    "text": "Poll Everywhere Question 8"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#tests-and-what-theyre-used-for",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#tests-and-what-theyre-used-for",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Tests and what they’re used for",
    "text": "Tests and what they’re used for\n\n\n\n\n\n\n\n\n\n\nWald test\nScore test\nLRT\n\n\n\n\nUsed to test significance of single coefficient\n\n\n\n\n\nCan be used to report confidence interval for a single coefficient\n\n\n\n\n\nConfidence interval reported by R for a single coefficient (and most commonly used)\n\n\n\n\n\nUse to test significance/contribution to outcome prediction of multi-level categorical covariate\n\n\n\n\n\nUsed for comparing two models with different (but nested) covariates"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#so-how-would-the-wald-test-and-lrt-show-up-in-research",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#so-how-would-the-wald-test-and-lrt-show-up-in-research",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "So how would the Wald test and LRT show up in research?",
    "text": "So how would the Wald test and LRT show up in research?\n\nWald test\n\nOften used when reporting estimates\nGenerally presented using a forest plot or table of ORs or RRs\n\nThen we highlight the specific variable of interest in text\nWill include the OR/RR estimate (not the coefficient like we saw today) with the 95% CI and proper interpretation of result\n\n\n\n \n\nLRT\n\nOften when performing model selection and comparing two models\n\nReporting model selection Cross Validated post\n\nOften does not show up explicitly in our reports, but is essential to get to our final model!!\n\n\n\n\nLesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#last-time-to-this-time",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#last-time-to-this-time",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Last time to this time",
    "text": "Last time to this time\n\nUsed the Wald test and Wald 95% confidence interval to interpret coefficients in a fitted model\nThis time: Interpret using odds ratio"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#we-typically-interpret-our-results-using-odds-ratios",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#we-typically-interpret-our-results-using-odds-ratios",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "We typically interpret our results using odds ratios",
    "text": "We typically interpret our results using odds ratios\nFor our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\n\nHow do we go from interpretations of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) using log odds to odds ratios?\nWe will need to take the exponential of our model:\n\n\\(\\text{exp}(\\widehat{\\beta}_0)\\): expected odds that \\(Y=1\\) when X is 0.\n\\(\\text{exp}(\\widehat{\\beta}_1)\\): expected odds ratio that \\(Y=1\\) for every 1 unit increase in X"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#we-typically-interpret-our-results-using-odds-ratios-1",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#we-typically-interpret-our-results-using-odds-ratios-1",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "We typically interpret our results using odds ratios",
    "text": "We typically interpret our results using odds ratios\nFor our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\n\nHow do we go from interpretations of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) using log odds to odds ratios?\nWe will need to take the exponential of our model:\n\n\\(\\text{exp}(\\widehat{\\beta}_0)\\): expected odds that \\(Y=1\\) when X is 0.\n\\(\\text{exp}(\\widehat{\\beta}_1)\\): expected odds ratio that \\(Y=1\\) for every 1 unit increase in X"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example",
    "text": "Example\n\nHow do we do this in R?\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T, exponentiate=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.023\n−42.637\n0.000\n0.355\n0.389\n    Age_c\n1.059\n0.003\n17.780\n0.000\n1.052\n1.065\n  \n  \n  \n\n\n\nlogistic.display(bc_reg, decimal = 3)\n\n\nLogistic regression predicting Late_stage_diag : 1 vs 0 \n \n                   OR(95%CI)            P(Wald's test) P(LR-test)\nAge_c (cont. var.) 1.059 (1.052,1.065)  &lt; 0.001        &lt; 0.001   \n                                                                 \nLog-likelihood = -5754.84419\nNo. of observations = 10000\nAIC value = 11513.68837"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#transformations-of-continuous-variable-to-make-more-interpretable",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#transformations-of-continuous-variable-to-make-more-interpretable",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Transformations of continuous variable to make more interpretable",
    "text": "Transformations of continuous variable to make more interpretable\n\nSometimes a change in “1” unit may not be considered clinically interesting\n\nFor example, a 1 year increase in age or a 1 mm Hg increase in systolic blood pressure may be too small for a meaningful change in log odds\nInstead, we may be interested to find out the log odds change for a increase of 10 years in age or 10 mm Hg in systolic blood pressure\nOn the other hand, if the range of x is small (say 0-1), than a change in 1 unit of 𝑥 is too large to be meaningful\n\nWe should be able to compute and interpret coefficients for a continuous independent covariate 𝑥 for an arbitrary change of “c” units in 𝑥"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#how-do-we-get-the-odds-and-odds-ratio",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#how-do-we-get-the-odds-and-odds-ratio",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "How do we get the odds and odds ratio?",
    "text": "How do we get the odds and odds ratio?\n\n\nFor \\(\\text{exp}(\\widehat{\\beta}_0)\\)\n\nWhen \\(X=0\\), we have \\[\\text{logit}(\\widehat{\\pi}(X=0)) = \\widehat{\\beta}_0\\]\nThus, \\[\\begin{aligned} \\widehat{\\beta}_0 & = \\text{logit}(\\widehat{\\pi}(X)) \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\big[\\text{logit}(\\widehat{\\pi}(X))\\big] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\Bigg[\\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)}\\Bigg)\\Bigg] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)} \\\\\n\\end{aligned}\\]\n\n\n\n\nFor \\(\\text{exp}(\\widehat{\\beta}_1)\\)\n\nWe compare \\(X=x\\) and \\(X=x+1\\), and we have \\(\\text{logit}(\\widehat{\\pi}(X = x)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x\\) and \\(\\text{logit}(\\widehat{\\pi}(X = x+1)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot (x+1)\\)\nThus, \\[\\begin{aligned} \\widehat{\\beta}_0 & = \\text{logit}(\\widehat{\\pi}(X)) \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\big[\\text{logit}(\\widehat{\\pi}(X))\\big] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\Bigg[\\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)}\\Bigg)\\Bigg] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#how-do-we-get-the-odds-for-the-intercept",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#how-do-we-get-the-odds-for-the-intercept",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "How do we get the odds for the intercept?",
    "text": "How do we get the odds for the intercept?\nFor \\(\\text{exp}(\\widehat{\\beta}_0)\\)\n\nWhen \\(X=0\\), we have \\[\\text{logit}(\\widehat{\\pi}(X=0)) = \\widehat{\\beta}_0\\]\nThus, \\[\\begin{aligned} \\widehat{\\beta}_0 & = \\text{logit}(\\widehat{\\pi}(X)) \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\big[\\text{logit}(\\widehat{\\pi}(X))\\big] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\Bigg[\\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)}\\Bigg)\\Bigg] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#how-do-we-get-the-odds-ratio-for-xs-coefficient",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#how-do-we-get-the-odds-ratio-for-xs-coefficient",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "How do we get the odds ratio for X’s coefficient?",
    "text": "How do we get the odds ratio for X’s coefficient?\n\n\nFor \\(\\text{exp}(\\widehat{\\beta}_1)\\)\n\nWe compare \\(X=x\\) and \\(X=x+1\\),\nSo we have \\[\\text{logit}(\\widehat{\\pi}(X = x)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x\\] and \\[\\text{logit}(\\widehat{\\pi}(X = x+1)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot (x+1)\\]\nAnd… \\[\\begin{aligned} & \\text{logit}(\\widehat{\\pi}(X = x+1)) - \\text{logit}(\\widehat{\\pi}(X = x)) \\\\ & =  \n\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot (x+1) - \\big[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x \\big] \\\\ &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x + \\widehat{\\beta}_1 - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 \\cdot x \\\\ & =\n\\widehat{\\beta}_1 \\end{aligned}\\]\n\n\n\nThus, \\[\\begin{aligned} \\widehat{\\beta}_1 & =  \\text{logit}(\\widehat{\\pi}(X = x+1)) - \\text{logit}(\\widehat{\\pi}(X = x)) \\\\\n\\widehat{\\beta}_1 & =  \\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X=x+1)}{1-\\widehat{\\pi}(X=x+1)}\\Bigg) - \\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X=x)}{1-\\widehat{\\pi}(X=x)}\\Bigg) \\\\\n\\widehat{\\beta}_1 & =  \\text{log}\\left(\\frac{\\dfrac{\\widehat{\\pi}(X=x+1)}{1-\\widehat{\\pi}(X=x+1)}} {\\dfrac{\\widehat{\\pi}(X=x)}{1-\\widehat{\\pi}(X=x)}}\\right) \\\\\n\\text{exp}\\big[\\widehat{\\beta}_1\\big] & =  \\text{exp}\\left[\\text{log}\\left(\\frac{\\text{odds}_{X=x+1}} {\\text{odds}_{X=x}}\\right) \\right] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_1\\big] & =  \\frac{\\text{odds}_{X=x+1}} {\\text{odds}_{X=x}} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-1",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-1",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example",
    "text": "Example\nFor our fitted simple logistic regression model with age as a predictor \\[\\text{logit}(\\widehat{\\pi}(Age)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot Age\\]\n\n\\(\\widehat{\\beta}_0\\): estimated log-odds when age is 61.71 years\n\\(\\widehat{\\beta}_1\\): estimated increase in log-odds for every 1 year increase in age\n\n\n\nLesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-2",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-2",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example",
    "text": "Example\nFor our fitted simple logistic regression model with age as a predictor \\[\\text{logit}(\\widehat{\\pi}(Age)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot Age\\]\n\n\\(\\widehat{\\beta}_0\\): estimated log-odds when age is 61.71 years\n\\(\\widehat{\\beta}_1\\): estimated increase in log-odds for every 1 year increase in age\n\n\n\nLesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-from-last-class",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#example-from-last-class",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: From last class",
    "text": "Example: From last class\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063\n  \n  \n  \n\n\n\n\n\nFor our fitted simple logistic regression model with age as a predictor \\[\\text{logit}(\\widehat{\\pi}(Age^c)) = −0.989  + 0.057     \\cdot Age^c\\]\n\\(\\widehat{\\beta}_0\\): The estimated log-odds is -0.989 when age is 61.71 years (95% CI: -1.035, -0.944)\n\\(\\widehat{\\beta}_1\\): The estimated increase in log-odds is 0.057 for every 1 year increase in age (95% CI: 0.051, 0.063)."
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#so-far-weve-looked-at-the-association-using-the-log-odds-scale",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html#so-far-weve-looked-at-the-association-using-the-log-odds-scale",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "So far we’ve looked at the association using the log-odds scale",
    "text": "So far we’ve looked at the association using the log-odds scale\n\n\nFor a population simple logistic regression model with a continuous predictor \\[\\text{logit}(\\pi(X)) = \\beta_0 + \\beta_1 \\cdot X\\]\n\n\\(\\beta_0\\): log-odds when X is 0\n\\(\\beta_1\\): increase in log-odds for every 1 unit increase in X\n\n\n\n\nFor our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\n\n\\(\\widehat{\\beta}_0\\): estimated log-odds of \\(Y=1\\) when X is 0.\n\\(\\widehat{\\beta}_1\\): estimated increase in log-odds of \\(Y=1\\) for every 1 unit increase in X\nCan use expected instead of estimated"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#predicted-probability",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#predicted-probability",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\n\nWe may be interested in predicting probability of having a late stage breast cancer diagnosis for a specific age.\nThe predicted probability is the estimated probability of having the event for given values of covariate(s)\nIn simple logistic regression, the fitted model is:\\[\\text{logit}(\\widehat{\\pi}(X)) = \\hat{\\beta}_0 +{\\hat{\\beta}}_1X \\]\nWe can convert it to the predicted probability: \\[\\hat{\\pi}\\left(X\\right)=\\frac{\\exp({\\hat{\\beta}}_0+{\\hat{\\beta}}_1X)}{1+\\exp({\\hat{\\beta}}_0+{\\hat{\\beta}}_1X)}\\]\n\nThis is an inverse logit calculation\n\nWe can calculate this using the the predict() function like in BSTA 512\n\nAnother option: taking inverse logit of fitted values from augment() function"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#example",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#example",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Example:",
    "text": "Example:\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone 50 years old, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\n\nnewdata = data.frame(Age_c = 60 - mean_age)\n\npred = predict(bc_reg, newdata = newdata, se.fit = T, type = \"response\")\n\nLL_CI = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\nc(Pred = pred$fit, LL_CI, UL_CI) %&gt;% round(digits=3)\n\nPred.1      1      1 \n 0.252  0.243  0.261"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#visualization",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#visualization",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Visualization",
    "text": "Visualization\n\n\n\n\n\\[\\text{logit}(\\widehat{\\pi}(Age)) = -0.989 + 0.057 \\cdot Age\\]\n\n\\[\\widehat{\\pi}(Age) = \\dfrac{ \\exp \\left[-0.989 + 0.057 \\cdot Age \\right]}{1+\\exp \\left[-0.989 + 0.057 \\cdot Age \\right]}\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Example: Late stage breast cancer diagnosis",
    "text": "Example: Late stage breast cancer diagnosis\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone 50 years old, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\nNeeded steps:\n\nCalculate probability prediction\nCheck if we can use Normal approximation\nCalculate confidence interval\n\nUsing logit scale then converting\nUsing Normal approximation\n\nInterpret results"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-1",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-1",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Example: Late stage breast cancer diagnosis",
    "text": "Example: Late stage breast cancer diagnosis\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone 50 years old, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCalculate probability prediction\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nnewdata = data.frame(Age_c = 60 - mean_age)\npred1 = predict(bc_reg, newdata = newdata, se.fit = T, type = \"response\")\npred1\n\n$fit\n        1 \n0.2522616 \n\n$se.fit\n          1 \n0.004709743 \n\n$residual.scale\n[1] 1"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-2",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-2",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Example: Late stage breast cancer diagnosis",
    "text": "Example: Late stage breast cancer diagnosis\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone 50 years old, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCheck if we can use Normal approximation\n\nWe can use the Normal approximation if: \\(\\widehat{p}n = \\widehat{\\pi}(X)\\cdot n &gt; 10\\) and \\((1-\\widehat{p})n = (1-\\widehat{\\pi}(X))\\cdot n &gt; 10\\).\n\nn = nobs(bc_reg)\np = pred1$fit\nn*p\n\n       1 \n2522.616 \n\nn*(1-p)\n\n       1 \n7477.384 \n\n\nWe can use the Normal approximation!"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#predicted-probability-1",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#predicted-probability-1",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\n\nPredicted probability is NOT our predicted outcome\n\nWe cannot interpret it as the predicted \\(Y\\) for individuals with certain covariate values\nExample: our predicted probability does not tell us that one individual is or is not diagnosed with late stage breast cancer\n\nThe predicted probability is the estimate of the mean (i.e., proportion) of individuals at a certain age who are diagnosed with late stage breast cancer"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#predicted-outcome",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#predicted-outcome",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Predicted outcome",
    "text": "Predicted outcome\n\nTypically, the predicted probability is the most important thing to use in a clinical setting\n\n \n\nIf you ever need to predict the outcome itself (from logistic regression with binary outcome):\n\nRemember that the predicted probability can be used in a Bernoulli (or Binomial with \\(n=1\\)) distribution to find the predicted outcome\n\nIf outcome is something like counts, then we would use a Poisson distribution\n\n \n\nBy putting it back through a Bernoulli/binomial distribution, we are re-introducing the random component of our observed outcome\n\n\nset.seed(8392)\nrbinom(n=1, size=1, prob = pred$fit)\n\n[1] 0\n\nrbinom(n=10, size=1, prob = pred$fit)\n\n [1] 0 0 0 0 1 0 0 0 1 0"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-also-make-a-plot-of-all-the-predictions",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-also-make-a-plot-of-all-the-predictions",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can also make a plot of all the predictions",
    "text": "We can also make a plot of all the predictions"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#confidence-interval-of-predicted-probability",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#confidence-interval-of-predicted-probability",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Confidence Interval of Predicted Probability",
    "text": "Confidence Interval of Predicted Probability\n\nNot as easy to construct\nI have searched around for a function that does this for us, but I cannot find one\nSo we have to construct the confidence interval “by hand”\n\n \nThere are a two ways to do this:\n\nConstruct the 95% confidence interval in the logit scale, then convert to probability scale\nUse Normal approximation (if appropriate) to construct confidence interval in probability scale"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#confidence-interval-in-logit-scale",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#confidence-interval-in-logit-scale",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "95% confidence interval in logit scale",
    "text": "95% confidence interval in logit scale\n\nWe start by predicting the log-odds and forming"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#visualization-of-ors",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#visualization-of-ors",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Visualization of ORs?",
    "text": "Visualization of ORs?\n\n\nLesson 7: Prediction and Visualization in Simple Logistic Regression"
  },
  {
    "objectID": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html",
    "href": "lectures/07_Interpretations_SLR/07_Interpretations_SLR.html",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "",
    "text": "Used the Wald test and Wald 95% confidence interval to interpret coefficients in a fitted model\nThis time: Interpret using odds ratio"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-1-95-confidence-interval-in-logit-scale",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-1-95-confidence-interval-in-logit-scale",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Option 1: 95% confidence interval in logit scale",
    "text": "Option 1: 95% confidence interval in logit scale\n\nRecall our our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\nWe can first find the predicted \\(\\text{logit}(\\widehat{\\pi}(X))\\) and then find the 95% confidence interval around it: \\[\\text{logit}(\\widehat{\\pi}(X)) \\pm 1.96 \\cdot SE_{\\text{logit}(\\widehat{\\pi}(X))}\\]\nWe’ll call this 95% CI: \\[\\left(\\text{logit}(\\widehat{\\pi}(X)) - 1.96 \\cdot SE_{\\text{logit}(\\widehat{\\pi}(X))}, \\ \\text{logit}(\\widehat{\\pi}(X)) + 1.96 \\cdot SE_{\\text{logit}(\\widehat{\\pi}(X))} \\right)\\] \\[\\left(\\text{logit}_{L}, \\ \\text{logit}_{U} \\right)\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-2-using-normal-approximation",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-2-using-normal-approximation",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Option 2: Using Normal approximation",
    "text": "Option 2: Using Normal approximation\n\nIf we meet the Normal approximation criteria, we can construct our confidence interval directly in the probability scale\n\nWe can use the Normal approximation if:\n\n\\(\\widehat{p}n = \\widehat{\\pi}(X)\\cdot n &gt; 10\\) and\n\\((1-\\widehat{p})n = (1-\\widehat{\\pi}(X))\\cdot n &gt; 10\\)\n\n\n\n \n\nWe can first find the predicted \\(\\widehat{\\pi}(X)\\) and then find the 95% confidence interval around it: \\[\\widehat{\\pi}(X) \\pm 1.96 \\cdot SE_{\\widehat{\\pi}(X)}\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-3",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-3",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Example: Late stage breast cancer diagnosis",
    "text": "Example: Late stage breast cancer diagnosis\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone 50 years old, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n3a. Calculate confidence interval (Option 1: logit scale, we could skip previous step)\n\npred1 = predict(bc_reg, newdata = newdata, se.fit = T, type = \"link\")\nLL_CI1 = pred1$fit - qnorm(1-0.05/2) * pred1$se.fit\nUL_CI1 = pred1$fit + qnorm(1-0.05/2) * pred1$se.fit\npred_link = c(Pred = pred1$fit, LL_CI1, UL_CI1)\n\n(exp(pred_link)/(1+exp(pred_link))) %&gt;% round(., digits=3)\n\nPred.1      1      1 \n 0.252  0.243  0.262 \n\ninv.logit(pred_link) %&gt;% round(., digits=3)\n\nPred.1      1      1 \n 0.252  0.243  0.262"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-4",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-4",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Example: Late stage breast cancer diagnosis",
    "text": "Example: Late stage breast cancer diagnosis\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone 50 years old, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n3b. Calculate confidence interval (Option 2: with Normal approximation)\n\npred = predict(bc_reg, newdata = newdata, se.fit = T, type = \"response\")\n\nLL_CI = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\nc(Pred = pred$fit, LL_CI, UL_CI) %&gt;% round(digits=3)\n\nPred.1      1      1 \n 0.252  0.243  0.261"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-5",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-late-stage-breast-cancer-diagnosis-5",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Example: Late stage breast cancer diagnosis",
    "text": "Example: Late stage breast cancer diagnosis\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone 50 years old, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nInterpret results\n\nFor someone who is 60 years old, the predicted probability of late stage breast cancer diagnosis is 0.252 (95% CI: 0.243, 0.261)."
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-also-make-a-plot-of-all-the-predicted-probabilities",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-also-make-a-plot-of-all-the-predicted-probabilities",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can also make a plot of all the predicted probabilities",
    "text": "We can also make a plot of all the predicted probabilities\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nbc_aug = augment(bc_reg)\n\n\nlibrary(boot)\nprob_stage = ggplot(data = bc_aug, aes(x=Age_c, y = inv.logit(.fitted))) + \n  geom_point(size = 4, color = \"#70AD47\", shape = 1) +\n  labs(x = \"Age centered (yrs)\", \n       y = \"Probability of \\n Late stage BC diagnosis\")  + theme_classic() +\n    theme(axis.title = element_text(\n        size = 30), \n        axis.text = element_text(\n        size = 25), \n        title = element_text(\n        size = 30)) +\n  ylim(0, 1)"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-1-95-confidence-interval-in-logit-scale-12",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-1-95-confidence-interval-in-logit-scale-12",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Option 1: 95% confidence interval in logit scale (1/2)",
    "text": "Option 1: 95% confidence interval in logit scale (1/2)\n\nRecall our our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\nWe can first find the predicted \\(\\text{logit}(\\widehat{\\pi}(X))\\) and then find the 95% confidence interval around it: \\[\\text{logit}(\\widehat{\\pi}(X)) \\pm 1.96 \\cdot SE_{\\text{logit}(\\widehat{\\pi}(X))}\\]\nWe’ll call this 95% CI: \\[\\left(\\text{logit}(\\widehat{\\pi}(X)) - 1.96 \\cdot SE_{\\text{logit}(\\widehat{\\pi}(X))}, \\ \\text{logit}(\\widehat{\\pi}(X)) + 1.96 \\cdot SE_{\\text{logit}(\\widehat{\\pi}(X))} \\right)\\] \\[\\left(\\text{logit}_{L}, \\ \\text{logit}_{U} \\right)\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-1-95-confidence-interval-in-logit-scale-12-1",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-1-95-confidence-interval-in-logit-scale-12-1",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Option 1: 95% confidence interval in logit scale (1/2)",
    "text": "Option 1: 95% confidence interval in logit scale (1/2)\n\nThen we need to convert to the probability scale\nTo convert from \\(\\text{logit}(\\widehat{\\pi}(X))\\) to \\(\\widehat{\\pi}(X)\\), we take the inverse logit\nThus, 95% CI in the probability scale is: \\[\\left(\\dfrac{\\exp\\left[\\text{logit}_{L}\\right]}{1 + \\exp\\left[\\text{logit}_{L}\\right]}, \\ \\dfrac{\\exp\\left[\\text{logit}_{U}\\right]}{1 + \\exp\\left[\\text{logit}_{U}\\right]} \\right)\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#reference-inverse-logit",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#reference-inverse-logit",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Reference: Inverse logit",
    "text": "Reference: Inverse logit\n\nIf we have \\(\\text{logit}(a) = b\\), then \\[\\begin{aligned}\n\\text{logit}(a) & = b \\\\\n\\text{log}\\left(\\dfrac{a}{1-a}\\right) & = b \\\\\n\\exp \\left[ \\text{log}\\left(\\dfrac{a}{1-a}\\right) \\right] & = \\exp[b] \\\\\n\\dfrac{a}{1-a} & = \\exp[b] \\\\\na & = \\exp[b]\\cdot(1-a) \\\\\na & = \\exp[b] - a\\cdot \\exp[b] \\\\\na +  a\\cdot \\exp[b]& = \\exp[b] \\\\\na\\cdot ( 1 + \\exp[b] )& = \\exp[b] \\\\\na& = \\dfrac{\\exp[b]}{1 + \\exp[b]} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-1-95-confidence-interval-in-logit-scale-22",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#option-1-95-confidence-interval-in-logit-scale-22",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Option 1: 95% confidence interval in logit scale (2/2)",
    "text": "Option 1: 95% confidence interval in logit scale (2/2)\n\nThen we need to convert to the probability scale\nTo convert from \\(\\text{logit}(\\widehat{\\pi}(X))\\) to \\(\\widehat{\\pi}(X)\\), we take the inverse logit\nThus, 95% CI in the probability scale is: \\[\\left(\\dfrac{\\exp\\left[\\text{logit}_{L}\\right]}{1 + \\exp\\left[\\text{logit}_{L}\\right]}, \\ \\dfrac{\\exp\\left[\\text{logit}_{U}\\right]}{1 + \\exp\\left[\\text{logit}_{U}\\right]} \\right)\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-also-make-a-plot-of-all-the-predicted-probabilities-12",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-also-make-a-plot-of-all-the-predicted-probabilities-12",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can also make a plot of all the predicted probabilities (1/2)",
    "text": "We can also make a plot of all the predicted probabilities (1/2)\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nbc_aug = augment(bc_reg)\n\n\nThen we plot the fitted values from the fitted model\n\n\nlibrary(boot) # for inv.logit()\nprob_stage = ggplot(data = bc_aug, aes(x=Age_c, y = inv.logit(.fitted))) + \n  # geom_point(size = 4, color = \"#70AD47\", shape = 1) +\n  geom_smooth(size = 4, color = \"#70AD47\") +\n  labs(x = \"Age centered (yrs)\", \n       y = \"Estimated probability of \\n Late stage BC diagnosis\")  + \n  theme_classic() +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30)) +\n  ylim(0, 1)"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-also-make-a-plot-of-all-the-predicted-probabilities-22",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-also-make-a-plot-of-all-the-predicted-probabilities-22",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can also make a plot of all the predicted probabilities (2/2)",
    "text": "We can also make a plot of all the predicted probabilities (2/2)\n\nIf we are interested in seeing all the predicted probabilities across the sample’s age range\nNote that the probabilities do not need to fill the full range of 0 to 1."
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can add the confidence intervals",
    "text": "We can add the confidence intervals\n\nnewdata2 = data.frame(Age_c = seq(min(bc$Age_c), max(bc$Age_c), by = 0.1))\npred2 = predict(bc_reg, newdata = newdata2, se.fit = T, type = \"link\")\nLL_CI1 = pred2$fit - qnorm(1-0.05/2) * pred2$se.fit\nUL_CI1 = pred2$fit + qnorm(1-0.05/2) * pred2$se.fit\n\nwith_CI = data.frame(Age_c = newdata2$Age_c, \n                     pred = inv.logit(pred2$fit), \n                     LL = inv.logit(LL_CI1), \n                     UL = inv.logit(UL_CI1))"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#section",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#section",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "",
    "text": "library(boot)\nprob_stage_CI = ggplot(data = with_CI, aes(x = Age_c)) +\n  geom_ribbon(aes(ymin = LL, ymax = UL), fill = \"grey\") +\n  geom_smooth(aes(x=Age_c, y = pred), size = 1, color = \"#70AD47\") +\n  labs(x = \"Age centered (yrs)\", \n       y = \"Probability of \\n Late stage BC diagnosis\")  + theme_classic() +\n    theme(axis.title = element_text(\n        size = 30), \n        axis.text = element_text(\n        size = 25), \n        title = element_text(\n        size = 30)) +\n  ylim(0, 0.6)"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#section-1",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#section-1",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "",
    "text": "prob_stage_CI"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-1",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-1",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can add the confidence intervals",
    "text": "We can add the confidence intervals\n\nlibrary(boot)\nprob_stage_CI = ggplot(data = with_CI, aes(x = Age_c)) +\n  geom_ribbon(aes(ymin = LL, ymax = UL), fill = \"grey\") +\n  geom_smooth(aes(x=Age_c, y = pred), size = 1, color = \"#70AD47\") +\n  labs(x = \"Age centered (yrs)\", \n       y = \"Probability of \\n Late stage BC diagnosis\")  + theme_classic() +\n    theme(axis.title = element_text(\n        size = 30), \n        axis.text = element_text(\n        size = 25), \n        title = element_text(\n        size = 30)) +\n  ylim(0, 0.6)"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-2",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-2",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can add the confidence intervals",
    "text": "We can add the confidence intervals"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-13",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-13",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can add the confidence intervals (1/3)",
    "text": "We can add the confidence intervals (1/3)\n\nnewdata2 = data.frame(Age_c = seq(min(bc$Age_c), max(bc$Age_c), by = 0.1))\npred2 = predict(bc_reg, newdata = newdata2, se.fit = T, type = \"link\")\nLL_CI1 = pred2$fit - qnorm(1-0.05/2) * pred2$se.fit\nUL_CI1 = pred2$fit + qnorm(1-0.05/2) * pred2$se.fit\n\nwith_CI = data.frame(Age_c = newdata2$Age_c, \n                     pred = inv.logit(pred2$fit), \n                     LL = inv.logit(LL_CI1), \n                     UL = inv.logit(UL_CI1))"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-23",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-23",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can add the confidence intervals (2/3)",
    "text": "We can add the confidence intervals (2/3)\n\nprob_stage_CI = ggplot(data = with_CI, aes(x = Age_c)) +\n  geom_ribbon(aes(ymin = LL, ymax = UL), fill = \"grey\") +\n  geom_smooth(aes(x=Age_c, y = pred), size = 1, color = \"#70AD47\") +\n  labs(x = \"Age centered (yrs)\", \n       y = \"Estimated probability of \\n Late stage BC diagnosis\")  + \n  theme_classic() +\n  theme(axis.title = element_text(size = 30), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 30)) +\n  ylim(0, 0.6)"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-33",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#we-can-add-the-confidence-intervals-33",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "We can add the confidence intervals (3/3)",
    "text": "We can add the confidence intervals (3/3)"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#poll-everywhere-question",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#poll-everywhere-question",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#visualization-of-odds-ratios",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#visualization-of-odds-ratios",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Visualization of odds ratios?",
    "text": "Visualization of odds ratios?\n\nWe will discuss this more on Wednesday when we look at interpretations of ORs"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-breast-cancer-diagnosis",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#example-breast-cancer-diagnosis",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Example: Breast cancer diagnosis",
    "text": "Example: Breast cancer diagnosis\n\nRecall that we fitted a simple logistic regression for late stage breast cancer diagnosis using the predictor, age:\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 38) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063\n  \n  \n  \n\n\n\n\n \n\nFitted logistic regression model: \\[\\text{logit}(\\widehat{\\pi}(Age)) = -0.989 + 0.057 \\cdot Age\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#recall-our-example-late-stage-breast-cancer-diagnosis",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#recall-our-example-late-stage-breast-cancer-diagnosis",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Recall our example: Late stage breast cancer diagnosis",
    "text": "Recall our example: Late stage breast cancer diagnosis\n\nRecall that we fitted a simple logistic regression for late stage breast cancer diagnosis using the predictor, age:\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 38) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063\n  \n  \n  \n\n\n\n\n \n\nFitted logistic regression model: \\[\\text{logit}(\\widehat{\\pi}(Age)) = -0.989 + 0.057 \\cdot Age\\]"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#predictedestimated-probability",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#predictedestimated-probability",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Predicted/Estimated probability",
    "text": "Predicted/Estimated probability\n\nPredicted probability is NOT our predicted outcome\n\nWe cannot interpret it as the predicted \\(Y\\) for individuals with certain covariate values\nExample: our predicted probability does not tell us that one individual will or will not be diagnosed with late stage breast cancer\n\n\n \n\nThe predicted probability is the estimate of the mean (i.e., proportion) of individuals at a certain age who are diagnosed with late stage breast cancer\nWe can use the predicted/estimated probability to predict the outcome"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#recall-our-example-late-stage-breast-cancer-diagnosis-1",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#recall-our-example-late-stage-breast-cancer-diagnosis-1",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Recall our example: Late stage breast cancer diagnosis",
    "text": "Recall our example: Late stage breast cancer diagnosis\n\nFitted logistic regression model: \\[\\text{logit}(\\widehat{\\pi}(Age)) = -0.989 + 0.057 \\cdot Age\\]\n\n     \n\nNow we want to caclulate the predicted/estimated probability from the above fitted model\nWe will need to calculate the predicted probability and its confidence interval\n\nThen we will visualize the fitted probability"
  },
  {
    "objectID": "lectures/07_Pred_Viz/07_Pred_Viz.html#visualization-of-observed-outcome-and-fitted-model",
    "href": "lectures/07_Pred_Viz/07_Pred_Viz.html#visualization-of-observed-outcome-and-fitted-model",
    "title": "Lesson 7: Prediction and Visualization in Simple Logistic Regression",
    "section": "Visualization of observed outcome and fitted model",
    "text": "Visualization of observed outcome and fitted model\n\n\n\n\n\\[\\text{logit}(\\widehat{\\pi}(Age)) = -0.989 + 0.057 \\cdot Age\\]\n\n\\[\\widehat{\\pi}(Age) = \\dfrac{ \\exp \\left[-0.989 + 0.057 \\cdot Age \\right]}{1+\\exp \\left[-0.989 + 0.057 \\cdot Age \\right]}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html",
    "title": "Lesson 7: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "",
    "text": "Used the Wald test and Wald 95% confidence interval to interpret coefficients in a fitted model\nThis time: Interpret using odds ratio"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#last-time-to-this-time",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#last-time-to-this-time",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Last time to this time",
    "text": "Last time to this time\n\nUsed the Wald test and Wald 95% confidence interval to interpret coefficients in a fitted model\nThis time: Interpret using odds ratio"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#so-far-weve-looked-at-the-association-using-the-log-odds-scale",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#so-far-weve-looked-at-the-association-using-the-log-odds-scale",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "So far we’ve looked at the association using the log-odds scale",
    "text": "So far we’ve looked at the association using the log-odds scale\n\n\nFor a population simple logistic regression model with a continuous predictor \\[\\text{logit}(\\pi(X)) = \\beta_0 + \\beta_1 \\cdot X\\]\n\n\\(\\beta_0\\): log-odds when X is 0\n\\(\\beta_1\\): increase in log-odds for every 1 unit increase in X\n\n\n\n\nFor our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\n\n\\(\\widehat{\\beta}_0\\): estimated log-odds of \\(Y=1\\) when X is 0.\n\\(\\widehat{\\beta}_1\\): estimated increase in log-odds of \\(Y=1\\) for every 1 unit increase in X\nCan use expected instead of estimated"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-from-last-class",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-from-last-class",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: From last class",
    "text": "Example: From last class\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063\n  \n  \n  \n\n\n\n\n\nFor our fitted simple logistic regression model with age as a predictor \\[\\text{logit}(\\widehat{\\pi}(Age^c)) = −0.989  + 0.057     \\cdot Age^c\\]\n\\(\\widehat{\\beta}_0\\): The estimated log-odds is -0.989 when age is 61.71 years (95% CI: -1.035, -0.944)\n\\(\\widehat{\\beta}_1\\): The estimated increase in log-odds is 0.057 for every 1 year increase in age (95% CI: 0.051, 0.063)."
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#we-typically-interpret-our-results-using-odds-ratios",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#we-typically-interpret-our-results-using-odds-ratios",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "We typically interpret our results using odds ratios",
    "text": "We typically interpret our results using odds ratios\nFor our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\n\nHow do we go from interpretations of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) using log odds to odds ratios?\nWe will need to take the exponential of our model:\n\n\\(\\text{exp}(\\widehat{\\beta}_0)\\): expected odds that \\(Y=1\\) when X is 0.\n\\(\\text{exp}(\\widehat{\\beta}_1)\\): expected odds ratio that \\(Y=1\\) for every 1 unit increase in X\n\nImportant distinction:\n\nWe take the inverse logit to find our predicted probability\nWe take the exponential to interpret the odds/odds ratios"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-do-we-get-the-odds-for-the-intercept",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-do-we-get-the-odds-for-the-intercept",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "How do we get the odds for the intercept?",
    "text": "How do we get the odds for the intercept?\nFor \\(\\text{exp}(\\widehat{\\beta}_0)\\)\n\nWhen \\(X=0\\), we have \\[\\text{logit}(\\widehat{\\pi}(X=0)) = \\widehat{\\beta}_0\\]\nThus, \\[\\begin{aligned} \\widehat{\\beta}_0 & = \\text{logit}(\\widehat{\\pi}(X)) \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\big[\\text{logit}(\\widehat{\\pi}(X))\\big] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\Bigg[\\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)}\\Bigg)\\Bigg] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-do-we-get-the-odds-ratio-for-xs-coefficient",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-do-we-get-the-odds-ratio-for-xs-coefficient",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "How do we get the odds ratio for X’s coefficient?",
    "text": "How do we get the odds ratio for X’s coefficient?\n\n\nFor \\(\\text{exp}(\\widehat{\\beta}_1)\\)\n\nWe compare \\(X=x\\) and \\(X=x+1\\),\nSo we have \\[\\text{logit}(\\widehat{\\pi}(X = x)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x\\] and \\[\\text{logit}(\\widehat{\\pi}(X = x+1)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot (x+1)\\]\nAnd… \\[\\begin{aligned} & \\text{logit}(\\widehat{\\pi}(X = x+1)) - \\text{logit}(\\widehat{\\pi}(X = x)) \\\\ & =  \n\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot (x+1) - \\big[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x \\big] \\\\ &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x + \\widehat{\\beta}_1 - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 \\cdot x \\\\ & =\n\\widehat{\\beta}_1 \\end{aligned}\\]\n\n\n\nThus, \\[\\begin{aligned} \\widehat{\\beta}_1 & =  \\text{logit}(\\widehat{\\pi}(X = x+1)) - \\text{logit}(\\widehat{\\pi}(X = x)) \\\\\n\\widehat{\\beta}_1 & =  \\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X=x+1)}{1-\\widehat{\\pi}(X=x+1)}\\Bigg) - \\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X=x)}{1-\\widehat{\\pi}(X=x)}\\Bigg) \\\\\n\\widehat{\\beta}_1 & =  \\text{log}\\left(\\dfrac{\\dfrac{\\widehat{\\pi}(X=x+1)}{1-\\widehat{\\pi}(X=x+1)}} {\\dfrac{\\widehat{\\pi}(X=x)}{1-\\widehat{\\pi}(X=x)}}\\right) \\\\\n\\text{exp}\\big[\\widehat{\\beta}_1\\big] & =  \\text{exp}\\left[\\text{log}\\left(\\dfrac{\\text{odds}_{X=x+1}} {\\text{odds}_{X=x}}\\right) \\right] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_1\\big] & =  \\dfrac{\\text{odds}_{X=x+1}} {\\text{odds}_{X=x}} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example",
    "text": "Example\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#transformations-of-continuous-variable-to-make-more-interpretable",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#transformations-of-continuous-variable-to-make-more-interpretable",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Transformations of continuous variable to make more interpretable",
    "text": "Transformations of continuous variable to make more interpretable\n\nSometimes a change in “1” unit may not be considered clinically interesting\n\nFor example, a 1 year increase in age or a 1 mm Hg increase in systolic blood pressure may be too small for a meaningful change in log odds\nInstead, we may be interested to find out the log odds change for a increase of 10 years in age or 10 mm Hg in systolic blood pressure\nOn the other hand, if the range of x is small (say 0-1), than a change in 1 unit of 𝑥 is too large to be meaningful\n\nWe should be able to compute and interpret coefficients for a continuous independent covariate \\(x\\) for an arbitrary change of “c” units in \\(x\\)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-1",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example",
    "text": "Example\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\\[ \\widehat{OR}\\left(10\\right)=\\exp{\\left(10\\cdot{\\hat{\\beta}}_1\\right)}=\\exp{\\left(0.56965\\right)}=\\mathrm{\\mathrm{1.767}}\\]\n\nThe 95% CI for \\(\\widehat{OR}\\left(10\\right)\\) is: \\[\\begin{aligned} \\widehat{OR}\\left(10\\right) &=\\exp{\\left(10\\cdot{\\hat{\\beta}}_1\\pm1.96\\cdot10\\cdot SE_{\\hat{\\beta}_1} \\right)} \\\\ &=\\exp{\\left(10\\cdot0.056965\\pm1.96\\cdot10\\cdot0.003204\\right)}\\\\\n&=(1.66,\\ 1.88) \\end{aligned}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#introrecap-of-interpreting-fitted-model",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#introrecap-of-interpreting-fitted-model",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Intro/Recap of Interpreting Fitted Model",
    "text": "Intro/Recap of Interpreting Fitted Model\n\nInterpret coefficients from fitted logistic regression model\n\nGoodness-of-fit of model should be assessed before summarizing findings (have not covered yet)\nIn this lecture: assume model fits data well\n\nThe interpretation of the coefficients involves two issues:\n\nThe functional relationship between the dependent variable and the independent variable (link function)\nUnit of change for the independent variable\n\nWe will learn the interpretation for\n\nBinary independent variable\nCategorical independent variable with multiple groups\n\nWe looked at this for our race and ethnicity variable\n\nContinuous independent variable"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-continuous-independent-variable-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-continuous-independent-variable-1",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Coefficient interpretation: Continuous Independent Variable 1",
    "text": "Coefficient interpretation: Continuous Independent Variable 1\n\nFor simplicity, we assume the linear relationship between logit and continuous variable 𝑥\nAgain using simple logistic regression model to illustrate the interpretation of \\(\\widehat{\\beta}\\) for a continuous variable \\(x\\) \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\nThe estimated slope coefficient, \\(\\widehat{\\beta}_1\\), is the expected change in the log odds for 1 unit increase in \\(x\\)\n\nAdditional attention should be paid to picking a meaningful units of change in \\(x\\)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-continuous-independent-variable-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-continuous-independent-variable-2",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Coefficient Interpretation: Continuous Independent Variable 2",
    "text": "Coefficient Interpretation: Continuous Independent Variable 2\n\nSometimes a change in “1” unit may not be considered clinically interesting\n\nFor example, a 1 year increase in age or a 1 mm Hg increase in systolic blood pressure may be too small for a meaningful change in log odds\nInstead, we may be interested to find out the log odds change for a increase of 10 years in age or 10 mm Hg in systolic blood pressure\nOn the other hand, if the range of x is small (say 0-1), than a change in 1 unit of 𝑥 is too large to be meaningful\n\nWe should be able to compute and interpret coefficients for a continuous independent covariate 𝑥 for an arbitrary change of “c” units in 𝑥"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-continuous-independent-variable-3",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-continuous-independent-variable-3",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Coefficient Interpretation: Continuous Independent Variable 3",
    "text": "Coefficient Interpretation: Continuous Independent Variable 3"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-i",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-i",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (I)",
    "text": "Example: Age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\nNeeded steps:\n\nFit the regression model\nTransform the coefficients into odds ratios\nInterpret the odds ratio"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-2",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis 2",
    "text": "Example: Age and Late Stage Diagnosis 2\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nInterpret the odds ratio\n\nFor every one year increase in age, there is an estimated 5.86% increase in the estimated odds of late stage breast cancer diagnosis (95% CI: 5.2%, 6.53%)."
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-interpretation-of-age-coefficientor-i",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-interpretation-of-age-coefficientor-i",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: Interpretation of Age Coefficient/OR (I)",
    "text": "Example: Interpretation of Age Coefficient/OR (I)\n\n\\(\\widehat{\\beta}_1\\) is 0.057, suggesting that one year increase in age is associated with 0.057 increase in log odds of receiving a late stage breast cancer diagnosis\n\\(\\exp\\left({\\widehat{\\beta}}_1\\right)\\) is 1.06, suggesting that one year increase in age is associated with 1.06 times the odds of receiving a late stage breast cancer diagnosis\nFor continuous covariates in logistic regression model, it is helpful to subtract 1 from the odds ratio and multiply by 100 to obtain the percentage change in odds for 1-unit increase.\n\nThe estimated OR for age is 1.06, suggesting that a 1-year increase in age is associated with a 6% increase in the predicted odds of late stage diagnosis in the patient population"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-interpretation-of-age-coefficientor-i-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-interpretation-of-age-coefficientor-i-1",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: Interpretation of Age Coefficient/OR (I)",
    "text": "Example: Interpretation of Age Coefficient/OR (I)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-interpretation-of-age-coefficientor-i-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-interpretation-of-age-coefficientor-i-2",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: Interpretation of Age Coefficient/OR (I)",
    "text": "Example: Interpretation of Age Coefficient/OR (I)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#last-note-about-continuous-independent-variable",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#last-note-about-continuous-independent-variable",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Last Note About Continuous Independent Variable",
    "text": "Last Note About Continuous Independent Variable\n\nNotice that the logistic regression model suggests that logit is linear in the covariate\nThe model implies the additional risk of late stage breast cancer diagnosis for a 40 year-old compared to a 30 year-old is the same as the additional risk of late stage breast cancer diagnosis for a 60 year-old compared to a 50-year-old\nThis assumption may not be realistic\nTo address this, we may consider using higher order terms (e.g., \\(x^2\\), \\(x^3\\),…) or other nonlinear transformation(e.g., \\(log(x)\\))\nCategorize the continuous variable may be another option"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-binary-independent-variable",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-binary-independent-variable",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Coefficient Interpretation: Binary Independent Variable",
    "text": "Coefficient Interpretation: Binary Independent Variable\n\nIndependent variable \\(x\\) is a binary variable (\\(x\\) can take values: 0 or 1)\nWe are fitting the simple logistic regression model: \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=1)\\]\nThe logit difference is \\(\\beta_1\\) for binary independent variable\n\n\\(\\beta_1\\) represents the change/difference in the logit for \\(x=1\\) vs. \\(x=0\\)\n\nIt will be much easier to understand if we can interpret the coefficient using odds ratio (OR)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#binary-how-do-we-interpret-the-coefficient-i",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#binary-how-do-we-interpret-the-coefficient-i",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Binary: How do we interpret the coefficient? (I)",
    "text": "Binary: How do we interpret the coefficient? (I)\n\nFor individuals with \\(X=0\\): \\[\\text{logit}\\left(\\pi(X=0)\\right)=\\beta_0+\\beta_1\\times\\left(0\\right)=\\beta_0\\]\nFor individuals with \\(X=1\\): \\[\\text{logit}\\left(\\pi(X=1)\\right)=\\beta_0+\\beta_1\\times\\left(1\\right)=\\beta_0 + \\beta_1\\]\nTo solve for \\(\\beta_1\\), we take the difference of the logits: \\[ \\text{logit}\\left(\\pi(X=1)\\right) - \\text{logit}\\left(\\pi(X=0)\\right) = \\left( \\beta_0 + \\beta_1 \\right) - \\left( \\beta_0 \\right) = \\beta_1\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#binary-how-do-we-interpret-the-coefficient-ii",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#binary-how-do-we-interpret-the-coefficient-ii",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Binary: How do we interpret the coefficient? (II)",
    "text": "Binary: How do we interpret the coefficient? (II)\n\\[ \\text{logit}\\left(\\pi(X=1)\\right) - \\text{logit}\\left(\\pi(X=0)\\right) = \\left( \\beta_0 + \\beta_1 \\right) - \\left( \\beta_0 \\right) = \\beta_1\\]\n\\[\\begin{aligned}\n\\beta_1&=l\\mathrm{ogit}\\left(\\pi(X=1)\\right)\\ -l\\mathrm{ogit}\\left(\\pi\\left(X=0\\right)\\right) \\\\  \\beta_1&=l\\mathrm{og}\\left(\\dfrac{\\pi(X=1)}{1-\\pi(X=1)}\\right)-l\\mathrm{og}\\left(\\dfrac{\\pi\\left(X=0\\right)}{1-\\pi\\left(X=0\\right)}\\right) \\\\\n\\beta_1&=\\log{\\left(\\dfrac{\\dfrac{\\pi(X=1)}{1-\\pi(X=1)}}{\\dfrac{\\pi(X=0)}{1-\\pi(X=0)}}\\right)} \\\\ \\exp{\\left(\\beta_1\\right)}&=\\dfrac{\\dfrac{\\pi(X=1)}{1-\\pi(X=1)}}{\\dfrac{\\pi(X=0)}{1-\\pi(X=0)}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#review-of-odds-ratio",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#review-of-odds-ratio",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Review of Odds Ratio",
    "text": "Review of Odds Ratio\n\nOdds for a subject with \\(X=1\\): \\[\\text{odds}_1 = \\dfrac{\\pi(X=1)}{1-\\pi(X=1)}\\]\nOdds for a subject with \\(X=0\\): \\[\\text{odds}_0 = \\dfrac{\\pi(X=0)}{1-\\pi(X=0)}\\]\nOdds Ratio for \\(X=1\\) vs. \\(X=0\\): \\[OR = \\dfrac{\\dfrac{\\pi(X=1)}{1-\\pi(X=1)}}{\\dfrac{\\pi(X=0)}{1-\\pi(X=0)}}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-does-this-relate-to-a-2x2-table",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-does-this-relate-to-a-2x2-table",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "How does this relate to a 2x2 table?",
    "text": "How does this relate to a 2x2 table?\n\n\n\n2x2 table with the respective logistic functions in each cell"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-3",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-3",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-does-this-relate-to-a-2x2-table-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-does-this-relate-to-a-2x2-table-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "How does this relate to a 2x2 table?",
    "text": "How does this relate to a 2x2 table?\n\\[\nOR=\\dfrac{a/c}{b/d}=\\dfrac{\\dfrac{\\left(\\dfrac{\\exp{\\left(\\beta_0+\\beta_1\\right)}}{1+\\exp{\\left(\\beta_0+\\beta_1\\right)}}\\right)}{\\left(\\dfrac{1}{1+\\exp(\\beta_0+\\beta_1)}\\right)}}{\\dfrac{\\left(\\dfrac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)}\\right)}{\\left(\\dfrac{1}{1+\\exp{\\left(\\beta_0\\right)}}\\right)}}=\\dfrac{\\exp(\\beta_0+\\beta_1)}{\\exp(\\beta_0)}=e^{\\beta_1}\n\\]\n\nSimple relationship between coefficient and odds ratio is a primary reason why we report OR for categorical data analysis.\nFor binary independent variable x, OR computed in logistic regression model is the same as OR computed using contingency table"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-binary-age-and-late-stage-diagnosis-i",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-binary-age-and-late-stage-diagnosis-i",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Binary age and Late Stage Diagnosis (I)",
    "text": "Example: Binary age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for older individuals (&gt;65 years old) compared to younger individuals (≤65 years old)?\n\n\n\nTwo options to calculate this value:\n\nOption 1: Calculate \\(\\widehat{OR}\\) from 2x2 contingency table\n\nRefer to Lesson 3 for this process\n\nOption 2: Calculate \\(\\widehat{OR}\\) from logistic regression\n\n\nNeeded steps for Option 2:\n\nFit the regression model\nTransform the coefficients into odds ratios\nInterpret the odds ratio"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#computing-or-from-𝛽-i",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#computing-or-from-𝛽-i",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Computing OR from 𝛽 (I)",
    "text": "Computing OR from 𝛽 (I)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-multi-group-categorical-variable",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-multi-group-categorical-variable",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Coefficient Interpretation: Multi-group Categorical Variable",
    "text": "Coefficient Interpretation: Multi-group Categorical Variable\n\nIndependent variable \\(x\\) is a multi-level categorical variable\nLet’s say \\(X\\) takes values: a, b, c, or d\nWe are fitting the simple logistic regression model: \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]\n\nWhere \\(a\\) is our reference group\n\nThe logit difference is \\(\\beta_1\\) for binary independent variable\n\n\\(\\beta_1\\) represents the change/difference in the logit for \\(x=b\\) vs. \\(x=a\\)\n\nIt will be much easier to understand if we can interpret the coefficient using odds ratio (OR)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-do-we-pick-the-reference-group",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-do-we-pick-the-reference-group",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "How do we pick the reference group?",
    "text": "How do we pick the reference group?\n\nThe choice can be more apparent for multi-group categorical independent variables within studies\nFor example, if we want to evaluate the association between clinical response and four treatments.\n\nThe treatment variable has 4 categories: “active treatment A”, “active treatment B”, “active treatment C” and “Placebo treatment”\nThe investigator is interested in comparing each of the three active treatment with the placebo treatment\nThen the placebo treatment should be picked as the reference group"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\n\nChose Non-Hispanic White individuals as reference group\nUnderlying health disparities linked to racism in healthcare and in clinical studies\nThere is evidence that white individuals receive a certain standard of care that is not paralleled for POC Mateo and Williams (2021)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#what-if-you-want-to-compare-other-groups",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#what-if-you-want-to-compare-other-groups",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "What if you want to compare other groups?",
    "text": "What if you want to compare other groups?\n\nWhat if we want to estimate OR comparing Non-Hispanic Asian Pacific Islander to Non-Hispanic Black individuals?\nOption 1: Change reference group and refit the model (maybe the easiest option)\nOption 2: Estimate OR using fitted coefficients (\\(\\widehat{\\beta}\\)’s) in the current model: \\[\\begin{aligned} \\text{log}\\left( OR (\\text{NH API}, \\text{NH B}) \\right) &= \\text{logit}\\left(\\pi \\left(X = \\text{NH API}\\right)\\right) - \\text{logit}\\left(\\pi \\left(X = \\text{NH B}\\right)\\right) \\\\\n& = \\left[\\beta_0 + \\beta_3 \\cdot 1\\right] - \\left[\\beta_0 + \\beta_4 \\cdot 1 \\right] \\\\\n\\text{log}\\left( \\widehat{OR} (\\text{NH API}, \\text{NH B}) \\right) &= \\widehat{\\beta}_3 - \\widehat{\\beta}_4 \\\\\n\\widehat{OR} (\\text{NH API}, \\text{NH B}) &= \\exp \\left( \\widehat{\\beta}_3 - \\widehat{\\beta}_4 \\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-5",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-5",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#transformations-of-continuous-variable-to-make-more-interpretable-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#transformations-of-continuous-variable-to-make-more-interpretable-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Transformations of continuous variable to make more interpretable",
    "text": "Transformations of continuous variable to make more interpretable\n\nThe estimated log odds ratio for a change of c units in x can be obtained from \\[\\hat{g}\\left(x+c\\right)-\\hat{g}\\left(x\\right)=c{\\hat{\\beta}}_1\\]\n\n\\(\\widehat{OR}\\left(c\\right)=\\exp\\left(c{\\hat{\\beta}}_1\\right)\\)\n\nThe 95% CI for \\(\\widehat{OR}(c)\\) is: \\[\\exp \\left( c \\hat{\\beta}_1 \\pm 1.96 \\cdot c \\cdot SE_{\\hat{\\beta}_1} \\right)\\]\nThe \\(c\\) is chosen to be a clinically meaningful unit change in \\(x\\)\nThe value of 𝑐 should be clearly specified in all tables and calculations\n\nBecause the estimated OR and the corresponding CI depends on the choice of 𝑐 value"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-2",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example",
    "text": "Example\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\nbc2 = bc %&gt;% mutate(Age_c_10 = Age_c/10)\nbc_reg_10 = glm(Late_stage_diag ~ Age_c_10, data = bc2, family = binomial)\ntidy(bc_reg_10, conf.int=T, exponentiate = T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.023\n−42.637\n0.000\n0.355\n0.389\n    Age_c_10\n1.768\n0.032\n17.780\n0.000\n1.661\n1.883"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-3",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-3",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example",
    "text": "Example\n\nHow do we do this in R?\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T, exponentiate=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;% fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.023\n−42.637\n0.000\n0.355\n0.389\n    Age_c\n1.059\n0.003\n17.780\n0.000\n1.052\n1.065\n  \n  \n  \n\n\n\nlogistic.display(bc_reg, decimal = 3)\n\n\nLogistic regression predicting Late_stage_diag : 1 vs 0 \n \n                   OR(95%CI)            P(Wald's test) P(LR-test)\nAge_c (cont. var.) 1.059 (1.052,1.065)  &lt; 0.001        &lt; 0.001   \n                                                                 \nLog-likelihood = -5754.84419\nNo. of observations = 10000\nAIC value = 11513.68837"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-4",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-4",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example",
    "text": "Example\nFor our fitted simple logistic regression model with age as a predictor \\[\\text{logit}(\\widehat{\\pi}(Age)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot Age\\]\n\n\\(\\widehat{\\beta}_0\\): estimated log-odds when age is 61.71 years\n\\(\\widehat{\\beta}_1\\): estimated increase in log-odds for every 1 year increase in age"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-i-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-i-1",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (I)",
    "text": "Example: Age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nFit the regression model\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-i-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-i-2",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (I)",
    "text": "Example: Age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nTransform the coefficients into odds ratios\n\n\nOption 1: tidy()\n\n\ntidy_bc_reg = tidy(bc_reg, conf.int=T, exponentiate = T) \ntidy_bc_reg %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.023\n−42.637\n0.000\n0.355\n0.389\n    Age_c\n1.059\n0.003\n17.780\n0.000\n1.052\n1.065\n  \n  \n  \n\n\n\ntidy_bc_reg$conf.low # I prefer tidy() bc now I can grab each component\n\n[1] 0.3551931 1.0520321"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-i-3",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-i-3",
    "title": "Lesson 8: Interpretations and Visualizations of Results in Simple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (I)",
    "text": "Example: Age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nTransform the coefficients into odds ratios\n\n\nOption 2: logistic.display()\n\n\nlogistic.display(bc_reg) # Cannot grab each component in this\n\n\nLogistic regression predicting Late_stage_diag : 1 vs 0 \n \n                   OR(95%CI)         P(Wald's test) P(LR-test)\nAge_c (cont. var.) 1.06 (1.05,1.07)  &lt; 0.001        &lt; 0.001   \n                                                              \nLog-likelihood = -5754.8442\nNo. of observations = 10000\nAIC value = 11513.6884"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-interpretation-of-age-coefficientor",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-interpretation-of-age-coefficientor",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Interpretation of Age Coefficient/OR",
    "text": "Example: Interpretation of Age Coefficient/OR\n\n\\(\\widehat{\\beta}_1\\) is 0.057, suggesting that one year increase in age is associated with 0.057 increase in log odds of receiving a late stage breast cancer diagnosis\n\\(\\exp\\left({\\widehat{\\beta}}_1\\right)\\) is 1.06, suggesting that one year increase in age is associated with 1.06 times the odds of receiving a late stage breast cancer diagnosis\nFor continuous covariates in logistic regression model, it is helpful to subtract 1 from the odds ratio and multiply by 100 to obtain the percentage change in odds for 1-unit increase.\n\nThe estimated OR for age is 1.06, suggesting that a 1-year increase in age is associated with a 6% increase in the predicted odds of late stage diagnosis in the patient population"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-1-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-1-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#poll-everywhere-question-2",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-binary-age-and-late-stage-diagnosis-i-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-binary-age-and-late-stage-diagnosis-i-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Binary age and Late Stage Diagnosis (I)",
    "text": "Example: Binary age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for older individuals (&gt;65 years old) compared to younger individuals (≤65 years old)?\n\n\n\nFit the regression model\n\n\nbc3 = bc %&gt;% mutate(Age_binary = ifelse(Age &gt; 65, 1, 0))\nage_bin_glm = glm(Late_stage_diag ~ Age_binary, data = bc3, family = binomial)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-binary-age-and-late-stage-diagnosis-i-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-binary-age-and-late-stage-diagnosis-i-2",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Binary age and Late Stage Diagnosis (I)",
    "text": "Example: Binary age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for older individuals (&gt;65 years old) compared to younger individuals (≤65 years old)?\n\n\n\nTransform the coefficients into odds ratios\n\n\nage_bin_tidy = tidy(age_bin_glm, conf.int=T, exponentiate = T) \nage_bin_tidy %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.297\n0.031\n−39.608\n0.000\n0.280\n0.315\n    Age_binary\n1.875\n0.045\n13.928\n0.000\n1.716\n2.048"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-binary-age-and-late-stage-diagnosis-i-3",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-binary-age-and-late-stage-diagnosis-i-3",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Binary age and Late Stage Diagnosis (I)",
    "text": "Example: Binary age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for older individuals (&gt;65 years old) compared to younger individuals (≤65 years old)?\n\n\n\nInterpret the odds ratio\n\nThe estimated odds of late stage breast cancer among individuals over 65 years old is 1.87 (95% CI: (1.72, 2.05)) times that of individuals 65 years or younger."
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-multi-group-categorical-variable-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-multi-group-categorical-variable-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Coefficient Interpretation: Multi-group Categorical Variable",
    "text": "Coefficient Interpretation: Multi-group Categorical Variable\nWe are fitting the simple logistic regression model with reference group \\(a\\): \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]\n\n\\(\\beta_0\\): the log-odds of event \\(Y=1\\) for group \\(a\\)\n\\(\\beta_1\\): the difference in log-odds of event \\(Y=1\\) comparing group \\(b\\) to group \\(a\\)\n\\(\\beta_2\\): the difference in log-odds of event \\(Y=1\\) comparing group \\(c\\) to group \\(a\\)\n\\(\\beta_3\\): the difference in log-odds of event \\(Y=1\\) comparing group \\(d\\) to group \\(a\\)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#multi-level-categorical-how-do-we-interpret-the-coefficient-ii",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#multi-level-categorical-how-do-we-interpret-the-coefficient-ii",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Multi-level categorical: How do we interpret the coefficient? (II)",
    "text": "Multi-level categorical: How do we interpret the coefficient? (II)\n\\[ \\text{logit}\\left(\\pi(X=c)\\right) - \\text{logit}\\left(\\pi(X=a)\\right) = \\left( \\beta_0 + \\beta_1\\cdot 0 + \\beta_2\\cdot 1 + \\beta_3\\cdot 0  \\right) - \\left( \\beta_0 + \\beta_1\\cdot 0 + \\beta_2\\cdot 0 + \\beta_3\\cdot 0 \\right) = \\beta_2\\]\n\\[\\begin{aligned}\n\\beta_2&=l\\mathrm{ogit}\\left(\\pi(X=c)\\right)\\ -l\\mathrm{ogit}\\left(\\pi\\left(X=a\\right)\\right) \\\\  \\beta_2&=l\\mathrm{og}\\left(\\dfrac{\\pi(X=c)}{1-\\pi(X=c)}\\right)-l\\mathrm{og}\\left(\\dfrac{\\pi\\left(X=a\\right)}{1-\\pi\\left(X=a\\right)}\\right) \\\\\n\\beta_2&=\\log{\\left(\\dfrac{\\dfrac{\\pi(X=c)}{1-\\pi(X=c)}}{\\dfrac{\\pi(X=a)}{1-\\pi(X=a)}}\\right)} \\\\ \\exp{\\left(\\beta_2\\right)}&=\\dfrac{\\dfrac{\\pi(X=c)}{1-\\pi(X=c)}}{\\dfrac{\\pi(X=a)}{1-\\pi(X=a)}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-multi-group-categorical-variable-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-multi-group-categorical-variable-2",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Coefficient Interpretation: Multi-group Categorical Variable",
    "text": "Coefficient Interpretation: Multi-group Categorical Variable\nWe are fitting the simple logistic regression model with reference group \\(a\\): \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]\n\n\\(\\exp\\left(\\beta_0\\right)\\): the odds of event \\(Y=1\\) for group \\(a\\)\n\\(\\exp\\left(\\beta_1\\right)\\): the odds of event \\(Y=1\\) for group \\(b\\) is \\(\\exp\\left(\\beta_1\\right)\\) times the odds of event \\(Y=1\\) for group \\(a\\)\n\\(\\exp\\left(\\beta_2\\right)\\): the odds of event \\(Y=1\\) for group \\(c\\) is \\(\\exp\\left(\\beta_2\\right)\\) times the odds of event \\(Y=1\\) for group \\(a\\)\n\\(\\exp\\left(\\beta_3\\right)\\): the odds of event \\(Y=1\\) for group \\(d\\) is \\(\\exp\\left(\\beta_3\\right)\\) times the odds of event \\(Y=1\\) for group \\(a\\)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#references",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#references",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "References",
    "text": "References\n\n\nLesson 8: Interpretations and Visualizations of Odds Ratios\n\n\n\nMateo, Camila M., and David R. Williams. 2021. “Racism: A Fundamental Driver of Racial Disparities in Health-Care Quality.” Nature Reviews Disease Primers 7 (1): 1–2. https://doi.org/10.1038/s41572-021-00258-1.\n\n\nYedjou, Clement G., Jennifer N. Sims, Lucio Miele, Felicite Noubissi, Leroy Lowe, Duber D. Fonseca, Richard A. Alo, Marinelle Payton, and Paul B. Tchounwou. 2019. “Health and Racial Disparity in Breast Cancer.” Advances in Experimental Medicine and Biology 1152: 31–49. https://doi.org/10.1007/978-3-030-20301-6_3."
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islander individuals compared to Non-Hispanic White individuals?\n\n\nNeeded steps:\n\nFit the regression model\nTransform the coefficients into odds ratios\nInterpret the odds ratio"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity-2",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islander individuals compared to Non-Hispanic White individuals?\n\n\n\nFit the regression model\n\n\nRE_glm = glm(Late_stage_diag ~ Race_Ethnicity, data = bc,  \n               family = binomial)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity-3",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity-3",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islander individuals compared to Non-Hispanic White individuals?\n\n\n\nTransform the coefficients into odds ratios\n\n\nRE_tidy = tidy(RE_glm, conf.int=T, exponentiate = T) \nRE_tidy %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.026\n−37.553\n0.000\n0.353\n0.392\n    Race_EthnicityHispanic-Latino\n0.968\n0.082\n−0.398\n0.691\n0.822\n1.135\n    Race_EthnicityNH American Indian/Alaskan Native\n0.948\n0.476\n−0.111\n0.911\n0.342\n2.287\n    Race_EthnicityNH Asian/Pacific Islander\n1.131\n0.082\n1.497\n0.134\n0.961\n1.327\n    Race_EthnicityNH Black\n1.405\n0.070\n4.826\n0.000\n1.223\n1.611"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity-4",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-late-stage-diagnosis-and-race-and-ethnicity-4",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islander individuals compared to Non-Hispanic White individuals?\n\n\n\nInterpret the odds ratio\n\nThe estimated odds of late stage breast cancer among Non-Hispanic Asian/Pacific Islander individuals is 1.13 (95% CI: (0.96, 1.33)) times that of Non-Hispanic White individuals."
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#what-if-you-want-to-compare-other-groups-option-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#what-if-you-want-to-compare-other-groups-option-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "What if you want to compare other groups? Option 1",
    "text": "What if you want to compare other groups? Option 1\n\nbc3 = bc %&gt;% \n  mutate(Race_Ethnicity = relevel(Race_Ethnicity, ref = \"NH Black\"))\nRE_glm2 = glm(Late_stage_diag ~ Race_Ethnicity, data = bc3, \n               family = binomial)\ntidy(RE_glm2, conf.int=T, exponentiate = T) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 38) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.523\n0.065\n−9.934\n0.000\n0.459\n0.594\n    Race_EthnicityNH White\n0.712\n0.070\n−4.826\n0.000\n0.621\n0.818\n    Race_EthnicityHispanic-Latino\n0.689\n0.102\n−3.664\n0.000\n0.564\n0.840\n    Race_EthnicityNH American Indian/Alaskan Native\n0.675\n0.479\n−0.819\n0.413\n0.242\n1.641\n    Race_EthnicityNH Asian/Pacific Islander\n0.805\n0.102\n−2.131\n0.033\n0.659\n0.982"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-to-present-odds-ratios-table",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-to-present-odds-ratios-table",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "How to present odds ratios: Table",
    "text": "How to present odds ratios: Table\n\ntbl_regression() in the gtsummary package is helpful for presenting the odds ratios in a clean way\n\n\nlibrary(gtsummary)\ntbl_regression(RE_glm, exponentiate = TRUE) %&gt;% \n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Race_Ethnicity\n\n\n\n        NH White\n—\n—\n\n        Hispanic-Latino\n0.97\n0.82, 1.14\n0.7\n        NH American Indian/Alaskan Native\n0.95\n0.34, 2.29\n&gt;0.9\n        NH Asian/Pacific Islander\n1.13\n0.96, 1.33\n0.13\n        NH Black\n1.40\n1.22, 1.61\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-to-present-odds-ratios-forest-plot",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-to-present-odds-ratios-forest-plot",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot\n\nggplot(data=RE_tidy, aes(y=label, x=estimate, xmin=conf.low, xmax=conf.high)) + \n  geom_point(size = 3) +  geom_errorbarh(height=.2) + \n  geom_vline(xintercept=1, color='#C2352F', linetype='dashed', alpha=1) +\n  theme_classic() +\n  labs(x = \"OR (95% CI)\", y = \"Race and ethnicity\", \n       title = \"Odds ratios of Late Stage \\n Breast Cancer Diagnosis\") +\n  theme(axis.title = element_text(size = 25), axis.text = element_text(size = 25), title = element_text(size = 25))"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-to-present-odds-ratios-forest-plot-setup",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#how-to-present-odds-ratios-forest-plot-setup",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nlibrary(broom.helpers)\nRE_tidy = tidy_and_attach(RE_glm, conf.int=T, exponentiate = T) %&gt;%\n  tidy_remove_intercept() %&gt;%\n  tidy_add_reference_rows() %&gt;% tidy_add_estimate_to_reference_rows() %&gt;%\n  tidy_add_term_labels()\nglimpse(RE_tidy)\n\nRows: 5\nColumns: 16\n$ term           &lt;chr&gt; \"Race_EthnicityNH White\", \"Race_EthnicityHispanic-Latin…\n$ variable       &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_label      &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_class      &lt;chr&gt; \"factor\", \"factor\", \"factor\", \"factor\", \"factor\"\n$ var_type       &lt;chr&gt; \"categorical\", \"categorical\", \"categorical\", \"categoric…\n$ var_nlevels    &lt;int&gt; 5, 5, 5, 5, 5\n$ contrasts      &lt;chr&gt; \"contr.treatment\", \"contr.treatment\", \"contr.treatment\"…\n$ contrasts_type &lt;chr&gt; \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tr…\n$ reference_row  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE\n$ label          &lt;chr&gt; \"NH White\", \"Hispanic-Latino\", \"NH American Indian/Alas…\n$ estimate       &lt;dbl&gt; 1.0000000, 0.9678002, 0.9484848, 1.1310170, 1.4046741\n$ std.error      &lt;dbl&gt; NA, 0.08224948, 0.47558680, 0.08224988, 0.07041472\n$ statistic      &lt;dbl&gt; NA, -0.3979312, -0.1112089, 1.4968682, 4.8257715\n$ p.value        &lt;dbl&gt; NA, 6.906809e-01, 9.114507e-01, 1.344276e-01, 1.394623e…\n$ conf.low       &lt;dbl&gt; NA, 0.8223138, 0.3417844, 0.9612074, 1.2226824\n$ conf.high      &lt;dbl&gt; NA, 1.135332, 2.286596, 1.327092, 1.611466"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-continuous-independent-variable",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#coefficient-interpretation-continuous-independent-variable",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Coefficient interpretation: Continuous Independent Variable",
    "text": "Coefficient interpretation: Continuous Independent Variable\n\nFor simplicity, we assume the linear relationship between logit and continuous variable 𝑥\nAgain using simple logistic regression model to illustrate the interpretation of \\(\\widehat{\\beta}\\) for a continuous variable \\(x\\) \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\nThe estimated slope coefficient, \\(\\widehat{\\beta}_1\\), is the expected change in the log odds for 1 unit increase in \\(x\\)\n\nAdditional attention should be paid to picking a meaningful units of change in \\(x\\)"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-15",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-15",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Age and Late Stage Diagnosis (1/5)",
    "text": "Example: Age and Late Stage Diagnosis (1/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\nNeeded steps:\n\nFit the regression model\nTransform the coefficients into odds ratios\nInterpret the odds ratio"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-25",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-25",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Age and Late Stage Diagnosis (2/5)",
    "text": "Example: Age and Late Stage Diagnosis (2/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nFit the regression model\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-35",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-35",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Age and Late Stage Diagnosis (3/5)",
    "text": "Example: Age and Late Stage Diagnosis (3/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nTransform the coefficients into odds ratios\n\n\nOption 1: tidy()\n\n\ntidy_bc_reg = tidy(bc_reg, conf.int=T, exponentiate = T) \ntidy_bc_reg %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.023\n−42.637\n0.000\n0.355\n0.389\n    Age_c\n1.059\n0.003\n17.780\n0.000\n1.052\n1.065\n  \n  \n  \n\n\n\ntidy_bc_reg$conf.low # I prefer tidy() bc now I can grab each component\n\n[1] 0.3551931 1.0520321"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-45",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-45",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Age and Late Stage Diagnosis (4/5)",
    "text": "Example: Age and Late Stage Diagnosis (4/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nTransform the coefficients into odds ratios\n\n\nOption 2: logistic.display()\n\n\nlogistic.display(bc_reg) # Cannot grab each component in this\n\n\nLogistic regression predicting Late_stage_diag : 1 vs 0 \n \n                   OR(95%CI)         P(Wald's test) P(LR-test)\nAge_c (cont. var.) 1.06 (1.05,1.07)  &lt; 0.001        &lt; 0.001   \n                                                              \nLog-likelihood = -5754.8442\nNo. of observations = 10000\nAIC value = 11513.6884"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-55",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-age-and-late-stage-diagnosis-55",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: Age and Late Stage Diagnosis (5/5)",
    "text": "Example: Age and Late Stage Diagnosis (5/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nInterpret the odds ratio\n\nFor every one year increase in age, there is an estimated 5.86% increase in the estimated odds of late stage breast cancer diagnosis (95% CI: 5.2%, 6.53%)."
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-10-year-increase-in-age-and-late-stage-diagnosis",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-10-year-increase-in-age-and-late-stage-diagnosis",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: 10 year increase in age and Late Stage Diagnosis",
    "text": "Example: 10 year increase in age and Late Stage Diagnosis\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-10-year-increase-in-age-and-late-stage-diagnosis-1",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-10-year-increase-in-age-and-late-stage-diagnosis-1",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: 10 year increase in age and Late Stage Diagnosis",
    "text": "Example: 10 year increase in age and Late Stage Diagnosis\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\\[ \\widehat{OR}\\left(10\\right)=\\exp{\\left(10\\cdot{\\hat{\\beta}}_1\\right)}=\\exp{\\left(0.56965\\right)}=\\mathrm{\\mathrm{1.767}}\\]\n\nThe 95% CI for \\(\\widehat{OR}\\left(10\\right)\\) is: \\[\\begin{aligned} \\widehat{OR}\\left(10\\right) &=\\exp{\\left(10\\cdot{\\hat{\\beta}}_1\\pm1.96\\cdot10\\cdot SE_{\\hat{\\beta}_1} \\right)} \\\\ &=\\exp{\\left(10\\cdot0.056965\\pm1.96\\cdot10\\cdot0.003204\\right)}\\\\\n&=(1.66,\\ 1.88) \\end{aligned}\\]"
  },
  {
    "objectID": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-10-year-increase-in-age-and-late-stage-diagnosis-2",
    "href": "lectures/08_Interpretations_SLR/08_Interpretations_SLR.html#example-10-year-increase-in-age-and-late-stage-diagnosis-2",
    "title": "Lesson 8: Interpretations and Visualizations of Odds Ratios",
    "section": "Example: 10 year increase in age and Late Stage Diagnosis",
    "text": "Example: 10 year increase in age and Late Stage Diagnosis\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\nbc2 = bc %&gt;% mutate(Age_c_10 = Age_c/10)\nbc_reg_10 = glm(Late_stage_diag ~ Age_c_10, data = bc2, family = binomial)\ntidy(bc_reg_10, conf.int=T, exponentiate = T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.023\n−42.637\n0.000\n0.355\n0.389\n    Age_c_10\n1.768\n0.032\n17.780\n0.000\n1.661\n1.883"
  },
  {
    "objectID": "project/Lab_03_instructions.html",
    "href": "project/Lab_03_instructions.html",
    "title": "Lab 3 Instructions",
    "section": "",
    "text": "Note\n\n\n\nThis is the required work for Lab 3. I am working on guided instructions on incorporating the optional missing data section of the Lab."
  },
  {
    "objectID": "project/Lab_03_instructions.html#directions",
    "href": "project/Lab_03_instructions.html#directions",
    "title": "Lab 3 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Purpose\nThe purpose of this lab is to fit a multiple logistic regression model and practice how we would interpret our results for this study.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/Lab_03_instructions.html#lab-activities",
    "href": "project/Lab_03_instructions.html#lab-activities",
    "title": "Lab 3 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format (1 sentence). You can change the wording if you’d like, but please make sure it is still clear. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Fit a multiple logistic regression model\n\n\n\n\n\n\nTask\n\n\n\nFit a main effects model with 5 of your covariates. This will not necessarily be your final model, but we can construct interpretations and code that will be useful in your final model.\n\n\n\n\n2.3 Present the odds ratio in a table\n\n\n\n\n\n\nTask\n\n\n\nUse the tbl_regression() function to make a table of the odds ratios from your fitted model.\n\n\n\n\n2.4 Present the odds ratio in a forest plot\n\n\n\n\n\n\nTask\n\n\n\nUse the code from Lesson 10 to make a forest plot of the odds ratio from your fitted model.\n\n\n\n\n2.5 Interpret the odds ratio for your main covariate\n\n\n\n\n\n\nTask\n\n\n\nInterpret the odds ratio(s) for the covariate from your research question. Make sure to include the 95% confidence intervals and what other variables you adjusted for."
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_03.html",
    "href": "project/LastName_FirstInit_Lab_03.html",
    "title": "Lab 3",
    "section": "",
    "text": "Note\n\n\n\nThis is the required work for Lab 3. I am working on guided instructions on incorporating the optional missing data section of the Lab."
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_03.html#directions",
    "href": "project/LastName_FirstInit_Lab_03.html#directions",
    "title": "Lab 3",
    "section": "1 Directions",
    "text": "1 Directions\nThis is your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Purpose\nThe purpose of this lab is to fit a multiple logistic regression model and practice how we would interpret our results for this study.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_03.html#lab-activities",
    "href": "project/LastName_FirstInit_Lab_03.html#lab-activities",
    "title": "Lab 3",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format (1 sentence). You can change the wording if you’d like, but please make sure it is still clear. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Fit a multiple logistic regression model\n\n\n\n\n\n\nTask\n\n\n\nFit a main effects model with 5 of your covariates. This will not necessarily be your final model, but we can construct interpretations and code that will be useful in your final model.\n\n\n\n\n2.3 Present the odds ratio in a table\n\n\n\n\n\n\nTask\n\n\n\nUse the tbl_regression() function to make a table of the odds ratios from your fitted model.\n\n\n\n\n2.4 Present the odds ratio in a forest plot\n\n\n\n\n\n\nTask\n\n\n\nUse the code from Lesson 10 to make a forest plot of the odds ratio from your fitted model.\n\n\n\n\n2.5 Interpret the odds ratio for your main covariate\n\n\n\n\n\n\nTask\n\n\n\nInterpret the odds ratio(s) for the covariate from your research question. Make sure to include the 95% confidence intervals and what other variables you adjusted for."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-time-to-this-time",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-time-to-this-time",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Last time to this time",
    "text": "Last time to this time\n\nUsed the Wald test and Wald 95% confidence interval to interpret coefficients in a fitted model\nThis time: Interpret using odds ratio"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#so-far-weve-looked-at-the-association-using-the-log-odds-scale",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#so-far-weve-looked-at-the-association-using-the-log-odds-scale",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "So far we’ve looked at the association using the log-odds scale",
    "text": "So far we’ve looked at the association using the log-odds scale\n\n\nFor a population simple logistic regression model with a continuous predictor \\[\\text{logit}(\\pi(X)) = \\beta_0 + \\beta_1 \\cdot X\\]\n\n\\(\\beta_0\\): log-odds when X is 0\n\\(\\beta_1\\): increase in log-odds for every 1 unit increase in X\n\n\n\n\nFor our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\n\n\\(\\widehat{\\beta}_0\\): estimated log-odds of \\(Y=1\\) when X is 0.\n\\(\\widehat{\\beta}_1\\): estimated increase in log-odds of \\(Y=1\\) for every 1 unit increase in X\nCan use expected instead of estimated"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-from-last-class",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-from-last-class",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: From last class",
    "text": "Example: From last class\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063\n  \n  \n  \n\n\n\n\n\nFor our fitted simple logistic regression model with age as a predictor \\[\\text{logit}(\\widehat{\\pi}(Age^c)) = −0.989  + 0.057     \\cdot Age^c\\]\n\\(\\widehat{\\beta}_0\\): The estimated log-odds is -0.989 when age is 61.71 years (95% CI: -1.035, -0.944)\n\\(\\widehat{\\beta}_1\\): The estimated increase in log-odds is 0.057 for every 1 year increase in age (95% CI: 0.051, 0.063)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#we-typically-interpret-our-results-using-odds-ratios",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#we-typically-interpret-our-results-using-odds-ratios",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "We typically interpret our results using odds ratios",
    "text": "We typically interpret our results using odds ratios\nFor our fitted simple logistic regression model with a continuous predictor \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\n\nHow do we go from interpretations of \\(\\widehat{\\beta}_0\\) and \\(\\widehat{\\beta}_1\\) using log odds to odds ratios?\nWe will need to take the exponential of our model:\n\n\\(\\text{exp}(\\widehat{\\beta}_0)\\): expected odds that \\(Y=1\\) when X is 0.\n\\(\\text{exp}(\\widehat{\\beta}_1)\\): expected odds ratio that \\(Y=1\\) for every 1 unit increase in X\n\nImportant distinction:\n\nWe take the inverse logit to find our predicted probability\nWe take the exponential to interpret the odds/odds ratios"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introrecap-of-interpreting-fitted-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introrecap-of-interpreting-fitted-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Intro/Recap of Interpreting Fitted Model",
    "text": "Intro/Recap of Interpreting Fitted Model\n\nInterpret coefficients from fitted logistic regression model\n\nGoodness-of-fit of model should be assessed before summarizing findings (have not covered yet)\nIn this lecture: assume model fits data well\n\nThe interpretation of the coefficients involves two issues:\n\nThe functional relationship between the dependent variable and the independent variable (link function)\nUnit of change for the independent variable\n\nWe will learn the interpretation for\n\nBinary independent variable\nCategorical independent variable with multiple groups\n\nWe looked at this for our race and ethnicity variable\n\nContinuous independent variable"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-continuous-independent-variable",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-continuous-independent-variable",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Coefficient interpretation: Continuous Independent Variable",
    "text": "Coefficient interpretation: Continuous Independent Variable\n\nFor simplicity, we assume the linear relationship between logit and continuous variable 𝑥\nAgain using simple logistic regression model to illustrate the interpretation of \\(\\widehat{\\beta}\\) for a continuous variable \\(x\\) \\[\\text{logit}(\\widehat{\\pi}(X)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot X\\]\nThe estimated slope coefficient, \\(\\widehat{\\beta}_1\\), is the expected change in the log odds for 1 unit increase in \\(x\\)\n\nAdditional attention should be paid to picking a meaningful units of change in \\(x\\)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-do-we-get-the-odds-ratio-for-xs-coefficient",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-do-we-get-the-odds-ratio-for-xs-coefficient",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How do we get the odds ratio for X’s coefficient?",
    "text": "How do we get the odds ratio for X’s coefficient?\n\n\nFor \\(\\text{exp}(\\widehat{\\beta}_1)\\)\n\nWe compare \\(X=x\\) and \\(X=x+1\\),\nSo we have \\[\\text{logit}(\\widehat{\\pi}(X = x)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x\\] and \\[\\text{logit}(\\widehat{\\pi}(X = x+1)) = \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot (x+1)\\]\nAnd… \\[\\begin{aligned} & \\text{logit}(\\widehat{\\pi}(X = x+1)) - \\text{logit}(\\widehat{\\pi}(X = x)) \\\\ & =  \n\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot (x+1) - \\big[\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x \\big] \\\\ &= \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot x + \\widehat{\\beta}_1 - \\widehat{\\beta}_0 - \\widehat{\\beta}_1 \\cdot x \\\\ & =\n\\widehat{\\beta}_1 \\end{aligned}\\]\n\n\n\nThus, \\[\\begin{aligned} \\widehat{\\beta}_1 & =  \\text{logit}(\\widehat{\\pi}(X = x+1)) - \\text{logit}(\\widehat{\\pi}(X = x)) \\\\\n\\widehat{\\beta}_1 & =  \\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X=x+1)}{1-\\widehat{\\pi}(X=x+1)}\\Bigg) - \\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X=x)}{1-\\widehat{\\pi}(X=x)}\\Bigg) \\\\\n\\widehat{\\beta}_1 & =  \\text{log}\\left(\\dfrac{\\dfrac{\\widehat{\\pi}(X=x+1)}{1-\\widehat{\\pi}(X=x+1)}} {\\dfrac{\\widehat{\\pi}(X=x)}{1-\\widehat{\\pi}(X=x)}}\\right) \\\\\n\\text{exp}\\big[\\widehat{\\beta}_1\\big] & =  \\text{exp}\\left[\\text{log}\\left(\\dfrac{\\text{odds}_{X=x+1}} {\\text{odds}_{X=x}}\\right) \\right] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_1\\big] & =  \\dfrac{\\text{odds}_{X=x+1}} {\\text{odds}_{X=x}} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-interpretation-of-age-coefficientor",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-interpretation-of-age-coefficientor",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Interpretation of Age Coefficient/OR",
    "text": "Example: Interpretation of Age Coefficient/OR\n\n\\(\\widehat{\\beta}_1\\) is 0.057, suggesting that one year increase in age is associated with 0.057 increase in log odds of receiving a late stage breast cancer diagnosis\n\\(\\exp\\left({\\widehat{\\beta}}_1\\right)\\) is 1.06, suggesting that one year increase in age is associated with 1.06 times the odds of receiving a late stage breast cancer diagnosis\nFor continuous covariates in logistic regression model, it is helpful to subtract 1 from the odds ratio and multiply by 100 to obtain the percentage change in odds for 1-unit increase.\n\nThe estimated OR for age is 1.06, suggesting that a 1-year increase in age is associated with a 6% increase in the predicted odds of late stage diagnosis in the patient population"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-15",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-15",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (1/5)",
    "text": "Example: Age and Late Stage Diagnosis (1/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\nNeeded steps:\n\nFit the regression model\nTransform the coefficients into odds ratios\nInterpret the odds ratio"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-25",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-25",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (2/5)",
    "text": "Example: Age and Late Stage Diagnosis (2/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nFit the regression model\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\nsummary(bc_reg)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Age_c, family = binomial, data = bc)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept) -0.989422   0.023205  -42.64   &lt;2e-16 ***\nAge_c        0.056965   0.003204   17.78   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11510  on 9998  degrees of freedom\nAIC: 11514\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-35",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-35",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (3/5)",
    "text": "Example: Age and Late Stage Diagnosis (3/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nTransform the coefficients into odds ratios\n\n\nOption 1: tidy()\n\n\ntidy_bc_reg = tidy(bc_reg, conf.int=T, exponentiate = T) \ntidy_bc_reg %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.023\n−42.637\n0.000\n0.355\n0.389\n    Age_c\n1.059\n0.003\n17.780\n0.000\n1.052\n1.065\n  \n  \n  \n\n\n\ntidy_bc_reg$conf.low # I prefer tidy() bc now I can grab each component\n\n[1] 0.3551931 1.0520321"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-45",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-45",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (4/5)",
    "text": "Example: Age and Late Stage Diagnosis (4/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nTransform the coefficients into odds ratios\n\n\nOption 2: logistic.display()\n\n\nlogistic.display(bc_reg) # Cannot grab each component in this\n\n\nLogistic regression predicting Late_stage_diag : 1 vs 0 \n \n                   OR(95%CI)         P(Wald's test) P(LR-test)\nAge_c (cont. var.) 1.06 (1.05,1.07)  &lt; 0.001        &lt; 0.001   \n                                                              \nLog-likelihood = -5754.8442\nNo. of observations = 10000\nAIC value = 11513.6884"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-1-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-1-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-55",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-age-and-late-stage-diagnosis-55",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Age and Late Stage Diagnosis (5/5)",
    "text": "Example: Age and Late Stage Diagnosis (5/5)\n\n\nOdds ratio from logistic regression\n\n\nCompute the estimate and 95% confidence interval for odds ratio for late stage breast cancer diagnosis for every 1 year increase in age.\n\n\n\nInterpret the odds ratio\n\nFor every one year increase in age, there is an estimated 5.86% increase in the estimated odds of late stage breast cancer diagnosis (95% CI: 5.2%, 6.53%)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#transformations-of-continuous-variable-to-make-more-interpretable",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#transformations-of-continuous-variable-to-make-more-interpretable",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Transformations of continuous variable to make more interpretable",
    "text": "Transformations of continuous variable to make more interpretable\n\nSometimes a change in “1” unit may not be considered clinically interesting\n\nFor example, a 1 year increase in age or a 1 mm Hg increase in systolic blood pressure may be too small for a meaningful change in log odds\nInstead, we may be interested to find out the log odds change for a increase of 10 years in age or 10 mm Hg in systolic blood pressure\nOn the other hand, if the range of x is small (say 0-1), than a change in 1 unit of 𝑥 is too large to be meaningful\n\nWe should be able to compute and interpret coefficients for a continuous independent covariate \\(x\\) for an arbitrary change of “c” units in \\(x\\)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#transformations-of-continuous-variable-to-make-more-interpretable-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#transformations-of-continuous-variable-to-make-more-interpretable-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Transformations of continuous variable to make more interpretable",
    "text": "Transformations of continuous variable to make more interpretable\n\nThe estimated log odds ratio for a change of c units in x can be obtained from \\[\\hat{g}\\left(x+c\\right)-\\hat{g}\\left(x\\right)=c{\\hat{\\beta}}_1\\]\n\n\\(\\widehat{OR}\\left(c\\right)=\\exp\\left(c{\\hat{\\beta}}_1\\right)\\)\n\nThe 95% CI for \\(\\widehat{OR}(c)\\) is: \\[\\exp \\left( c \\hat{\\beta}_1 \\pm 1.96 \\cdot c \\cdot SE_{\\hat{\\beta}_1} \\right)\\]\nThe \\(c\\) is chosen to be a clinically meaningful unit change in \\(x\\)\nThe value of 𝑐 should be clearly specified in all tables and calculations\n\nBecause the estimated OR and the corresponding CI depends on the choice of 𝑐 value"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-10-year-increase-in-age-and-late-stage-diagnosis",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-10-year-increase-in-age-and-late-stage-diagnosis",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: 10 year increase in age and Late Stage Diagnosis",
    "text": "Example: 10 year increase in age and Late Stage Diagnosis\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\nbc_reg = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\ntidy(bc_reg, conf.int=T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.989\n0.023\n−42.637\n0.000\n−1.035\n−0.944\n    Age_c\n0.057\n0.003\n17.780\n0.000\n0.051\n0.063"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-10-year-increase-in-age-and-late-stage-diagnosis-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-10-year-increase-in-age-and-late-stage-diagnosis-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: 10 year increase in age and Late Stage Diagnosis",
    "text": "Example: 10 year increase in age and Late Stage Diagnosis\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\\[ \\widehat{OR}\\left(10\\right)=\\exp{\\left(10\\cdot{\\hat{\\beta}}_1\\right)}=\\exp{\\left(0.56965\\right)}=\\mathrm{\\mathrm{1.767}}\\]\n\nThe 95% CI for \\(\\widehat{OR}\\left(10\\right)\\) is: \\[\\begin{aligned} \\widehat{OR}\\left(10\\right) &=\\exp{\\left(10\\cdot{\\hat{\\beta}}_1\\pm1.96\\cdot10\\cdot SE_{\\hat{\\beta}_1} \\right)} \\\\ &=\\exp{\\left(10\\cdot0.056965\\pm1.96\\cdot10\\cdot0.003204\\right)}\\\\\n&=(1.66,\\ 1.88) \\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-10-year-increase-in-age-and-late-stage-diagnosis-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-10-year-increase-in-age-and-late-stage-diagnosis-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: 10 year increase in age and Late Stage Diagnosis",
    "text": "Example: 10 year increase in age and Late Stage Diagnosis\n\nWhat if we are interested in learning the OR corresponding to 10-year increase in age?\n\n\nbc2 = bc %&gt;% mutate(Age_c_10 = Age_c/10)\nbc_reg_10 = glm(Late_stage_diag ~ Age_c_10, data = bc2, family = binomial)\ntidy(bc_reg_10, conf.int=T, exponentiate = T) %&gt;% gt() %&gt;% tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.023\n−42.637\n0.000\n0.355\n0.389\n    Age_c_10\n1.768\n0.032\n17.780\n0.000\n1.661\n1.883"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-note-about-continuous-independent-variable",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-note-about-continuous-independent-variable",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Last Note About Continuous Independent Variable",
    "text": "Last Note About Continuous Independent Variable\n\nNotice that the logistic regression model suggests that logit is linear in the covariate\nThe model implies the additional risk of late stage breast cancer diagnosis for a 40 year-old compared to a 30 year-old is the same as the additional risk of late stage breast cancer diagnosis for a 60 year-old compared to a 50-year-old\nThis assumption may not be realistic\nTo address this, we may consider using higher order terms (e.g., \\(x^2\\), \\(x^3\\),…) or other nonlinear transformation(e.g., \\(log(x)\\))\nCategorize the continuous variable may be another option"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-do-we-get-the-odds-for-the-intercept",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-do-we-get-the-odds-for-the-intercept",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How do we get the odds for the intercept?",
    "text": "How do we get the odds for the intercept?\nFor \\(\\text{exp}(\\widehat{\\beta}_0)\\)\n\nWhen \\(X=0\\), we have \\[\\text{logit}(\\widehat{\\pi}(X=0)) = \\widehat{\\beta}_0\\]\nThus, \\[\\begin{aligned} \\widehat{\\beta}_0 & = \\text{logit}(\\widehat{\\pi}(X)) \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\big[\\text{logit}(\\widehat{\\pi}(X))\\big] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\text{exp}\\Bigg[\\text{log}\\Bigg(\\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)}\\Bigg)\\Bigg] \\\\\n\\text{exp}\\big[\\widehat{\\beta}_0\\big] & = \\dfrac{\\widehat{\\pi}(X)}{1-\\widehat{\\pi}(X)} \\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-binary-independent-variable",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-binary-independent-variable",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Coefficient Interpretation: Binary Independent Variable",
    "text": "Coefficient Interpretation: Binary Independent Variable\n\nIndependent variable \\(x\\) is a binary variable (\\(x\\) can take values: 0 or 1)\nWe are fitting the simple logistic regression model: \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=1)\\]\nThe logit difference is \\(\\beta_1\\) for binary independent variable\n\n\\(\\beta_1\\) represents the change/difference in the logit for \\(x=1\\) vs. \\(x=0\\)\n\nIt will be much easier to understand if we can interpret the coefficient using odds ratio (OR)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#binary-how-do-we-interpret-the-coefficient-i",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#binary-how-do-we-interpret-the-coefficient-i",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Binary: How do we interpret the coefficient? (I)",
    "text": "Binary: How do we interpret the coefficient? (I)\n\nFor individuals with \\(X=0\\): \\[\\text{logit}\\left(\\pi(X=0)\\right)=\\beta_0+\\beta_1\\times\\left(0\\right)=\\beta_0\\]\nFor individuals with \\(X=1\\): \\[\\text{logit}\\left(\\pi(X=1)\\right)=\\beta_0+\\beta_1\\times\\left(1\\right)=\\beta_0 + \\beta_1\\]\nTo solve for \\(\\beta_1\\), we take the difference of the logits: \\[ \\text{logit}\\left(\\pi(X=1)\\right) - \\text{logit}\\left(\\pi(X=0)\\right) = \\left( \\beta_0 + \\beta_1 \\right) - \\left( \\beta_0 \\right) = \\beta_1\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#binary-how-do-we-interpret-the-coefficient-ii",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#binary-how-do-we-interpret-the-coefficient-ii",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Binary: How do we interpret the coefficient? (II)",
    "text": "Binary: How do we interpret the coefficient? (II)\n\\[ \\text{logit}\\left(\\pi(X=1)\\right) - \\text{logit}\\left(\\pi(X=0)\\right) = \\left( \\beta_0 + \\beta_1 \\right) - \\left( \\beta_0 \\right) = \\beta_1\\]\n\\[\\begin{aligned}\n\\beta_1&=l\\mathrm{ogit}\\left(\\pi(X=1)\\right)\\ -l\\mathrm{ogit}\\left(\\pi\\left(X=0\\right)\\right) \\\\  \\beta_1&=l\\mathrm{og}\\left(\\dfrac{\\pi(X=1)}{1-\\pi(X=1)}\\right)-l\\mathrm{og}\\left(\\dfrac{\\pi\\left(X=0\\right)}{1-\\pi\\left(X=0\\right)}\\right) \\\\\n\\beta_1&=\\log{\\left(\\dfrac{\\dfrac{\\pi(X=1)}{1-\\pi(X=1)}}{\\dfrac{\\pi(X=0)}{1-\\pi(X=0)}}\\right)} \\\\ \\exp{\\left(\\beta_1\\right)}&=\\dfrac{\\dfrac{\\pi(X=1)}{1-\\pi(X=1)}}{\\dfrac{\\pi(X=0)}{1-\\pi(X=0)}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#review-of-odds-ratio",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#review-of-odds-ratio",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Review of Odds Ratio",
    "text": "Review of Odds Ratio\n\nOdds for a subject with \\(X=1\\): \\[\\text{odds}_1 = \\dfrac{\\pi(X=1)}{1-\\pi(X=1)}\\]\nOdds for a subject with \\(X=0\\): \\[\\text{odds}_0 = \\dfrac{\\pi(X=0)}{1-\\pi(X=0)}\\]\nOdds Ratio for \\(X=1\\) vs. \\(X=0\\): \\[OR = \\dfrac{\\dfrac{\\pi(X=1)}{1-\\pi(X=1)}}{\\dfrac{\\pi(X=0)}{1-\\pi(X=0)}}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-does-this-relate-to-a-2x2-table",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-does-this-relate-to-a-2x2-table",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How does this relate to a 2x2 table?",
    "text": "How does this relate to a 2x2 table?\n\n\n\n2x2 table with the respective logistic functions in each cell"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-does-this-relate-to-a-2x2-table-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-does-this-relate-to-a-2x2-table-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How does this relate to a 2x2 table?",
    "text": "How does this relate to a 2x2 table?\n\\[\nOR=\\dfrac{a/c}{b/d}=\\dfrac{\\dfrac{\\left(\\dfrac{\\exp{\\left(\\beta_0+\\beta_1\\right)}}{1+\\exp{\\left(\\beta_0+\\beta_1\\right)}}\\right)}{\\left(\\dfrac{1}{1+\\exp(\\beta_0+\\beta_1)}\\right)}}{\\dfrac{\\left(\\dfrac{\\exp(\\beta_0)}{1+\\exp(\\beta_0)}\\right)}{\\left(\\dfrac{1}{1+\\exp{\\left(\\beta_0\\right)}}\\right)}}=\\dfrac{\\exp(\\beta_0+\\beta_1)}{\\exp(\\beta_0)}=e^{\\beta_1}\n\\]\n\nSimple relationship between coefficient and odds ratio is a primary reason why we report OR for categorical data analysis.\nFor binary independent variable x, OR computed in logistic regression model is the same as OR computed using contingency table"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-binary-age-and-late-stage-diagnosis-i",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-binary-age-and-late-stage-diagnosis-i",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Binary age and Late Stage Diagnosis (I)",
    "text": "Example: Binary age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for older individuals (&gt;65 years old) compared to younger individuals (≤65 years old)?\n\n\n\nTwo options to calculate this value:\n\nOption 1: Calculate \\(\\widehat{OR}\\) from 2x2 contingency table\n\nRefer to Lesson 3 for this process\n\nOption 2: Calculate \\(\\widehat{OR}\\) from logistic regression\n\n\nNeeded steps for Option 2:\n\nFit the regression model\nTransform the coefficients into odds ratios\nInterpret the odds ratio"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-binary-age-and-late-stage-diagnosis-i-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-binary-age-and-late-stage-diagnosis-i-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Binary age and Late Stage Diagnosis (I)",
    "text": "Example: Binary age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for older individuals (&gt;65 years old) compared to younger individuals (≤65 years old)?\n\n\n\nFit the regression model\n\n\nbc3 = bc %&gt;% mutate(Age_binary = ifelse(Age &gt; 65, 1, 0))\nage_bin_glm = glm(Late_stage_diag ~ Age_binary, data = bc3, family = binomial)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-binary-age-and-late-stage-diagnosis-i-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-binary-age-and-late-stage-diagnosis-i-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Binary age and Late Stage Diagnosis (I)",
    "text": "Example: Binary age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for older individuals (&gt;65 years old) compared to younger individuals (≤65 years old)?\n\n\n\nTransform the coefficients into odds ratios\n\n\nage_bin_tidy = tidy(age_bin_glm, conf.int=T, exponentiate = T) \nage_bin_tidy %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.297\n0.031\n−39.608\n0.000\n0.280\n0.315\n    Age_binary\n1.875\n0.045\n13.928\n0.000\n1.716\n2.048"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-binary-age-and-late-stage-diagnosis-i-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-binary-age-and-late-stage-diagnosis-i-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Binary age and Late Stage Diagnosis (I)",
    "text": "Example: Binary age and Late Stage Diagnosis (I)\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for older individuals (&gt;65 years old) compared to younger individuals (≤65 years old)?\n\n\n\nInterpret the odds ratio\n\nThe estimated odds of late stage breast cancer among individuals over 65 years old is 1.87 (95% CI: (1.72, 2.05)) times that of individuals 65 years or younger."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-multi-group-categorical-variable",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-multi-group-categorical-variable",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Coefficient Interpretation: Multi-group Categorical Variable",
    "text": "Coefficient Interpretation: Multi-group Categorical Variable\n\nIndependent variable \\(x\\) is a multi-level categorical variable\nLet’s say \\(X\\) takes values: a, b, c, or d\nWe are fitting the simple logistic regression model: \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]\n\nWhere \\(a\\) is our reference group\n\nThe logit difference is \\(\\beta_1\\) for binary independent variable\n\n\\(\\beta_1\\) represents the change/difference in the logit for \\(x=b\\) vs. \\(x=a\\)\n\nIt will be much easier to understand if we can interpret the coefficient using odds ratio (OR)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-multi-group-categorical-variable-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-multi-group-categorical-variable-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Coefficient Interpretation: Multi-group Categorical Variable",
    "text": "Coefficient Interpretation: Multi-group Categorical Variable\nWe are fitting the simple logistic regression model with reference group \\(a\\): \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]\n\n\\(\\beta_0\\): the log-odds of event \\(Y=1\\) for group \\(a\\)\n\\(\\beta_1\\): the difference in log-odds of event \\(Y=1\\) comparing group \\(b\\) to group \\(a\\)\n\\(\\beta_2\\): the difference in log-odds of event \\(Y=1\\) comparing group \\(c\\) to group \\(a\\)\n\\(\\beta_3\\): the difference in log-odds of event \\(Y=1\\) comparing group \\(d\\) to group \\(a\\)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multi-level-categorical-how-do-we-interpret-the-coefficient-ii",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multi-level-categorical-how-do-we-interpret-the-coefficient-ii",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multi-level categorical: How do we interpret the coefficient? (II)",
    "text": "Multi-level categorical: How do we interpret the coefficient? (II)\n\\[ \\text{logit}\\left(\\pi(X=c)\\right) - \\text{logit}\\left(\\pi(X=a)\\right) = \\left( \\beta_0 + \\beta_1\\cdot 0 + \\beta_2\\cdot 1 + \\beta_3\\cdot 0  \\right) - \\left( \\beta_0 + \\beta_1\\cdot 0 + \\beta_2\\cdot 0 + \\beta_3\\cdot 0 \\right) = \\beta_2\\]\n\\[\\begin{aligned}\n\\beta_2&=l\\mathrm{ogit}\\left(\\pi(X=c)\\right)\\ -l\\mathrm{ogit}\\left(\\pi\\left(X=a\\right)\\right) \\\\  \\beta_2&=l\\mathrm{og}\\left(\\dfrac{\\pi(X=c)}{1-\\pi(X=c)}\\right)-l\\mathrm{og}\\left(\\dfrac{\\pi\\left(X=a\\right)}{1-\\pi\\left(X=a\\right)}\\right) \\\\\n\\beta_2&=\\log{\\left(\\dfrac{\\dfrac{\\pi(X=c)}{1-\\pi(X=c)}}{\\dfrac{\\pi(X=a)}{1-\\pi(X=a)}}\\right)} \\\\ \\exp{\\left(\\beta_2\\right)}&=\\dfrac{\\dfrac{\\pi(X=c)}{1-\\pi(X=c)}}{\\dfrac{\\pi(X=a)}{1-\\pi(X=a)}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-multi-group-categorical-variable-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#coefficient-interpretation-multi-group-categorical-variable-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Coefficient Interpretation: Multi-group Categorical Variable",
    "text": "Coefficient Interpretation: Multi-group Categorical Variable\nWe are fitting the simple logistic regression model with reference group \\(a\\): \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]\n\n\\(\\exp\\left(\\beta_0\\right)\\): the odds of event \\(Y=1\\) for group \\(a\\)\n\\(\\exp\\left(\\beta_1\\right)\\): the odds of event \\(Y=1\\) for group \\(b\\) is \\(\\exp\\left(\\beta_1\\right)\\) times the odds of event \\(Y=1\\) for group \\(a\\)\n\\(\\exp\\left(\\beta_2\\right)\\): the odds of event \\(Y=1\\) for group \\(c\\) is \\(\\exp\\left(\\beta_2\\right)\\) times the odds of event \\(Y=1\\) for group \\(a\\)\n\\(\\exp\\left(\\beta_3\\right)\\): the odds of event \\(Y=1\\) for group \\(d\\) is \\(\\exp\\left(\\beta_3\\right)\\) times the odds of event \\(Y=1\\) for group \\(a\\)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-do-we-pick-the-reference-group",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-do-we-pick-the-reference-group",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How do we pick the reference group?",
    "text": "How do we pick the reference group?\n\nThe choice can be more apparent for multi-group categorical independent variables within studies\nFor example, if we want to evaluate the association between clinical response and four treatments.\n\nThe treatment variable has 4 categories: “active treatment A”, “active treatment B”, “active treatment C” and “Placebo treatment”\nThe investigator is interested in comparing each of the three active treatment with the placebo treatment\nThen the placebo treatment should be picked as the reference group"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\n\nChose Non-Hispanic White individuals as reference group\nUnderlying health disparities linked to racism in healthcare and in clinical studies\nThere is evidence that white individuals receive a certain standard of care that is not paralleled for POC [@yedjou2019, @mateo2021]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islander individuals compared to Non-Hispanic White individuals?\n\n\nNeeded steps:\n\nFit the regression model\nTransform the coefficients into odds ratios\nInterpret the odds ratio"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islander individuals compared to Non-Hispanic White individuals?\n\n\n\nFit the regression model\n\n\nRE_glm = glm(Late_stage_diag ~ Race_Ethnicity, data = bc,  \n               family = binomial)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islander individuals compared to Non-Hispanic White individuals?\n\n\n\nTransform the coefficients into odds ratios\n\n\nRE_tidy = tidy(RE_glm, conf.int=T, exponentiate = T) \nRE_tidy %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.372\n0.026\n−37.553\n0.000\n0.353\n0.392\n    Race_EthnicityHispanic-Latino\n0.968\n0.082\n−0.398\n0.691\n0.822\n1.135\n    Race_EthnicityNH American Indian/Alaskan Native\n0.948\n0.476\n−0.111\n0.911\n0.342\n2.287\n    Race_EthnicityNH Asian/Pacific Islander\n1.131\n0.082\n1.497\n0.134\n0.961\n1.327\n    Race_EthnicityNH Black\n1.405\n0.070\n4.826\n0.000\n1.223\n1.611"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity-4",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-late-stage-diagnosis-and-race-and-ethnicity-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Late stage diagnosis and race and ethnicity",
    "text": "Example: Late stage diagnosis and race and ethnicity\n\n\nOdds ratio from logistic regression\n\n\nWhat is the odds ratio of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islander individuals compared to Non-Hispanic White individuals?\n\n\n\nInterpret the odds ratio\n\nThe estimated odds of late stage breast cancer among Non-Hispanic Asian/Pacific Islander individuals is 1.13 (95% CI: (0.96, 1.33)) times that of Non-Hispanic White individuals."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#what-if-you-want-to-compare-other-groups",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#what-if-you-want-to-compare-other-groups",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "What if you want to compare other groups?",
    "text": "What if you want to compare other groups?\n\nWhat if we want to estimate OR comparing Non-Hispanic Asian Pacific Islander to Non-Hispanic Black individuals?\nOption 1: Change reference group and refit the model (maybe the easiest option)\nOption 2: Estimate OR using fitted coefficients (\\(\\widehat{\\beta}\\)’s) in the current model: \\[\\begin{aligned} \\text{log}\\left( OR (\\text{NH API}, \\text{NH B}) \\right) &= \\text{logit}\\left(\\pi \\left(X = \\text{NH API}\\right)\\right) - \\text{logit}\\left(\\pi \\left(X = \\text{NH B}\\right)\\right) \\\\\n& = \\left[\\beta_0 + \\beta_3 \\cdot 1\\right] - \\left[\\beta_0 + \\beta_4 \\cdot 1 \\right] \\\\\n\\text{log}\\left( \\widehat{OR} (\\text{NH API}, \\text{NH B}) \\right) &= \\widehat{\\beta}_3 - \\widehat{\\beta}_4 \\\\\n\\widehat{OR} (\\text{NH API}, \\text{NH B}) &= \\exp \\left( \\widehat{\\beta}_3 - \\widehat{\\beta}_4 \\right)\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-5",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#poll-everywhere-question-5",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#what-if-you-want-to-compare-other-groups-option-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#what-if-you-want-to-compare-other-groups-option-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "What if you want to compare other groups? Option 1",
    "text": "What if you want to compare other groups? Option 1\n\nbc3 = bc %&gt;% \n  mutate(Race_Ethnicity = relevel(Race_Ethnicity, ref = \"NH Black\"))\nRE_glm2 = glm(Late_stage_diag ~ Race_Ethnicity, data = bc3, \n               family = binomial)\ntidy(RE_glm2, conf.int=T, exponentiate = T) %&gt;% gt() %&gt;%\n  tab_options(table.font.size = 38) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.523\n0.065\n−9.934\n0.000\n0.459\n0.594\n    Race_EthnicityNH White\n0.712\n0.070\n−4.826\n0.000\n0.621\n0.818\n    Race_EthnicityHispanic-Latino\n0.689\n0.102\n−3.664\n0.000\n0.564\n0.840\n    Race_EthnicityNH American Indian/Alaskan Native\n0.675\n0.479\n−0.819\n0.413\n0.242\n1.641\n    Race_EthnicityNH Asian/Pacific Islander\n0.805\n0.102\n−2.131\n0.033\n0.659\n0.982"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-table",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Table",
    "text": "How to present odds ratios: Table\n\ntbl_regression() in the gtsummary package is helpful for presenting the odds ratios in a clean way\n\n\nlibrary(gtsummary)\ntbl_regression(multi_bc, exponentiate = TRUE) %&gt;% \n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Race_Ethnicity\n\n\n\n        NH White\n—\n—\n\n        Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n        NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n        NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n        NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n    Age_c\n1.06\n1.05, 1.07\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nlibrary(broom.helpers)\nMLR_tidy = tidy_and_attach(multi_bc, conf.int=T, exponentiate = T) %&gt;%\n  tidy_remove_intercept() %&gt;%\n  tidy_add_reference_rows() %&gt;%\n  tidy_add_estimate_to_reference_rows() %&gt;%\n  tidy_add_term_labels()\nglimpse(MLR_tidy)\n\nRows: 6\nColumns: 16\n$ term           &lt;chr&gt; \"Race_EthnicityNH White\", \"Race_EthnicityHispanic-Latin…\n$ variable       &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_label      &lt;chr&gt; \"Race_Ethnicity\", \"Race_Ethnicity\", \"Race_Ethnicity\", \"…\n$ var_class      &lt;chr&gt; \"factor\", \"factor\", \"factor\", \"factor\", \"factor\", \"nume…\n$ var_type       &lt;chr&gt; \"categorical\", \"categorical\", \"categorical\", \"categoric…\n$ var_nlevels    &lt;int&gt; 5, 5, 5, 5, 5, NA\n$ contrasts      &lt;chr&gt; \"contr.treatment\", \"contr.treatment\", \"contr.treatment\"…\n$ contrasts_type &lt;chr&gt; \"treatment\", \"treatment\", \"treatment\", \"treatment\", \"tr…\n$ reference_row  &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, NA\n$ label          &lt;chr&gt; \"NH White\", \"Hispanic-Latino\", \"NH American Indian/Alas…\n$ estimate       &lt;dbl&gt; 1.0000000, 0.9846940, 0.9178662, 1.1433526, 1.4300256, …\n$ std.error      &lt;dbl&gt; NA, 0.083653090, 0.484110085, 0.083796726, 0.071788616,…\n$ statistic      &lt;dbl&gt; NA, -0.1843845, -0.1770333, 1.5986877, 4.9825778, 17.81…\n$ p.value        &lt;dbl&gt; NA, 8.537118e-01, 8.594822e-01, 1.098900e-01, 6.274274e…\n$ conf.low       &lt;dbl&gt; NA, 0.8344282, 0.3262638, 0.9688184, 1.2414629, 1.05221…\n$ conf.high      &lt;dbl&gt; NA, 1.158411, 2.254643, 1.345732, 1.645053, 1.065538"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot\n\nplot_MLR = ggplot(data=MLR_tidy, \n       aes(y=label, x=estimate, xmin=conf.low, xmax=conf.high)) + \n  geom_point(size = 3) +  geom_errorbarh(height=.2) + \n  \n  geom_vline(xintercept=1, color='#C2352F', linetype='dashed', alpha=1) +\n  theme_classic() +\n  \n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y', switch = \"y\") + \n  \n  labs(x = \"OR (95% CI)\", \n       title = \"Odds ratios of Late Stage Breast Cancer Diagnosis\") +\n  theme(axis.title = element_text(size = 25), \n        axis.text = element_text(size = 25), \n        title = element_text(size = 25), \n        axis.title.y=element_blank(), \n        strip.text = element_text(size = 25), \n        strip.placement = \"outside\", \n        strip.background = element_blank())"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#references",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#references",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "References",
    "text": "References\n\n\nLesson 10: Multiple Logistic Regression"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#last-class",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Last class",
    "text": "Last class\n\nLooked at simple logistic regression for binary outcome with\n\nOne continuous predictor \\[\\text{logit}(\\pi(X)) = \\beta_0 + \\beta_1 \\cdot X\\]\nOne binary predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=1)\\]\nOne multi-level predictor \\[\\text{logit}\\left(\\pi(X) \\right) = \\beta_0 + \\beta_1 \\cdot I(X=b) + \\beta_2 \\cdot I(X=c) + \\beta_3 \\cdot I(X=d)\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer example",
    "text": "Breast Cancer example\n\nFor breast cancer diagnosis example, recall:\n\nOutcome: early or late stage breast cancer diagnosis (binary, categorical)\n\n\n \n\nPrimary covariate: Race/ethnicity\n\nNon-Hispanic white individuals are more likely to be diagnosed with breast cancer\n\nBut POC are more likely to be diagnosed at a later stage\n\n\n\n \n\nAdditional covariate: Age\n\nRisk factor for cancer diagnosis\n\n\n \n\nWe want to fit a multiple logistic regression model with both risk factors included as independent variables"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#introduction-to-multiple-logistic-regression",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Introduction to Multiple Logistic Regression",
    "text": "Introduction to Multiple Logistic Regression\n\nIn multiple logistic regression model, we have &gt; 1 independent variable\n\nSometimes referred to as the “multivariable regression”\nThe independent variable can be any type:\n\nContinuous\nCategorical (ordinal or nominal)\n\n\n\n \n\nWe will follow similar procedures as we did for simple logistic regression\n\nBut we need to change our interpretation of estimates because we are adjusting for other variables"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multiple Logistic Regression Model",
    "text": "Multiple Logistic Regression Model\n\n\n\nAssume we have a collection of \\(k\\) independent variables, denoted by \\(\\mathbf{X}=\\left( X_1, X_2, ..., X_k \\right)\\)\n\n \n\nThe conditional probability is \\(P(Y=1 | \\mathbf{X}) = \\pi(\\mathbf{X})\\)\n\n \n\nWe then model the probability with logistic regression: \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0\n+\\beta_1 \\cdot X_1 +\\beta_2 \\cdot X_2  +\\beta_3 \\cdot X_3 + ... + \\beta_k \\cdot X_k\\]\n\n\n\n\n\n\nWhy the bold \\(X\\)?\n\n\n\\(\\mathbf{X}\\) represents the vector of all the \\(X\\)’s. This is how we represent our group of covariates in our model."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-the-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting the Multiple Logistic Regression Model",
    "text": "Fitting the Multiple Logistic Regression Model\n\nFor a multiple logistic regression model with \\(k\\) independent variables, the vector of coefficients can be denoted by \\[\\boldsymbol{\\beta}^{T} = \\left(\\beta_0, \\beta_1, \\beta_2, ..., \\beta_k \\right)\\]\n\n \n\nAs with the simple logistic regression, we use maximum likelihood method for estimating coefficients\n\nVector of estimated coefficients: \\[\\widehat{\\boldsymbol{\\beta}}^{T} = \\left(\\widehat{\\beta}_0, \\widehat{\\beta}_1, \\widehat{\\beta}_2, ..., \\widehat{\\beta}_k \\right)\\]\n\n\n \n\nFor a model with \\(k\\) independent variables, there is \\(k+1\\) coefficients to estimate\n\nUnless one of those independent variables is a multi-level categorical variables, then we need more than \\(k+1\\) coefficients"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-population-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Population Model",
    "text": "Breast Cancer Example: Population Model\n\nWe can fit a logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n \n\nNote that race and ethnicity requires 4 coefficients to include the indicator for each category\n\n \n\nCan replace \\(\\pi(\\mathbf{X})\\) with \\(\\pi({\\text{Race/ethnicity, Age}})\\)\n\n \n\n6 total coefficients (\\(\\beta_0\\) to \\(\\beta_5\\))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#fitting-multiple-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Fitting Multiple Logistic Regression Model",
    "text": "Fitting Multiple Logistic Regression Model\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nsummary(multi_bc)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Race_Ethnicity + Age_c, family = binomial, \n    data = bc)\n\nCoefficients:\n                                                 Estimate Std. Error z value\n(Intercept)                                     -1.038389   0.027292 -38.048\nRace_EthnicityHispanic-Latino                   -0.015424   0.083653  -0.184\nRace_EthnicityNH American Indian/Alaskan Native -0.085704   0.484110  -0.177\nRace_EthnicityNH Asian/Pacific Islander          0.133965   0.083797   1.599\nRace_EthnicityNH Black                           0.357692   0.071789   4.983\nAge_c                                            0.057151   0.003209  17.811\n                                                Pr(&gt;|z|)    \n(Intercept)                                      &lt; 2e-16 ***\nRace_EthnicityHispanic-Latino                      0.854    \nRace_EthnicityNH American Indian/Alaskan Native    0.859    \nRace_EthnicityNH Asian/Pacific Islander            0.110    \nRace_EthnicityNH Black                          6.27e-07 ***\nAge_c                                            &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11484  on 9994  degrees of freedom\nAIC: 11496\n\nNumber of Fisher Scoring iterations: 4"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#breast-cancer-example-fitted-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Breast Cancer Example: Fitted Model",
    "text": "Breast Cancer Example: Fitted Model\n\nWe now have the fitted logistic regression model with both race and ethnicity and age:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = & \\widehat{\\beta}_0\n+ \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right)\n+ \\widehat{\\beta}_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& + \\widehat{\\beta}_3 \\cdot I \\left( R/E = NH API \\right)\n+ \\widehat{\\beta}_4 \\cdot I \\left( R/E = NH B \\right)\n+ \\widehat{\\beta}_5 \\cdot Age \\\\\n\\\\\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\n\n6 total coefficients (\\(\\widehat{\\beta}_0\\) to \\(\\widehat{\\beta}_5\\))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#testing-significance-of-the-coefficients",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Testing Significance of the Coefficients",
    "text": "Testing Significance of the Coefficients\n\nRefer to Lesson 6 for more information on each test!!\n\n \n\nWe use the same three tests that we discussed in Simple Logistic Regression to test individual coefficients\n\nWald test\n\nCan be used to test a single coefficient\n\nScore test\nLikelihood ratio test (LRT)\n\nCan be used to test a single coefficient or multiple coefficients\n\n\n\n \n\nTextbook and our class focuses on Wald and LRT only"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#all-three-tests-together",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "All three tests together",
    "text": "All three tests together"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#a-note-on-wording",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "A note on wording",
    "text": "A note on wording\n\nWhen I say “test a single coefficient” or “test multiple coefficients” I am referring to the \\(\\beta\\)’s\n\nA single variable can have a single coefficient\n\nExample: testing age\n\nA single variable can have multiple coefficients\n\nExample: testing race and ethnicity\n\nMuliple variables will have multiple coefficients\n\nExample: testing age and race and ethnicity together\n\n\n \nWhen I say “test a variable” I mean “determine if the model with the variable is more likely than the model without that variable”\n\nWe can use the Wald test to do this is some scenarios (single, continuous covariate)\nBUT I advise you practice using the LRT whenever comparing models (aka testing variables)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-13",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-13",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test (1/3)",
    "text": "Wald test (1/3)\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n\nWe can only test the coefficient for age using the Wald test!"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-13",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-13",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (1/3)",
    "text": "Likelihood ratio test (1/3)\n\nLikelihood ratio test answers the question:\n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate or the model omitting the covariate?\nAka: Which model is more likely given our data: model including the covariate or the model omitting the covariate?\n\n\n \n\nTest a single coefficient by comparing different models\n\nVery similar to the F-test\n\n\n \n\nImportant: LRT can be used conduct hypothesis tests for multiple coefficients\n\nJust like F-test, we can test a single coefficient, continuous/binary covariate, multi-level covariate, or multiple covariates"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-13-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-13-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (1/3)",
    "text": "Likelihood ratio test (1/3)\n\nLikelihood ratio test answers the question:\n \n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate or the model omitting the covariate?\n\n \n\nFor a set of covariates, which model tell us more about the outcome variable: the model including the set of covariates or the model omitting the set of covariates?"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-23",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-23",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (2/3)",
    "text": "Likelihood ratio test (2/3)\n\nLikelihood ratio test answers the question:\n \n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate or the model omitting the covariate?\n\n \n\nFor a set of covariates, which model tell us more about the outcome variable: the model including the set of covariates or the model omitting the set of covariates?"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nIf testing single variable and it’s continuous or binary, still use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j = 0\\)\n\\(H_1\\): \\(\\beta_j \\neq 0\\)\n\n\n \n\nIf testing single variable and it’s categorical with mroe than 2 groups, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_j=\\beta_{j+1}=\\ldots=\\beta_{j+i-1}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0\n\n\n \n\nIf testing a set of variables, use this hypothesis test:\n\n\\(H_0\\): \\(\\beta_1=\\beta_{2}=\\ldots=\\beta_{k}=0\\)\n\\(H_1\\): at least one of the above \\(\\beta\\)’s is not equal to 0"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-procedure-with-confidence-intervals",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test procedure with confidence intervals",
    "text": "Wald test procedure with confidence intervals\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the confidence interval and determine if it overlaps with null\n\nOverlap with null (usually 0 for coefficient) = fail to reject null\nNo overlap with null (usually 0 for coefficient) = reject null\n\nWrite a conclusion to the hypothesis test\n\nWhat is the estimate and its confidence interval?\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#lrt-procedure",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "LRT procedure",
    "text": "LRT procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\nIn symbols\nIn words\nAlternative: one- or two-sided?\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-13-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-13-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test (1/3)",
    "text": "Wald test (1/3)\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n\nWe can only test the coefficient for age using the Wald test!"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test-33-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test (3/3)",
    "text": "Likelihood ratio test (3/3)\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n\nWe can test a single coefficient or multiple coefficients\n \n\nExample 1: Single, continuous variable: Age\n\n \n\nExample 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n \n\nExample 3: Set of variables: Race and Ethnicity, and Age"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the confidence interval and determine if it overlaps with null\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\)\n\\(H_1: \\beta_5 \\neq 0\\)"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#notes-on-likelihood-ratio-test",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#notes-on-likelihood-ratio-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Notes on Likelihood Ratio Test",
    "text": "Notes on Likelihood Ratio Test\n\nLikelihood ratio test is only suitable to test “nested” models\n“Nested” models means the bigger model (full model) contains all the independent variables of the smaller model (reduced model)\nWe cannot compare the following two models using LRT:\n\nModel 1: \\[ \\begin{aligned} \\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0 +\\beta_1 \\cdot I \\left( R/E = H/L \\right) +\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\ & +\\beta_3 \\cdot I \\left( R/E = NH API \\right) +\\beta_4 \\cdot I \\left( R/E = NH B \\right) \\end{aligned}\\]\nModel 2: \\[\\begin{aligned} \\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0+\\beta_1 \\cdot Age  \\end{aligned}\\]\n\nIf the two models to be compared are not nested, likelihood ratio test should not be used"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#estimatedpredicted-probability-for-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Estimated/Predicted Probability for MLR",
    "text": "Estimated/Predicted Probability for MLR\n\nBasic idea for predicting/estimating probability stays the same\n\n \n\nCalculations will be slightly different\n\nEspecially for the confidence interval\n\n\n \n\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicted-probability",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicted Probability",
    "text": "Predicted Probability\n\nWe may be interested in predicting probability of having a late stage breast cancer diagnosis for a specific age.\nThe predicted probability is the estimated probability of having the event for given values of covariate(s)\nRecall our fitted model for late stage breast cancer diagnosis: \\[ \\begin{aligned}\n\\text{logit}\\left(\\widehat{\\pi}(\\mathbf{X})\\right) = &-4.56\n-0.02 \\cdot I \\left( R/E = H/L \\right)\n-0.09 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +0.13 \\cdot I \\left( R/E = NH API \\right)\n+0.36 \\cdot I \\left( R/E = NH B \\right)\n+0.06 \\cdot Age\n\\end{aligned}\\]\nWe can convert it to the predicted probability: \\[\\hat{\\pi}(\\mathbf{X})=\\dfrac{\\exp \\left( \\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}  {1+\\exp \\left(\\widehat{\\beta}_0 + \\widehat{\\beta}_1 \\cdot I \\left( R/E = H/L \\right) + ... + \\widehat{\\beta}_5 \\cdot Age \\right)}\\]\n\nThis is an inverse logit calculation\n\nWe can calculate this using the the predict() function like in BSTA 512\n\nAnother option: taking inverse logit of fitted values from augment() function"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#prediction-and-confidence-interval-in-r",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#prediction-and-confidence-interval-in-r",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Prediction and Confidence Interval in R",
    "text": "Prediction and Confidence Interval in R"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-setup-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot Setup",
    "text": "How to present odds ratios: Forest Plot Setup\n\nMLR_tidy = MLR_tidy %&gt;%\n  mutate(var_label = case_match(var_label, \n                               \"Race_Ethnicity\" ~ \"Race and ethnicity\", \n                               \"Age_c\" ~ \"\"), \n         label = case_match(label, \n                            \"NH White\" ~ \"Non-Hispanic White\", \n                            \"Hispanic-Latino\" ~ \"Hispanic-Latinx\", \n                            \"NH American Indian/Alaskan Native\" ~ \"Non-Hispanic American \\n Indian/Alaskan Native\", \n                            \"NH Asian/Pacific Islander\" ~ \"Non-Hispanic \\n Asian/Pacific Islander\",\n                            \"NH Black\" ~ \"Non-Hispanic Black\", \n                               \"Age_c\" ~ \"Age (yrs)\"))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#how-to-present-odds-ratios-forest-plot-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "How to present odds ratios: Forest Plot",
    "text": "How to present odds ratios: Forest Plot"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#adding-odds-ratios",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Adding odds ratios",
    "text": "Adding odds ratios\n\nMLR_tidy = MLR_tidy %&gt;% \n  mutate(estimate_r = round(estimate, 2), \n         conf.low_r = round(conf.low, 2), \n         conf.high_r = round(conf.high, 2), \n         OR_char = paste0(estimate_r, \" (\", conf.low_r, \", \", conf.high_r, \")\"), \n         OR_char = ifelse(reference_row == F | is.na(reference_row), OR_char, NA))\n\n \n\n“Plot” of the text for odds ratios estimates\n\n\nOR_labs = ggplot(data=MLR_tidy, aes(y=label)) +\n  geom_text(aes(x = -1, label = OR_char), hjust = 0, size=8) +   \n  xlim(-1, 1) +\n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y') +\n  theme_void() + \n    theme(strip.text = element_blank())"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#section",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#section",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "",
    "text": "OR_labs = ggplot(data=MLR_tidy, aes(y=label)) +\n  geom_text(aes(x = -1, label = OR_char), hjust = 0, size=8) +   xlim(-1, 1) +\n  facet_grid(rows = vars(var_label), scales = \"free\",\n             space='free_y') +\n  theme_void() + \n    theme(strip.text = element_blank())"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#combine-them",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Combine them!!",
    "text": "Combine them!!\n\nlibrary(cowplot)\nplot_grid(plot_MLR, OR_labs, ncol=2, align = \"h\", rel_widths = c(4, 1))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#multivariable-logistic-regression-model",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Multivariable Logistic Regression Model",
    "text": "Multivariable Logistic Regression Model\n\nThe multivariable model of logistic regression (called multiple logistic regression) is useful in that it statistically adjusts the estimated effect of each variable in the model\n\n \n\nEach estimated coefficient provides an estimate of the log odds adjusting for all other variables included in the model\n \n\nThe adjusted odds ratio can be different from or similar to the unadjusted odds ratio\n\n \n\nComparing adjusted vs. unadjusted odds ratios can be a useful activity"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#interpretation-of-coefficients-in-mlr",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Interpretation of Coefficients in MLR",
    "text": "Interpretation of Coefficients in MLR\n\nThe interpretation of coefficients in multiple logistic regression is essentially the same as the interpretation of coefficients in simple logistic regression\n\n \n\nFor interpretation, we need to\n\npoint out that these are adjusted estimates\nprovide a list of other variables in the model"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Race and Ethnicity and Age model fit",
    "text": "Example: Race and Ethnicity and Age model fit\n\n\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Race_Ethnicity\n\n\n\n        NH White\n—\n—\n\n        Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n        NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n        NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n        NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n    Age_c\n1.06\n1.05, 1.07\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nThe estimated odds of late stage breast cancer diagnosis for Hispanic-Latinx individuals is 0.98 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.83, 1.16).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic American Indian/Alaskan Natives is 0.92 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.33, 2.25).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islanders is 1.14 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.97, 1.35).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Black individuals is 1.43 times that of Non-Hispanic White individuals, controlling for age (95% CI: 1.24, 1.65).\nFor every one year increase in age, there is an estimated 6% increase in the estimated odds of late stage breast cancer diagnosis (95% CI: 5%, 7%)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#what-does-controlling-for-mean-i",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#what-does-controlling-for-mean-i",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "What does “controlling for” mean? (I)",
    "text": "What does “controlling for” mean? (I)\n\nConsider a multivariable model with two independent variables: one dichotomous (the risk factor) and one continuous (say, age)\n\nPrimary interest: to estimate the effect of the risk factor on the outcome variable\nBut we want to assess whether to adjust for age\n\nAdjusting for age may not be necessary if age distribution is quite similar for the exposed and unexposed groups\nBut if the age distribution does differs for the two groups (for example, the exposed group are older than the unexposed group), we need to adjust for age using multivariable regression modeling\n\n\n\nLesson 10: Multiple Logistic Regression"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/old_work/Class_9_code.html",
    "href": "lectures/10_Multiple_logistic_regression/old_work/Class_9_code.html",
    "title": "Class 9 Code",
    "section": "",
    "text": "library(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(epiDisplay)\n\nLoading required package: foreign\n\n\nLoading required package: survival\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: nnet\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(here)\n\nhere() starts at /Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/S2024_BSTA_513_613/S2024_BSTA_513\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:epiDisplay':\n\n    alpha\n\nload(here(\"data\", \"bc_diagnosis.Rda\"))"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/old_work/Class_9_code.html#glow-study-example",
    "href": "lectures/10_Multiple_logistic_regression/old_work/Class_9_code.html#glow-study-example",
    "title": "Class 9 Code",
    "section": "GLOW study example",
    "text": "GLOW study example\n\nlibrary(aplore3)\nglow = glow500\n\n\nglow_m1 = glm(fracture ~ priorfrac, data = glow, family = binomial)\n\n\nglow_m2 = glm(fracture ~ priorfrac + age, data = glow, family = binomial)\n\n\nglow_m3 = glm(fracture ~ priorfrac + age + priorfrac*age, data = glow, family = binomial)\n\n\nsummary(glow_m1); summary(glow_m2); summary(glow_m3)\n\n\nCall:\nglm(formula = fracture ~ priorfrac, family = binomial, data = glow)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.4167     0.1305 -10.859  &lt; 2e-16 ***\npriorfracYes   1.0638     0.2231   4.769 1.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 540.07  on 498  degrees of freedom\nAIC: 544.07\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nCall:\nglm(formula = fracture ~ priorfrac + age, family = binomial, \n    data = glow)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -4.21429    0.84784  -4.971 6.67e-07 ***\npriorfracYes  0.83884    0.23416   3.582 0.000340 ***\nage           0.04119    0.01218   3.382 0.000719 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 528.52  on 497  degrees of freedom\nAIC: 534.52\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nCall:\nglm(formula = fracture ~ priorfrac + age + priorfrac * age, family = binomial, \n    data = glow)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -5.68942    1.08408  -5.248 1.54e-07 ***\npriorfracYes      4.96134    1.81022   2.741  0.00613 ** \nage               0.06251    0.01546   4.043 5.27e-05 ***\npriorfracYes:age -0.05738    0.02501  -2.294  0.02179 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 523.27  on 496  degrees of freedom\nAIC: 531.27\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nprior_age = expand_grid(priorfrac = c(\"No\", \"Yes\"), \n                        age = 55:90)\nnewdata = with(glow, data.frame(prior_age))\nfrac_pred = predict(glow_m3, newdata, se.fit = T, type=\"response\")\npred_glow = newdata %&gt;% mutate(frac_pred = frac_pred$fit)\nggplot(pred_glow) + #geom_point(aes(x = age, y = frac_pred, color = priorfrac)) +\n  geom_smooth(method = \"loess\", aes(x = age, y = frac_pred, color = priorfrac)) +\n  theme(legend.position = \"right\", \n        text = element_text(size=20), \n        title = element_text(size=16)) +\n  scale_color_discrete(name = \"Prior Fracture\") +\n  xlab(\"Age (years)\") + ylab(\"Predicted Probability of Fracture\") +\n  ylim(0,1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nprior_age = expand_grid(priorfrac = c(\"No\", \"Yes\"), \n                        age = 55:90)\nnewdata = with(glow, data.frame(prior_age))\nfrac_pred_log = predict(glow_m3, newdata, se.fit = T, type=\"link\")\npred_glow2 = newdata %&gt;% mutate(frac_pred_log = frac_pred_log$fit)\nggplot(pred_glow2) + #geom_point(aes(x = age, y = frac_pred, color = priorfrac)) +\n  geom_smooth(method = \"loess\", aes(x = age, y = frac_pred_log, color = priorfrac)) +\n  theme(legend.position = \"right\", \n        text = element_text(size=20), \n        title = element_text(size=16)) +\n  scale_color_discrete(name = \"Prior Fracture\") +\n  xlab(\"Age (years)\") + ylab(\"Log-Odds of Fracture\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nage = 55:90\nlogit_OR = glow_m3$coefficients[2] + glow_m3$coefficients[4]*age\nOR_data = data.frame(Age = age, OR = exp(logit_OR))\nggplot(OR_data) + \n  geom_hline(yintercept=1, color = \"#65A43D\", size = 1.5) +\n  geom_smooth(method = \"loess\", aes(x = Age, y = OR), color = \"black\", size = 2) +\n  theme(legend.position = \"right\", \n        text = element_text(size=20), \n        title = element_text(size=16)) +\n  xlab(\"Age (years)\") + ylab(\"Odds Ratio\") +\n  ylim(0, 10)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nInteraction model - Poll Everywhere\n\ninter_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age + Race_Ethnicity*Age, data = bc, \n               family = binomial())\nsummary(inter_bc)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Race_Ethnicity + Age + Race_Ethnicity * \n    Age, family = binomial(), data = bc)\n\nCoefficients:\n                                                     Estimate Std. Error\n(Intercept)                                         -4.540270   0.242635\nRace_EthnicityHispanic-Latino                        0.317162   0.733038\nRace_EthnicityNH American Indian/Alaskan Native     -4.104745   5.085605\nRace_EthnicityNH Asian/Pacific Islander              0.532264   0.719711\nRace_EthnicityNH Black                              -0.337456   0.657331\nAge                                                  0.056762   0.003806\nRace_EthnicityHispanic-Latino:Age                   -0.005279   0.011561\nRace_EthnicityNH American Indian/Alaskan Native:Age  0.062026   0.077179\nRace_EthnicityNH Asian/Pacific Islander:Age         -0.006314   0.011338\nRace_EthnicityNH Black:Age                           0.011035   0.010350\n                                                    z value Pr(&gt;|z|)    \n(Intercept)                                         -18.712   &lt;2e-16 ***\nRace_EthnicityHispanic-Latino                         0.433    0.665    \nRace_EthnicityNH American Indian/Alaskan Native      -0.807    0.420    \nRace_EthnicityNH Asian/Pacific Islander               0.740    0.460    \nRace_EthnicityNH Black                               -0.513    0.608    \nAge                                                  14.913   &lt;2e-16 ***\nRace_EthnicityHispanic-Latino:Age                    -0.457    0.648    \nRace_EthnicityNH American Indian/Alaskan Native:Age   0.804    0.422    \nRace_EthnicityNH Asian/Pacific Islander:Age          -0.557    0.578    \nRace_EthnicityNH Black:Age                            1.066    0.286    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11481  on 9990  degrees of freedom\nAIC: 11501\n\nNumber of Fisher Scoring iterations: 4\n\nwald_test = summary(inter_bc)$coefficients\nrownames(wald_test) = c(\"Intercept\", \"R_E: HL\", \"R_E: NH AIAN\",\n                        \"R_E: NH API\",\"R_E: NH B\", \"Age\", \n                        \"R_E: HL * Age\", \"R_E: NH AIAN * Age\",\n                        \"R_E: NH API * Age\",\"R_E: NH B * Age\")\nround(wald_test, 3)\n\n                   Estimate Std. Error z value Pr(&gt;|z|)\nIntercept            -4.540      0.243 -18.712    0.000\nR_E: HL               0.317      0.733   0.433    0.665\nR_E: NH AIAN         -4.105      5.086  -0.807    0.420\nR_E: NH API           0.532      0.720   0.740    0.460\nR_E: NH B            -0.337      0.657  -0.513    0.608\nAge                   0.057      0.004  14.913    0.000\nR_E: HL * Age        -0.005      0.012  -0.457    0.648\nR_E: NH AIAN * Age    0.062      0.077   0.804    0.422\nR_E: NH API * Age    -0.006      0.011  -0.557    0.578\nR_E: NH B * Age       0.011      0.010   1.066    0.286"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#from-lesson-6-wald-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "From Lesson 6: Wald test",
    "text": "From Lesson 6: Wald test\n\nAssumes test statistic W follows a standard normal distribution under the null hypothesis\nTest statistic: \\[W=\\frac{{\\hat{\\beta}}_j}{SE_{\\hat{\\beta}_j}}\\sim N(0,1)\\]\n\nwhere \\(\\widehat{\\beta}_j\\) is a MLE of coefficient \\(j\\)\n\n95% Wald confidence interval: \\[{\\hat{\\beta}}_1\\pm1.96 \\cdot SE_{{\\hat{\\beta}}_j}\\]\nThe Wald test is a routine output in R (summary() of glm() output)\n\nIncludes \\(SE_{{\\hat{\\beta}}_j}\\) and can easily find confidence interval with tidy()\n\nImportant note: Wald test is best for confidence intervals of our coefficient estimates or estimated odds ratios."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#wald-test-in-our-example",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Wald test in our example",
    "text": "Wald test in our example\n\nFrom our multiple logistic regression model:\n\n\\[ \\begin{aligned}\n\\text{logit}\\left(\\pi(x_i)\\right) = & \\beta_0\n+\\beta_1 \\cdot I \\left( R/E = H/L \\right)\n+\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\\n& +\\beta_3 \\cdot I \\left( R/E = NH API \\right)\n+\\beta_4 \\cdot I \\left( R/E = NH B \\right)\n+\\beta_5 \\cdot Age\n\\end{aligned}\\]\n \n\nWald test can help us construct the confidence interval for ALL coefficient estimates\n\n \n\nIf we want to use the Wald test to determine if a covariate is significant in our model\n\nCan only do so for age"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntidy(multi_bc, conf.int=T) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 28) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n\n\n  \n    \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.04\n0.03\n−38.05\n0.00\n−1.09\n−0.99\n    Race_EthnicityHispanic-Latino\n−0.02\n0.08\n−0.18\n0.85\n−0.18\n0.15\n    Race_EthnicityNH American Indian/Alaskan Native\n−0.09\n0.48\n−0.18\n0.86\n−1.12\n0.81\n    Race_EthnicityNH Asian/Pacific Islander\n0.13\n0.08\n1.60\n0.11\n−0.03\n0.30\n    Race_EthnicityNH Black\n0.36\n0.07\n4.98\n0.00\n0.22\n0.50\n    Age_c\n0.06\n0.00\n17.81\n0.00\n0.05\n0.06"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the confidence interval and determine if it overlaps with null\n\n\nLook at coefficient estimates (CI should not contain 0) OR estimated odds ratio (CI should not contain 1)\n\n\n\n\ntbl_regression(multi_bc, \n               exponentiate = TRUE) %&gt;%\n  as_gt() %&gt;% \n  tab_options(table.font.size = 30)\n\n\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Race_Ethnicity\n\n\n\n        NH White\n—\n—\n\n        Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n        NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n        NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n        NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n    Age_c\n1.06\n1.05, 1.07\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; 0.001)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-5",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\nNeeded steps:\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#likelihood-ratio-test",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Likelihood ratio test",
    "text": "Likelihood ratio test\n\nLikelihood ratio test answers the question:\n\nFor a specific covariate, which model tell us more about the outcome variable: the model including the covariate (or set of covariates) or the model omitting the covariate (or set of covariates)?\nAka: Which model is more likely given our data: model including the covariate or the model omitting the covariate?\n\n\n \n\nTest a single coefficient by comparing different models\n\nVery similar to the F-test\n\n\n \n\nImportant: LRT can be used conduct hypothesis tests for multiple coefficients\n\nJust like F-test, we can test a single coefficient, continuous/binary covariate, multi-level covariate, or multiple covariates"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#reminder-on-nested-models",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Reminder on nested models",
    "text": "Reminder on nested models\n\nLikelihood ratio test is only suitable to test “nested” models\n“Nested” models means the bigger model (full model) contains all the independent variables of the smaller model (reduced model)\nWe cannot compare the following two models using LRT:\n\nModel 1: \\[ \\begin{aligned} \\text{logit}\\left(\\pi(\\mathbf{X})\\right) = & \\beta_0 +\\beta_1 \\cdot I \\left( R/E = H/L \\right) +\\beta_2 \\cdot I \\left( R/E = NH AIAN \\right) \\\\ & +\\beta_3 \\cdot I \\left( R/E = NH API \\right) +\\beta_4 \\cdot I \\left( R/E = NH B \\right) \\end{aligned}\\]\nModel 2: \\[\\begin{aligned} \\text{logit}\\left(\\pi(Age)\\right) = & \\beta_0+\\beta_1 \\cdot Age  \\end{aligned}\\]\n\nIf the two models to be compared are not nested, likelihood ratio test should not be used"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-6",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_5 = 0\\) or model without age is more likely\n\\(H_1: \\beta_5 \\neq 0\\) or model with age is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-7",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nre_bc = glm(Late_stage_diag ~ Race_Ethnicity, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, re_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Race_Ethnicity\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   5 -5918.1 -1 352.63  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-1-single-continuous-variable-age-8",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 1: Single, continuous variable: Age",
    "text": "Example 1: Single, continuous variable: Age\n\n\nSingle, continuous variable: Age\n\n\nGiven race and ethnicity is already in the model, is the regression model with age more likely than the model without age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven race and ethnicity is already in the model, the regression model with age is more likely than the model without age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = 0\\) or model without race and ethnicity is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nage_bc = glm(Late_stage_diag ~ Age_c, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, age_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ Age_c\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   2 -5754.8 -4 26.053  3.087e-05 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-2-single-2-categorical-variable-race-and-ethnicity-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 2: Single, >2 categorical variable: Race and Ethnicity",
    "text": "Example 2: Single, &gt;2 categorical variable: Race and Ethnicity\n\n\nSingle, &gt;2 categorical variable: Race and Ethnicity\n\n\nGiven age is already in the model, is the regression model with race and ethnicity more likely than the model without race and ethnicity?\n\n\n\nWrite a conclusion to the hypothesis test\n\nGiven age is already in the model, the regression model with race and ethnicity is more likely than the model without race and ethnicity (p-val = \\(3.1\\cdot10^{-5}\\) &lt; 0.05)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nSet the level of significance \\(\\alpha\\)\n\n\\(\\alpha = 0.05\\)\n\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses\n\n\\(H_0: \\beta_1 = \\beta_2 = \\beta_3 = \\beta_4 = \\beta_5 = 0\\) or model without race and ethnicity and age is more likely\n$H_1: at least one \\(\\beta\\) is not 0 or model with race and ethnicity and age is more likely"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nCalculate the test statistic and p-value\n\n\nmulti_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age_c, data = bc, family = binomial)\nintercept_bc = glm(Late_stage_diag ~ 1, data = bc, family = binomial)\n\nlmtest::lrtest(multi_bc, intercept_bc)\n\nLikelihood ratio test\n\nModel 1: Late_stage_diag ~ Race_Ethnicity + Age_c\nModel 2: Late_stage_diag ~ 1\n  #Df  LogLik Df  Chisq Pr(&gt;Chisq)    \n1   6 -5741.8                         \n2   1 -5930.5 -5 377.32  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-3-set-of-variables-race-and-ethnicity-and-age-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example 3: Set of variables: Race and Ethnicity, and Age",
    "text": "Example 3: Set of variables: Race and Ethnicity, and Age\n\n\nSet of variables: Race and Ethnicity, and Age\n\n\nIs the regression model with race and ethnicity and age more likely than the model without race and ethnicity nor age?\n\n\n\nWrite a conclusion to the hypothesis test\n\nThe regression model with race and ethnicity and age is more likely than the model omitting race and ethnicity and age (p-val &lt; \\(2.2\\cdot10^{-16}\\))."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\nNeeded steps:\n\nCalculate probability prediction\nCheck if we can use Normal approximation\nCalculate confidence interval\n\nUsing logit scale then converting\nUsing Normal approximation\n\nInterpret results"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-1",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCalculate probability prediction\n\n\nnewdata = data.frame(Age_c = 60 - mean_age, \n                     Race_Ethnicity = \"NH Asian/Pacific Islander\")\npred1 = predict(multi_bc, newdata, se.fit = T, type=\"response\")\npred1\n\n$fit\n        1 \n0.2685667 \n\n$se.fit\n         1 \n0.01572695 \n\n$residual.scale\n[1] 1"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-2",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nCheck if we can use Normal approximation\n\nWe can use the Normal approximation if: \\(\\widehat{p}n = \\widehat{\\pi}(X)\\cdot n &gt; 10\\) and \\((1-\\widehat{p})n = (1-\\widehat{\\pi}(X))\\cdot n &gt; 10\\).\n\nn = nobs(multi_bc)\np = pred1$fit\nn*p\n\n       1 \n2685.667 \n\nn*(1-p)\n\n       1 \n7314.333 \n\n\nWe can use the Normal approximation!"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-3",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n3b. Calculate confidence interval (Option 2: with Normal approximation)\n\npred = predict(multi_bc, newdata, se.fit = T, type = \"response\")\n\nLL_CI = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\nc(Pred = pred$fit, LL_CI, UL_CI) %&gt;% round(digits=3)\n\nPred.1      1      1 \n 0.269  0.238  0.299"
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#predicting-probability-in-r-4",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Predicting probability in R",
    "text": "Predicting probability in R\n\n\nPredicting probability of late stage breast cancer diagnosis\n\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, what is the predicted probability for late stage breast cancer diagnosis (with confidence intervals)?\n\n\n\nInterpret results\n\nFor someone who is 60 years old and Non-Hispanic Asian/Pacific Islander, the predicted probability of late stage breast cancer diagnosis is 0.269 (95% CI: 0.238, 0.299)."
  },
  {
    "objectID": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "href": "lectures/10_Multiple_logistic_regression/10_Multiple_logistic_regression.html#example-race-and-ethnicity-and-age-model-fit-fixed",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "Example: Race and Ethnicity and Age model fit (FIXED)",
    "text": "Example: Race and Ethnicity and Age model fit (FIXED)\n\n\n\n\n\n\n\n\n  \n    \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    Race_Ethnicity\n\n\n\n        NH White\n—\n—\n\n        Hispanic-Latino\n0.98\n0.83, 1.16\n0.9\n        NH American Indian/Alaskan Native\n0.92\n0.33, 2.25\n0.9\n        NH Asian/Pacific Islander\n1.14\n0.97, 1.35\n0.11\n        NH Black\n1.43\n1.24, 1.65\n&lt;0.001\n    Age_c\n1.06\n1.05, 1.07\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\n\nThe estimated odds of late stage breast cancer diagnosis for Hispanic-Latinx individuals is 0.98 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.83, 1.16).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic American Indian/Alaskan Natives is 0.92 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.33, 2.25).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Asian/Pacific Islanders is 1.14 times that of Non-Hispanic White individuals, controlling for age (95% CI: 0.97, 1.35).\nThe estimated odds of late stage breast cancer diagnosis for Non-Hispanic Black individuals is 1.43 times that of Non-Hispanic White individuals, controlling for age (95% CI: 1.24, 1.65).\nFor every one year increase in age, there is an 6% increase in the estimated odds of late stage breast cancer diagnosis, adjusting for race and ethnicity (95% CI: 5%, 7%)."
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#what-is-a-confounder",
    "href": "lectures/11_Interactions/11_Interactions.html#what-is-a-confounder",
    "title": "Lesson 10: Multiple Logistic Regression",
    "section": "What is a confounder?",
    "text": "What is a confounder?\n\nA confounding variable, or confounder, is a factor/variable that wholly or partially accounts for the observed effect of the risk factor on the outcome\nA confounder must be…\n\nRelated to the outcome Y, but not a consequence of Y\nRelated to the explanatory variable X, but not a consequence of X\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLesson 10: Multiple Logistic Regression"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#review-of-interactions-and-confounding-from-bsta-512",
    "href": "lectures/11_Interactions/11_Interactions.html#review-of-interactions-and-confounding-from-bsta-512",
    "title": "Lesson 11: Interactions",
    "section": "Review of interactions and confounding from BSTA 512",
    "text": "Review of interactions and confounding from BSTA 512"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#revisit-from-what-is-a-confounder",
    "href": "lectures/11_Interactions/11_Interactions.html#revisit-from-what-is-a-confounder",
    "title": "Lesson 11: Interactions",
    "section": "Revisit from : What is a confounder?",
    "text": "Revisit from : What is a confounder?\n\nA confounding variable, or confounder, is a factor/variable that wholly or partially accounts for the observed effect of the risk factor on the outcome\nA confounder must be…\n\nRelated to the outcome Y, but not a consequence of Y\nRelated to the explanatory variable X, but not a consequence of X"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#including-a-confounder-in-the-model",
    "href": "lectures/11_Interactions/11_Interactions.html#including-a-confounder-in-the-model",
    "title": "Lesson 11: Interactions",
    "section": "Including a confounder in the model",
    "text": "Including a confounder in the model\n\nIn the following model we have two variables, \\(X_1\\) and \\(X_2\\)\n\n\\[Y= \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} + \\epsilon\\]\n\nAnd we assume that every level of the confounder, there is parallel slopes\nNote: to interpret \\(\\beta_1\\), we did not specify any value of \\(X_2\\); only specified that it be held constant\n\nImplicit assumption: effect of \\(X_1\\) is equal across all values of \\(X_2\\)\n\nThe above model assumes that \\(X_{1}\\) and \\(X_{2}\\) do not interact (with respect to their effect on \\(Y\\))\n\nepidemiology: no “effect modification”\nmeaning the effect of \\(X_{1}\\) is the same regardless of the values of \\(X_{2}\\)"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#where-have-we-modeled-a-confounder-before",
    "href": "lectures/11_Interactions/11_Interactions.html#where-have-we-modeled-a-confounder-before",
    "title": "Lesson 11: Interactions",
    "section": "Where have we modeled a confounder before?",
    "text": "Where have we modeled a confounder before?\n\n\n\nWe have seen a plot of Life expectancy vs. female literacy rate with different levels of food supply colored (Lesson 8)\nIn our plot and the model, we treat food supply as a confounder\nIf food supply is a confounder in the relationship between life expectancy and female literacy rate, then we only use main effects in the model:\n\n\\[\\text{LE} = \\beta_0 + \\beta_1 \\text{FLR} + \\beta_2 \\text{FS} + \\epsilon\\]"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-1",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-1",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#what-is-an-effect-modifier",
    "href": "lectures/11_Interactions/11_Interactions.html#what-is-an-effect-modifier",
    "title": "Lesson 11: Interactions",
    "section": "What is an effect modifier?",
    "text": "What is an effect modifier?\n\n\n\nAn additional variable in the model\n\nOutside of the main relationship between \\(Y\\) and \\(X_1\\) that we are studying\n\nAn effect modifier will change the effect of \\(X_1\\) on \\(Y\\) depending on its value\n\nAka: as the effect modifier’s values change, so does the association between \\(Y\\) and \\(X_1\\)\nSo the coefficient estimating the relationship between \\(Y\\) and \\(X_1\\) changes with another variable"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#how-do-we-include-an-effect-modifier-in-the-model",
    "href": "lectures/11_Interactions/11_Interactions.html#how-do-we-include-an-effect-modifier-in-the-model",
    "title": "Lesson 11: Interactions",
    "section": "How do we include an effect modifier in the model?",
    "text": "How do we include an effect modifier in the model?\n\nInteractions!!\nWe can incorporate interactions into our model through product terms: \\[Y  =  \\beta_0 + \\beta_1X_{1}+ \\beta_2X_{2} +\n\\beta_3X_{1}X_{2} + \\epsilon\\]\nTerminology:\n\nmain effect parameters: \\(\\beta_1,\\beta_2\\)\n\nThe main effect models estimate the average \\(X_{1}\\) and \\(X_{2}\\) effects\n\ninteraction parameter: \\(\\beta_3\\)"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#types-of-interactions-non-interactions",
    "href": "lectures/11_Interactions/11_Interactions.html#types-of-interactions-non-interactions",
    "title": "Lesson 11: Interactions",
    "section": "Types of interactions / non-interactions",
    "text": "Types of interactions / non-interactions\nNo interaction and three potential effects of interaction between two covariates A and B:\n\nNo interaction between A and B (confounder with no interaction)\n\n \n\nUnilateralism: exposure to A has no effect in the absence of exposure to B, but a considerable effect when B is present.\n\n \n\nSynergism: the effect of A is in the same direction, but stronger in the presence of B.\n\n \n\nAntagonism: the effect of A works in the opposite direction in the presence of B."
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#deciding-between-confounder-and-effect-modifier",
    "href": "lectures/11_Interactions/11_Interactions.html#deciding-between-confounder-and-effect-modifier",
    "title": "Lesson 11: Interactions",
    "section": "Deciding between confounder and effect modifier",
    "text": "Deciding between confounder and effect modifier\n\nThis is more of a model selection question (in coming lectures)\nBut if we had a model with only TWO covariates, we could step through the following process:\n\nTest the interaction (of potential effect modifier): use a partial F-test to test if interaction term(s) explain enough variation compared to model without interaction\n\nRecall that for two continuous covariates, we will test a single coefficient\nFor a binary and continuous covariate, we will test a single coefficient\nFor two binary categorical covariates, we will test a single coefficient\nFor a multi-level categorical covariate (with any other type of covariate), we must test a group of coefficients!!\n\nThen look at the main effect (or potential confounder)\n\nIf interaction already included, then automatically included as main effect (and thus not checked for confounding)\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#reminder-from-lesson-9-general-steps-for-f-test",
    "href": "lectures/11_Interactions/11_Interactions.html#reminder-from-lesson-9-general-steps-for-f-test",
    "title": "Lesson 11: Interactions",
    "section": "Reminder from Lesson 9: General steps for F-test",
    "text": "Reminder from Lesson 9: General steps for F-test\n\n\n\n\n\nMet underlying LINE assumptions\n\n\n\n\n\n\nState the null hypothesis\n\n\n\n\\[\\begin{align}\nH_0 &: \\beta_1=\\beta_2= \\ldots=\\beta_k=0\\\\\n\\text{vs. } H_A&: \\text{At least one } \\beta_j\\neq0, \\text{for }j=1, 2, \\ldots, k\n\\end{align}\\]\n\n\n\nSpecify the significance level.\n\n\n\nOften we use \\(\\alpha = 0.05\\)\n\n\n\nSpecify the test statistic and its distribution under the null\n\n\n\nThe test statistic is \\(F\\), and follows an F-distribution with numerator \\(df=k\\) and denominator \\(df=n-k-1\\). (\\(n\\) = # obversation, \\(k\\) = # covariates)\n\n\n\n\n\n\nCompute the value of the test statistic\n\n\n\nThe calculated test statistic is\n\\[F^ = \\dfrac{\\frac{SSE(R) - SSE(F)}{df_R - df_F}}{\\frac{SSE(F)}{df_F}} = \\frac{MSR_{full}}{MSE_{full}}\\]\n\n\n\nCalculate the p-value\n\n\n\nWe are generally calculating: \\(P(F_{k, n-k-1} &gt; F)\\)\n\n\n\nWrite conclusion for hypothesis test\n\n\n\nWe (reject/fail to reject) the null hypothesis at the \\(100\\alpha\\%\\) significance level."
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#step-1-testing-the-interaction",
    "href": "lectures/11_Interactions/11_Interactions.html#step-1-testing-the-interaction",
    "title": "Lesson 11: Interactions",
    "section": "Step 1: Testing the interaction",
    "text": "Step 1: Testing the interaction\n\nWe test with \\(\\alpha = 0.10\\)\nFollow the LRT procedure in Lesson 6, slide 38\nUse the hypothesis tests for the specific covariate combo:\n\n\n\n\n\nBinary & continuous variable\n\n\nTesting a single coefficient for the interaction term using LRT comparing full model (with interaction) to reduced model (without interaction)\n\n\n\n\n\nBinary & continuous variables\n\n\nTesting a group of coefficients for the interaction term using LRT comparing full model (with interaction) to reduced model (without interaction)\n\n\n\n\n\n\n\n\nBinary & multi-level variable\n\n\nTesting a group of coefficients for the interaction term using LRT comparing full model (with interaction) to reduced model (without interaction)\n\n\n\n\n\nTwo continuous variables\n\n\nTesting a single coefficient for the interaction term using LRT comparing full model (with interaction) to reduced model (without interaction)"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-questions-2-4",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-questions-2-4",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Questions 2-4",
    "text": "Poll Everywhere Questions 2-4"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#step-2-testing-a-confounder",
    "href": "lectures/11_Interactions/11_Interactions.html#step-2-testing-a-confounder",
    "title": "Lesson 11: Interactions",
    "section": "Step 2: Testing a confounder",
    "text": "Step 2: Testing a confounder\n\nIf interaction already included:\n\nMeaning: LRT showed evidence for alternative/full model\nThen the variable is an effect modifier and we don’t need to consider it as a confounder\nThen automatically included as main effect (and thus not checked for confounding)\n\nFor variables that are not included in any interactions:\n\nCheck to see if they are confounders\nOne way to do this is by seeing whether exclusion of the variable changes any of the main effect of the primary explanatory variable by more than 10%\n\nIf the main effect of the primary explanatory variable changes by less than 10%, then the additional variable is neither an effect modifier nor a confounder\n\nWe leave the variable out of the model"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#testing-for-percent-change-delta-in-a-coefficient",
    "href": "lectures/11_Interactions/11_Interactions.html#testing-for-percent-change-delta-in-a-coefficient",
    "title": "Lesson 11: Interactions",
    "section": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient",
    "text": "Testing for percent change ( \\(\\Delta\\%\\)) in a coefficient\n\nLet’s say we have \\(X_1\\) and \\(X_2\\), and we specifically want to see if \\(X_2\\) is a confounder for \\(X_1\\) (the explanatory variable or variable of interest)\nIf we are only considering \\(X_1\\) and \\(X_2\\), then we need to run the following two models:\n\nFitted model 1 / reduced model (mod1):  \\(\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) = \\widehat\\beta_0 + \\widehat\\beta_1X_1\\)\n\nWe call the above \\(\\widehat\\beta_1\\) the reduced model coefficient: \\(\\widehat\\beta_{1, \\text{mod1}}\\) or \\(\\widehat\\beta_{1, \\text{red}}\\)\n\nFitted model 2 / full model (mod2):  \\(\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) = \\widehat\\beta_0 + \\widehat\\beta_1X_1 +\\widehat\\beta_2X_2\\)\n\nWe call this \\(\\widehat\\beta_1\\) the full model coefficient: \\(\\widehat\\beta_{1, \\text{mod2}}\\) or \\(\\widehat\\beta_{1, \\text{full}}\\)\n\n\n\n\n\n\n\n\n\nCalculation for % change in coefficient\n\n\n\\[\n\\Delta\\% = 100\\% \\cdot\\frac{\\widehat\\beta_{1, \\text{mod1}} - \\widehat\\beta_{1, \\text{mod2}}}{\\widehat\\beta_{1, \\text{mod2}}} = 100\\% \\cdot \\frac{\\widehat\\beta_{1, \\text{red}} - \\widehat\\beta_{1, \\text{full}}}{\\widehat\\beta_{1, \\text{full}}}\n\\]"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#confounding-vs.-interaction",
    "href": "lectures/11_Interactions/11_Interactions.html#confounding-vs.-interaction",
    "title": "Lesson 11: Interactions",
    "section": "Confounding vs. Interaction",
    "text": "Confounding vs. Interaction\n\n\n\nConfounders: The adjusted odds ratio for one variable adjusting for confounders can be quite different from unadjusted odds ratio\n\nAdjusting for them is called controlling for confounding.\n\n\n \n\nInteractions: When odds ratio for one variable is not constant over the levels of another variable, the two variables are said to have a statistical interaction (sometimes also called effect modification)\n\ni.e.: the log odds of one variable is modified/changed with different values of the other variable\nA variable is an effect modifier if it interacts with a risk factor\n\n\n\n\n\n\n\nNote\n\n\nPlease refer to Lesson 11 from BSTA 512/612 – lots of information about these concepts!"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-of-interaction",
    "href": "lectures/11_Interactions/11_Interactions.html#example-of-interaction",
    "title": "Lesson 11: Interactions",
    "section": "Example of interaction",
    "text": "Example of interaction\n\n\n\nIn a cohort study of elderly people the chance of death (outcome) within 2 years was much higher for those who had previously suffered a hip fracture at the start of these 2 years, but the excess risk associated with a hip fracture was significantly higher for males vs. females\n\n \n\nThis is an interaction between hip fracture status (yes/no) and sex (unclear if assigned at birth or no)\n\n \n\nOdds ratio for females &gt; odds ratio for males"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#types-of-interactions-non-interactions-1",
    "href": "lectures/11_Interactions/11_Interactions.html#types-of-interactions-non-interactions-1",
    "title": "Lesson 11: Interactions",
    "section": "Types of interactions / non-interactions",
    "text": "Types of interactions / non-interactions"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#understand-the-interaction-13",
    "href": "lectures/11_Interactions/11_Interactions.html#understand-the-interaction-13",
    "title": "Lesson 11: Interactions",
    "section": "Understand the interaction (1/3)",
    "text": "Understand the interaction (1/3)\n\n\n\nFigure plots the logits (log-odds) under three different models showing the presence and absence of interaction.\n\n \n\nResponse variable: CHD\n\n \n\nRisk factor: sex\n\n \n\nCovariate to be controlled: age"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#understand-the-interaction-23",
    "href": "lectures/11_Interactions/11_Interactions.html#understand-the-interaction-23",
    "title": "Lesson 11: Interactions",
    "section": "Understand the interaction (2/3)",
    "text": "Understand the interaction (2/3)\n\n\n\nIf age does not interact with sex, the distance between \\(l_2\\) and \\(l_1\\) is the log odds ratio for sex, controlling for age (\\(l_2 - l_1\\)) stays the same for all values of age.\n\n \n\nIf age interacts with sex, the distance between \\(l_3\\) and \\(l_1\\) is the log odds ratio for sex, controlling for age.\n\nAge values need to be specified because (\\(l_3 - l_1\\)) differs for different values of age.\nMust specify age when reporting odds ratio comparing sex"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#understand-the-interaction-33",
    "href": "lectures/11_Interactions/11_Interactions.html#understand-the-interaction-33",
    "title": "Lesson 11: Interactions",
    "section": "Understand the interaction (3/3)",
    "text": "Understand the interaction (3/3)\n\n\n\nIn the real world, it is rare to see two completely parallel logit plots as we see \\(l_2\\) and \\(l_1\\)\n\nBut we need to determine if the difference between \\(l_2\\) and \\(l_3\\) is important in the model\n\n\n \n\nWe may not want to include the interaction term unless it is statistically significant and/or clinically meaningful\n\n \n\nLikelihood ratio test (or Wald test) may be used to test the significance of coefficients for variables of the interaction term"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#summary",
    "href": "lectures/11_Interactions/11_Interactions.html#summary",
    "title": "Lesson 11: Interactions",
    "section": "Summary",
    "text": "Summary\n\nIn a logistic model with two covariates : \\(X_1\\) (the risk factor, a binary variable) and \\(X_2\\) (potential confounder/effect modifier)\nThe role of \\(X_2\\) can be one of the three possibilities:\n\nNot a confounder nor effect modifier, and not significantly associated with Y\n\nNo need to include \\(X_2\\) in the model (for your dataset)\nMay still be nice to include if other literature in the field includes it\n\n\n \n\nIt is a confounder but not an effect modifier. There is statistical adjustment but no statistical interaction\n\nShould include \\(X_2\\) in the model as main effect\n\n\n \n\nIt is an effect modifier. There is statistical interaction.\n\nShould include \\(X_2\\) in the model as main effect and interaction term"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study",
    "text": "Example: GLOW Study\n\nFrom GLOW (Global Longitudinal Study of Osteoporosis in Women) study\n\n \n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\n\n \n\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\n\n \n\nPotential confounder or effect modifier: age (AGE, a continuous variable)\n\nCenter age will be used! We will center around the rounded mean age of 69 years old\n\n\n\nlibrary(aplore3)\nmean_age = mean(glow500$age) %&gt;% round()\nglow = glow500 %&gt;% mutate(age_c = age - mean_age)"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-1",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-1",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study",
    "text": "Example: GLOW Study\nWe also could jump right into model fitting (connecting to the three possible roles of Age):\n\nModel 1:  Age not included\n\n\\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF})\\]\n \n\nModel 2:  Age as main effect (age as potential confounder)\n\n\\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF}) +\\beta_2\\cdot Age\\]\n \n\nModel 3:  Age and Prior Fracture interaction (age as potential effect modifier)\n\n\\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF}) +\\beta_2\\cdot Age + \\beta_3 \\cdot I(\\text{PF}) \\cdot Age\\]"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-2",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-2",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study",
    "text": "Example: GLOW Study\nWe also could jump right into model fitting (connecting to the three possible roles of Age):\n\nModel 1:  Age not included\n\n\nglow_m1 = glm(fracture ~ priorfrac, \n              data = glow, family = binomial)\n\n \n\nModel 2:  Age as main effect (age as potential confounder)\n\n\nglow_m2 = glm(fracture ~ priorfrac + age_c, \n              data = glow, family = binomial)\n\n \n\nModel 3:  Age and Prior Fracture interaction (age as potential effect modifier)\n\n\nglow_m3 = glm(fracture ~ priorfrac + age_c + priorfrac*age_c, \n              data = glow, family = binomial)"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-3",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-3",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study",
    "text": "Example: GLOW Study\n\nAge is an effect modifier of prior fracture\n\n \n\nWhen a covariate is an effect modifier, its status as a confounder is of secondary importance since the estimate of the effect of the risk factor depends on the specific value of the covariate\n\n \n\nMust summarize the effect of prior fracture on current fracture by age\n\nCannot summarize as a single (log) odds ratio"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-4",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-4",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study",
    "text": "Example: GLOW Study\n\nIdentify two sets of values that you want to compare with only one variable changed\n\nSet 1: \\(PF = 1, Age = a\\)\nSet 2: \\(PF = 0, Age = a\\)\n\n\n \n\nSubstitute values in the fitted log-odds model\n\n\\[\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) = \\widehat\\beta_0 + \\widehat\\beta_1\\cdot I(\\text{PF}) + \\widehat\\beta_2\\cdot Age + \\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age\\] \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(PF = 1, Age = a)\\right) =& \\widehat\\beta_0 + \\widehat\\beta_1\\cdot 1 + \\widehat\\beta_2\\cdot a + \\widehat\\beta_3 \\cdot 1\\cdot Age \\\\\n=& \\widehat\\beta_0 + \\widehat\\beta_1 + \\widehat\\beta_2\\cdot a + \\widehat\\beta_3\\cdot a\n\\end{aligned}\\]\n\\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(PF = 0, Age = a)\\right) =& \\widehat\\beta_0 + \\widehat\\beta_1\\cdot 0 + \\widehat\\beta_2\\cdot a + \\widehat\\beta_3 \\cdot 0\\cdot Age \\\\\n=& \\widehat\\beta_0 + \\widehat\\beta_2\\cdot a\n\\end{aligned}\\]"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-5",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-5",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study",
    "text": "Example: GLOW Study\n\nTake the difference of the two log-odds\n\n\\[\\begin{aligned}\n\\left[logit\\left(\\pi\\left(PF=1,\\ Age=a\\right)\\right)\\right]- & \\left[logit\\left(\\pi\\left(PF=0,\\ Age=a\\right)\\right)\\right]\n\\\\ & =\\left[{\\hat{\\beta}}_0+{\\hat{\\beta}}_1+{\\hat{\\beta}}_2a+{\\hat{\\beta}}_3a\\right]-\\left[{\\hat{\\beta}}_0+{\\hat{\\beta}}_2a\\right]\\\\ &={\\hat{\\beta}}_1+{\\hat{\\beta}}_3a\n\\end{aligned}\\]\n\nExponentiate the resulting difference\n\n\\[\\widehat{OR}\\left[\\left(PF=1,\\ Age=a\\right),\\left(PF=0,\\ Age=a\\right)\\right]=exp\\ \\left({\\hat{\\beta}}_1+{\\hat{\\beta}}_3a\\right)\\]"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-6",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-6",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study",
    "text": "Example: GLOW Study\n\n\nLesson 11: Interactions"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-7",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-7",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study",
    "text": "Example: GLOW Study\n\n\nLesson 11: Interactions"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#plot-the-proportions",
    "href": "lectures/11_Interactions/11_Interactions.html#plot-the-proportions",
    "title": "Lesson 11: Interactions",
    "section": "Plot the proportions",
    "text": "Plot the proportions\n\nggplot(data = glow2, aes(y = freq, x = age, color = priorfrac)) +\n  geom_point() + ylim(0, 1) + geom_smooth(se = F) +\n  labs(x = \"Age (years)\", y = \"Proportion of fracture\", \n       color = \"Prior fracture\", title = \"Sample proportion of fracture by age and prior fracture\") +\n  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 18), \n        title = element_text(size = 18), legend.text=element_text(size=18))"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-calculate-the-proportions",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-calculate-the-proportions",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study: Calculate the proportions",
    "text": "Example: GLOW Study: Calculate the proportions\n\nglow2 = glow %&gt;% \n  group_by(age, priorfrac, fracture) %&gt;% # last one needs to be outcome\n  summarise(n = n()) %&gt;%\n  mutate(freq = n / sum(n)) %&gt;% # takes the proportion of yes/no\n  filter(fracture == \"Yes\") # Filtering so only \"success\" shown\n\n`summarise()` has grouped output by 'age', 'priorfrac'. You can override using\nthe `.groups` argument.\n\n  #filter(freq != 1 | n != 1) \n\nhead(glow2)\n\n# A tibble: 6 × 5\n# Groups:   age, priorfrac [6]\n    age priorfrac fracture     n  freq\n  &lt;int&gt; &lt;fct&gt;     &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1    56 No        Yes          3 0.15 \n2    56 Yes       Yes          1 1    \n3    57 No        Yes          3 0.158\n4    57 Yes       Yes          1 0.333\n5    58 No        Yes          3 0.15 \n6    59 Yes       Yes          2 0.667"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-plot-the-proportions",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-plot-the-proportions",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study: Plot the proportions",
    "text": "Example: GLOW Study: Plot the proportions\n\nggplot(data = glow2, aes(y = freq, x = age, color = priorfrac)) +\n  geom_point() + ylim(0, 1) + geom_smooth(se = F) +\n  labs(x = \"Age (years)\", y = \"Proportion of fracture\", \n       color = \"Prior fracture\", title = \"Sample proportion of fracture by age and prior fracture\") +\n  theme(axis.title = element_text(size = 18), axis.text = element_text(size = 18), \n        title = element_text(size = 18), legend.text=element_text(size=18))\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nFrom sample proportions, looks like age and prior fracture may have an interaction!"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-try-to-visual-the-sample-proportions",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-try-to-visual-the-sample-proportions",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study: Try to visual the sample proportions",
    "text": "Example: GLOW Study: Try to visual the sample proportions\n\nBack in BSTA 512/612, we could visual the data to get a sense if there was an interaction before fitting a model\n\n \n\nWith a binary outcome, this is a little harder\n\nWe could use a contingency table or plot proportions of the outcome\nHard to do this when our potential confounder or effect modifier is continuous"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-age-an-effect-modifier-or-confounder",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-age-an-effect-modifier-or-confounder",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study: Age an effect modifier or confounder?",
    "text": "Example: GLOW Study: Age an effect modifier or confounder?\n\n\n\nModel 1:  Age not included\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.417\n0.130\n−10.859\n0.000\n−1.679\n−1.167\n    priorfracYes\n1.064\n0.223\n4.769\n0.000\n0.626\n1.502\n  \n  \n  \n\n\n\n\n\nModel 2:  Age as main effect (age as potential confounder)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.372\n0.132\n−10.407\n0.000\n−1.637\n−1.120\n    priorfracYes\n0.839\n0.234\n3.582\n0.000\n0.378\n1.297\n    age_c\n0.041\n0.012\n3.382\n0.001\n0.017\n0.065\n  \n  \n  \n\n\n\n\n\nModel 3:  Age and Prior Fracture interaction (age as potential effect modifier)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.376\n0.134\n−10.270\n0.000\n−1.646\n−1.120\n    priorfracYes\n1.002\n0.240\n4.184\n0.000\n0.530\n1.471\n    age_c\n0.063\n0.015\n4.043\n0.000\n0.032\n0.093\n    priorfracYes:age_c\n−0.057\n0.025\n−2.294\n0.022\n−0.107\n−0.008"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-age-an-effect-modifier-or-confounder-1",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-age-an-effect-modifier-or-confounder-1",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study: Age an effect modifier or confounder?",
    "text": "Example: GLOW Study: Age an effect modifier or confounder?\n\n\n\nModel 1:  Age not included\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.417\n0.130\n−10.859\n0.000\n−1.679\n−1.167\n    priorfracYes\n1.064\n0.223\n4.769\n0.000\n0.626\n1.502\n  \n  \n  \n\n\n\n\n\nModel 2:  Age as main effect (age as potential confounder)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.372\n0.132\n−10.407\n0.000\n−1.637\n−1.120\n    priorfracYes\n0.839\n0.234\n3.582\n0.000\n0.378\n1.297\n    age_c\n0.041\n0.012\n3.382\n0.001\n0.017\n0.065\n  \n  \n  \n\n\n\n\n\nModel 3:  Age and Prior Fracture interaction (age as potential effect modifier)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.376\n0.134\n−10.270\n0.000\n−1.646\n−1.120\n    priorfracYes\n1.002\n0.240\n4.184\n0.000\n0.530\n1.471\n    age_c\n0.063\n0.015\n4.043\n0.000\n0.032\n0.093\n    priorfracYes:age_c\n−0.057\n0.025\n−2.294\n0.022\n−0.107\n−0.008\n  \n  \n  \n\n\n\n\n\n\nIs age an effect modifier?\n\nTest the significance of the interaction term in Model 3\nWe can use the Wald test or LRT\n\n\n \n\nIf not an effect modifier, check the change in coefficient for prior fracture between Model 1 and Model 2"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-study-age-an-effect-modifier-or-confounder-2",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-study-age-an-effect-modifier-or-confounder-2",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW Study: Age an effect modifier or confounder?",
    "text": "Example: GLOW Study: Age an effect modifier or confounder?\n\n\n\nModel 1:  Age not included\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.417\n0.130\n−10.859\n0.000\n−1.679\n−1.167\n    priorfracYes\n1.064\n0.223\n4.769\n0.000\n0.626\n1.502\n  \n  \n  \n\n\n\n\n\nModel 2:  Age as main effect (age as potential confounder)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.372\n0.132\n−10.407\n0.000\n−1.637\n−1.120\n    priorfracYes\n0.839\n0.234\n3.582\n0.000\n0.378\n1.297\n    age_c\n0.041\n0.012\n3.382\n0.001\n0.017\n0.065\n  \n  \n  \n\n\n\n\n\nModel 3:  Age and Prior Fracture interaction (age as potential effect modifier)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.376\n0.134\n−10.270\n0.000\n−1.646\n−1.120\n    priorfracYes\n1.002\n0.240\n4.184\n0.000\n0.530\n1.471\n    age_c\n0.063\n0.015\n4.043\n0.000\n0.032\n0.093\n    priorfracYes:age_c\n−0.057\n0.025\n−2.294\n0.022\n−0.107\n−0.008\n  \n  \n  \n\n\n\n\n\n\nIs age an effect modifier?\n\nTest the significance of the interaction term in Model 3\nWe can use the Wald test or LRT\n\n\n \n\nIf not an effect modifier, check the change in coefficient for prior fracture between Model 1 and Model 2\n\n \n\nShort version of testing the interaction: Wald statistic for the interaction coefficient, \\(\\widehat{\\beta}_3\\), is statistically significant with \\(p= 0.022\\). Thus, there is evidence of a statistical interaction between these age and prior fracture."
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#please-please-please-reference-your-work-from-bsta-512612",
    "href": "lectures/11_Interactions/11_Interactions.html#please-please-please-reference-your-work-from-bsta-512612",
    "title": "Lesson 11: Interactions",
    "section": "Please please please reference your work from BSTA 512/612",
    "text": "Please please please reference your work from BSTA 512/612\n\nWe had lessons and homeworks dedicated to this process!\nThe process will be the same!\n\nOnly differences are t-test and F-test are replaced by Wald test and Likelihood ratio test, respectively!!"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#section",
    "href": "lectures/11_Interactions/11_Interactions.html#section",
    "title": "Lesson 11: Interactions",
    "section": "",
    "text": "prior_age = expand_grid(priorfrac = c(\"No\", \"Yes\"), \n                        age = 55:90)\nnewdata = with(glow, data.frame(prior_age))\nfrac_pred = predict(glow_m3, newdata, se.fit = T, type=\"response\")\npred_glow = newdata %&gt;% mutate(frac_pred = frac_pred$fit)\nggplot(pred_glow) + #geom_point(aes(x = age, y = frac_pred, color = priorfrac)) +\n  geom_smooth(method = \"loess\", aes(x = age, y = frac_pred, color = priorfrac)) +\n  theme(legend.position = \"right\", \n        text = element_text(size=20), \n        title = element_text(size=16)) +\n  scale_color_discrete(name = \"Prior Fracture\") +\n  xlab(\"Age (years)\") + ylab(\"Predicted Probability of Fracture\") +\n  ylim(0,1)"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-interaction-interpretation",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-interaction-interpretation",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW – Interaction interpretation",
    "text": "Example: GLOW – Interaction interpretation\n\nModel 3:  \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.376\n0.134\n−10.270\n0.000\n−1.646\n−1.120\n    priorfracYes\n1.002\n0.240\n4.184\n0.000\n0.530\n1.471\n    age_c\n0.063\n0.015\n4.043\n0.000\n0.032\n0.093\n    priorfracYes:age_c\n−0.057\n0.025\n−2.294\n0.022\n−0.107\n−0.008"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#example-glow-interaction-interpretation-1",
    "href": "lectures/11_Interactions/11_Interactions.html#example-glow-interaction-interpretation-1",
    "title": "Lesson 11: Interactions",
    "section": "Example: GLOW – Interaction interpretation",
    "text": "Example: GLOW – Interaction interpretation\n\n\n\nModel 3 estimated coefficients:\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.376\n0.000\n−1.646\n−1.120\n    priorfracYes\n1.002\n0.000\n0.530\n1.471\n    age_c\n0.063\n0.000\n0.032\n0.093\n    priorfracYes:age_c\n−0.057\n0.022\n−0.107\n−0.008\n  \n  \n  \n\n\n\n\n\nModel 3 estimated odds ratios:\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.25\n0.00\n0.19\n0.33\n    priorfracYes\n2.72\n0.00\n1.70\n4.35\n    age_c\n1.06\n0.00\n1.03\n1.10\n    priorfracYes:age_c\n0.94\n0.02\n0.90\n0.99\n  \n  \n  \n\n\n\n\n\n\n\\(\\widehat\\beta_3 = -0.057\\)\nThe effect of having a prior fracture on the log odds of having a new fracture decreases by an estimated 0.057 for every one year increase in age (95% CI: 0.008, 0.107).\n\nAka the log odds of a new fracture comparing prior fracture to no prior fracture gets closer to one another as age increases\n\n\\(\\widehat\\beta_1 = 1.002\\)\nFor individuals 69 years old, the estimated difference in log odds for a new fracture is 1.002 comparing individuals with a prior fracture to individuals with no prior fracture (95% CI: 0.530, 1.471).\n\\(\\exp(\\widehat\\beta_1) = 2.72\\)\n\nFor individuals 69 years old, the estimated odds of a new fracture for individuals with prior fracture is 2.72 times the estimated odds of a new fracture for individuals with no prior fracture (95% CI: 1.70, 4.35)."
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-5",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-5",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#plot-the-predicted-probability-of-fracture",
    "href": "lectures/11_Interactions/11_Interactions.html#plot-the-predicted-probability-of-fracture",
    "title": "Lesson 11: Interactions",
    "section": "Plot the predicted probability of fracture",
    "text": "Plot the predicted probability of fracture\n\nfrac_pred = predict(glow_m3, prior_age, se.fit = T, type=\"response\")\npred_glow = prior_age %&gt;% mutate(frac_pred = frac_pred$fit, \n                                  age = age_c + mean_age)\n\nggplot(pred_glow) + #geom_point(aes(x = age, y = frac_pred, color = priorfrac)) +\n  geom_smooth(method = \"loess\", aes(x = age, y = frac_pred, color = priorfrac)) +\n  theme(text = element_text(size=20), title = element_text(size=16)) + ylim(0,1) +\n  labs(color = \"Prior Fracture\", x = \"Age (years)\", y = \"Predicted Probability of Fracture\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#plot-of-estimated-log-odds",
    "href": "lectures/11_Interactions/11_Interactions.html#plot-of-estimated-log-odds",
    "title": "Lesson 11: Interactions",
    "section": "Plot of estimated log odds",
    "text": "Plot of estimated log odds\n\nprior_age = expand_grid(priorfrac = c(\"No\", \"Yes\"), age_c = (55:90)-69)\nfrac_pred_log = predict(glow_m3, prior_age, se.fit = T, type=\"link\")\npred_glow2 = prior_age %&gt;% mutate(frac_pred_log = frac_pred_log$fit, \n                                  age = age_c + mean_age)\n\nggplot(pred_glow2) + #geom_point(aes(x = age, y = frac_pred, color = priorfrac)) +\n  geom_smooth(method = \"loess\", aes(x = age, y = frac_pred_log, color = priorfrac)) +\n  theme(text = element_text(size=20), title = element_text(size=16)) +\n  labs(color = \"Prior Fracture\", x = \"Age (years)\", y = \"Log-Odds of Fracture\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#whats-the-best-way-to-report-on-the-interaction",
    "href": "lectures/11_Interactions/11_Interactions.html#whats-the-best-way-to-report-on-the-interaction",
    "title": "Lesson 11: Interactions",
    "section": "What’s the best way to report on the interaction?",
    "text": "What’s the best way to report on the interaction?"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-i",
    "href": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-i",
    "title": "Lesson 11: Interactions",
    "section": "Odds Ratio in the Presence of Interaction (I)",
    "text": "Odds Ratio in the Presence of Interaction (I)\n\nWhen interaction exists between a risk factor (F) and another variable (X), the estimate of the odds ratio for F depends on the value of X\n\n \n\nWhen an interaction term (F*X) exists in the model\n\n\\({\\widehat{OR}}_F \\neq \\exp(\\widehat\\beta_F)\\) in general\n\n\n \n\nAssume we want to compute the odds ratio for (\\(F = f_1\\) and \\(F = f_0\\), the correct model-based estimate is \\[\\begin{aligned}\n{\\widehat{OR}}_F = &\\exp{\\left(\\hat{g}\\left(F=f_1,X=x\\right)-\\hat{g}\\left(F=f_0,X=x\\right)\\right)}\n\\end{aligned}\\]\n\nLet’s work this out on the next slide!"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question",
    "text": "Poll Everywhere Question"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-1-1",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-1-1",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-2",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-2",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-3",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-3",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-4",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-4",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-6-bonus-q-if-were-feeling-it",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-6-bonus-q-if-were-feeling-it",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 6 (Bonus q if we’re feeling it)",
    "text": "Poll Everywhere Question 6 (Bonus q if we’re feeling it)"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-ii",
    "href": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-ii",
    "title": "Lesson 11: Interactions",
    "section": "Odds Ratio in the Presence of Interaction (II)",
    "text": "Odds Ratio in the Presence of Interaction (II)\n\nWe may write the two logits (log-odds) for given x as below:\n\n\\[ \\begin{aligned}\n\\hat{g}\\left(F = f_1,X = x\\right)&={\\hat{\\beta}}_0+{\\hat{\\beta}}_1f_1+{\\hat{\\beta}}_2x+{\\hat{\\beta}}_3f_1\\cdot\\ x \\\\\n\\hat{g}\\left(F = f_0,X=x\\right)&={\\hat{\\beta}}_0+{\\hat{\\beta}}_1f_0+{\\hat{\\beta}}_2x+{\\hat{\\beta}}_3f_0\\cdot\\ x\n\\end{aligned}\\]\n\nThe difference in two logits (log-odds) is:\n\n\\[ \\begin{aligned}\n\\hat{g}\\left(f_1,x\\right)-\\hat{g}\\left(f_0,x\\right) & = \\left[{\\hat{\\beta}}_0+{\\hat{\\beta}}_1f_1+{\\hat{\\beta}}_2x+{\\hat{\\beta}}_3f_1\\cdot\\ x \\right] - \\left[{\\hat{\\beta}}_0+{\\hat{\\beta}}_1f_0+{\\hat{\\beta}}_2x+{\\hat{\\beta}}_3f_0\\cdot\\ x \\right] \\\\\n\\hat{g}\\left(f_1,x\\right)-\\hat{g}\\left(f_0,x\\right) & = \\hat{\\beta}_1 f_1- \\hat{\\beta}_1f_0 + \\hat{\\beta}_3 x f_1 - \\hat\\beta_3 x f_0\\\\\n\\hat{g}\\left(f_1,x\\right)-\\hat{g}\\left(f_0,x\\right) & = {\\hat{\\beta}}_1\\left(f_1-f_0\\right)+{\\hat{\\beta}}_3x\\left(f_1-f_0\\right)\n\\end{aligned}\\]\n\nTherefore,\n\n\\[\\widehat{OR}(F=f-1, F=f_0, X=x) = \\widehat{OR}_F = \\exp\\left[ {\\hat{\\beta}}_1\\left(f_1-f_0\\right)+{\\hat{\\beta}}_3x\\left(f_1-f_0\\right) \\right]\\]"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-ii-1",
    "href": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-ii-1",
    "title": "Lesson 11: Interactions",
    "section": "Odds Ratio in the Presence of Interaction (II)",
    "text": "Odds Ratio in the Presence of Interaction (II)\n\n\nLesson 11: Interactions"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#steps-to-compute-or-under-interation",
    "href": "lectures/11_Interactions/11_Interactions.html#steps-to-compute-or-under-interation",
    "title": "Lesson 11: Interactions",
    "section": "Steps to compute OR under interation",
    "text": "Steps to compute OR under interation\n\nNote: You don’t need to know the math itself, but I think it’s helpful to think of it this way\n\n\nIdentify two sets of values that you want to compare with only one variable changed\n\nIn previous slides, one set was \\((F=f_1, X=x)\\) and the other was \\((F=f_0, X=x)\\)\n\nSubstitute values in the fitted log-odds model\n\nYou should have two equations, one for each set of values\n\nTake the difference of the two log-odds\nExponentiate the resulting difference"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#we-can-put-in-values-for-age-to-see-how-the-or-changes",
    "href": "lectures/11_Interactions/11_Interactions.html#we-can-put-in-values-for-age-to-see-how-the-or-changes",
    "title": "Lesson 11: Interactions",
    "section": "We can put in values for age to see how the OR changes",
    "text": "We can put in values for age to see how the OR changes\n\nIf we let \\(a = 60\\), i.e., compute OR for age = 60, then \\[{\\widehat{OR}}_{a=60}=\\exp(1.002-0.057\\cdot(60-69))\\ =4.55\\]\nIf we let \\(a = 70\\), i.e., compute OR for age = 70, then \\[{\\widehat{OR}}_{a=70}=\\exp(1.002-0.057\\cdot(70-69))\\ =2.57\\]"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#table-of-the-odds-ratio-for-an-interaction",
    "href": "lectures/11_Interactions/11_Interactions.html#table-of-the-odds-ratio-for-an-interaction",
    "title": "Lesson 11: Interactions",
    "section": "Table of the odds ratio for an interaction",
    "text": "Table of the odds ratio for an interaction"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#plotting-the-odds-ratio-for-an-interaction",
    "href": "lectures/11_Interactions/11_Interactions.html#plotting-the-odds-ratio-for-an-interaction",
    "title": "Lesson 11: Interactions",
    "section": "Plotting the odds ratio for an interaction",
    "text": "Plotting the odds ratio for an interaction\n\nggplot(pred_glow2) +\n  geom_hline(yintercept = 1) +\n  geom_smooth(method = \"loess\", aes(x = age, y = OR_YN)) +\n  theme(text = element_text(size=20), title = element_text(size=16)) +\n  labs(x = \"Age (years)\", y = \"Estimated odds ratio\", title = \"Odds ratio of fracture outcome comparing \\n prior fracture to no prior fracture\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#how-would-i-report-these-results",
    "href": "lectures/11_Interactions/11_Interactions.html#how-would-i-report-these-results",
    "title": "Lesson 11: Interactions",
    "section": "How would I report these results?",
    "text": "How would I report these results?\n\nRemember our main covariate is prior fracture, so we want to focuse on how age changes the relationship between prior fracture and a new fracture!\n\n\n\nFor individuals 69 years old, the estimated odds of a new fracture for individuals with prior fracture is 2.72 times the estimated odds of a new fracture for individuals with no prior fracture (95% CI: 1.70, 4.35). As seen in Figure 1 (a), the odds ratio of a new fracture when comparing prior fracture status decreases with age, indicating that the effect of prior fractures on new fractures decreases as individuals get older. In Figure 1 (b), it is evident that for both prior fracture statuses, the predict probability of a new fracture increases as age increases. However, the predicted probability of new fracture for those without a prior fracture increases at a higher rate than that of individuals with a prior fracture. Thus, the predicted probabilities of a new fracture converge at age [insert age here].\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n(a) Odds ratio of fracture outcome comparing prior fracture to no prior fracture\n\n\n\n\n\n\n\n(b) Predicted probability of fracture\n\n\n\n\nFigure 1: Plots of odds ratio and predicted probability from fitted interaction model"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#plotting-the-odds-ratio-for-an-interaction-1",
    "href": "lectures/11_Interactions/11_Interactions.html#plotting-the-odds-ratio-for-an-interaction-1",
    "title": "Lesson 11: Interactions",
    "section": "Plotting the odds ratio for an interaction",
    "text": "Plotting the odds ratio for an interaction\n\nggplot(pred_glow2) +\n  geom_hline(yintercept = 1) +\n  geom_smooth(method = \"loess\", aes(x = age, y = OR_YN)) +\n  theme(text = element_text(size=20), title = element_text(size=16)) +\n  labs(x = \"Age (years)\", y = \"Odds Ratio\", title = \"Odds ratio of fracture outcome comparing \\n prior fracture to no prior fracture\")"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#calculate-odds-ratios-across-values",
    "href": "lectures/11_Interactions/11_Interactions.html#calculate-odds-ratios-across-values",
    "title": "Lesson 11: Interactions",
    "section": "Calculate odds ratios across values",
    "text": "Calculate odds ratios across values\n\nprior_age = expand_grid(priorfrac = c(\"No\", \"Yes\"), age_c = (55:90)-69)\nfrac_pred_logit = predict(glow_m3, prior_age, se.fit = T, type=\"link\")\npred_glow2 = prior_age %&gt;% mutate(frac_pred = frac_pred_logit$fit, \n                                  age = age_c + mean_age) %&gt;%\n\n                pivot_wider(names_from = priorfrac, values_from = frac_pred) %&gt;%\n\n                mutate(OR_YN = exp(Yes - No))\nhead(pred_glow2)\n\n# A tibble: 6 × 5\n  age_c   age    No    Yes OR_YN\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n1   -14    55 -2.25 -0.446  6.08\n2   -13    56 -2.19 -0.441  5.74\n3   -12    57 -2.13 -0.436  5.42\n4   -11    58 -2.06 -0.430  5.12\n5   -10    59 -2.00 -0.425  4.83\n6    -9    60 -1.94 -0.420  4.56"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#revisit-from-512-what-is-a-confounder",
    "href": "lectures/11_Interactions/11_Interactions.html#revisit-from-512-what-is-a-confounder",
    "title": "Lesson 11: Interactions",
    "section": "Revisit from 512: What is a confounder?",
    "text": "Revisit from 512: What is a confounder?\n\nA confounding variable, or confounder, is a factor/variable that wholly or partially accounts for the observed effect of the risk factor on the outcome\nA confounder must be…\n\nRelated to the outcome Y, but not a consequence of Y\nRelated to the explanatory variable X, but not a consequence of X"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-questions-2",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-questions-2",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Questions 2",
    "text": "Poll Everywhere Questions 2"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-2-1",
    "href": "lectures/11_Interactions/11_Interactions.html#poll-everywhere-question-2-1",
    "title": "Lesson 11: Interactions",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-12",
    "href": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-12",
    "title": "Lesson 11: Interactions",
    "section": "Odds Ratio in the Presence of Interaction (1/2)",
    "text": "Odds Ratio in the Presence of Interaction (1/2)\n\nWhen interaction exists between a risk factor (F) and another variable (X), the estimate of the odds ratio for F depends on the value of X\n\n \n\nWhen an interaction term (F*X) exists in the model\n\n\\({\\widehat{OR}}_F \\neq \\exp(\\widehat\\beta_F)\\) in general\n\n\n \n\nAssume we want to compute the odds ratio for (\\(F = f_1\\) and \\(F = f_0\\), the correct model-based estimate is \\[\\begin{aligned}\n{\\widehat{OR}}_F = &\\exp{\\left(\\hat{g}\\left(F=f_1,X=x\\right)-\\hat{g}\\left(F=f_0,X=x\\right)\\right)}\n\\end{aligned}\\]\n\nLet’s work this out on the next slide!"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-22",
    "href": "lectures/11_Interactions/11_Interactions.html#odds-ratio-in-the-presence-of-interaction-22",
    "title": "Lesson 11: Interactions",
    "section": "Odds Ratio in the Presence of Interaction (2/2)",
    "text": "Odds Ratio in the Presence of Interaction (2/2)\n\nWe may write the two logits (log-odds) for given x as below:\n\n\\[ \\begin{aligned}\n\\hat{g}\\left(F = f_1,X = x\\right)&={\\hat{\\beta}}_0+{\\hat{\\beta}}_1f_1+{\\hat{\\beta}}_2x+{\\hat{\\beta}}_3f_1\\cdot\\ x \\\\\n\\hat{g}\\left(F = f_0,X=x\\right)&={\\hat{\\beta}}_0+{\\hat{\\beta}}_1f_0+{\\hat{\\beta}}_2x+{\\hat{\\beta}}_3f_0\\cdot\\ x\n\\end{aligned}\\]\n\nThe difference in two logits (log-odds) is:\n\n\\[ \\begin{aligned}\n\\hat{g}\\left(f_1,x\\right)-\\hat{g}\\left(f_0,x\\right) & = \\left[{\\hat{\\beta}}_0+{\\hat{\\beta}}_1f_1+{\\hat{\\beta}}_2x+{\\hat{\\beta}}_3f_1\\cdot\\ x \\right] - \\left[{\\hat{\\beta}}_0+{\\hat{\\beta}}_1f_0+{\\hat{\\beta}}_2x+{\\hat{\\beta}}_3f_0\\cdot\\ x \\right] \\\\\n\\hat{g}\\left(f_1,x\\right)-\\hat{g}\\left(f_0,x\\right) & = \\hat{\\beta}_1 f_1- \\hat{\\beta}_1f_0 + \\hat{\\beta}_3 x f_1 - \\hat\\beta_3 x f_0\\\\\n\\hat{g}\\left(f_1,x\\right)-\\hat{g}\\left(f_0,x\\right) & = {\\hat{\\beta}}_1\\left(f_1-f_0\\right)+{\\hat{\\beta}}_3x\\left(f_1-f_0\\right)\n\\end{aligned}\\]\n\nTherefore,\n\n\\[\\widehat{OR}(F=f-1, F=f_0, X=x) = \\widehat{OR}_F = \\exp\\left[ {\\hat{\\beta}}_1\\left(f_1-f_0\\right)+{\\hat{\\beta}}_3x\\left(f_1-f_0\\right) \\right]\\]"
  },
  {
    "objectID": "lectures/11_Interactions/old_work/Class_9_code.html",
    "href": "lectures/11_Interactions/old_work/Class_9_code.html",
    "title": "Class 9 Code",
    "section": "",
    "text": "library(tidyr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(epiDisplay)\n\nLoading required package: foreign\n\n\nLoading required package: survival\n\n\nLoading required package: MASS\n\n\n\nAttaching package: 'MASS'\n\n\nThe following object is masked from 'package:dplyr':\n\n    select\n\n\nLoading required package: nnet\n\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(here)\n\nhere() starts at /Users/wakim/Library/CloudStorage/OneDrive-OregonHealth&ScienceUniversity/Teaching/Classes/S2024_BSTA_513_613/S2024_BSTA_513\n\nlibrary(ggplot2)\n\n\nAttaching package: 'ggplot2'\n\n\nThe following object is masked from 'package:epiDisplay':\n\n    alpha\n\nload(here(\"data\", \"bc_diagnosis.Rda\"))"
  },
  {
    "objectID": "lectures/11_Interactions/old_work/Class_9_code.html#glow-study-example",
    "href": "lectures/11_Interactions/old_work/Class_9_code.html#glow-study-example",
    "title": "Class 9 Code",
    "section": "GLOW study example",
    "text": "GLOW study example\n\nlibrary(aplore3)\nglow = glow500\n\n\nglow_m1 = glm(fracture ~ priorfrac, data = glow, family = binomial)\n\n\nglow_m2 = glm(fracture ~ priorfrac + age, data = glow, family = binomial)\n\n\nglow_m3 = glm(fracture ~ priorfrac + age + priorfrac*age, data = glow, family = binomial)\n\n\nsummary(glow_m1); summary(glow_m2); summary(glow_m3)\n\n\nCall:\nglm(formula = fracture ~ priorfrac, family = binomial, data = glow)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   -1.4167     0.1305 -10.859  &lt; 2e-16 ***\npriorfracYes   1.0638     0.2231   4.769 1.85e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 540.07  on 498  degrees of freedom\nAIC: 544.07\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nCall:\nglm(formula = fracture ~ priorfrac + age, family = binomial, \n    data = glow)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -4.21429    0.84784  -4.971 6.67e-07 ***\npriorfracYes  0.83884    0.23416   3.582 0.000340 ***\nage           0.04119    0.01218   3.382 0.000719 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 528.52  on 497  degrees of freedom\nAIC: 534.52\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nCall:\nglm(formula = fracture ~ priorfrac + age + priorfrac * age, family = binomial, \n    data = glow)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)      -5.68942    1.08408  -5.248 1.54e-07 ***\npriorfracYes      4.96134    1.81022   2.741  0.00613 ** \nage               0.06251    0.01546   4.043 5.27e-05 ***\npriorfracYes:age -0.05738    0.02501  -2.294  0.02179 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 523.27  on 496  degrees of freedom\nAIC: 531.27\n\nNumber of Fisher Scoring iterations: 4\n\n\n\nprior_age = expand_grid(priorfrac = c(\"No\", \"Yes\"), \n                        age = 55:90)\nnewdata = with(glow, data.frame(prior_age))\nfrac_pred = predict(glow_m3, newdata, se.fit = T, type=\"response\")\npred_glow = newdata %&gt;% mutate(frac_pred = frac_pred$fit)\nggplot(pred_glow) + #geom_point(aes(x = age, y = frac_pred, color = priorfrac)) +\n  geom_smooth(method = \"loess\", aes(x = age, y = frac_pred, color = priorfrac)) +\n  theme(legend.position = \"right\", \n        text = element_text(size=20), \n        title = element_text(size=16)) +\n  scale_color_discrete(name = \"Prior Fracture\") +\n  xlab(\"Age (years)\") + ylab(\"Predicted Probability of Fracture\") +\n  ylim(0,1)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nprior_age = expand_grid(priorfrac = c(\"No\", \"Yes\"), \n                        age = 55:90)\nnewdata = with(glow, data.frame(prior_age))\nfrac_pred_log = predict(glow_m3, newdata, se.fit = T, type=\"link\")\npred_glow2 = newdata %&gt;% mutate(frac_pred_log = frac_pred_log$fit)\nggplot(pred_glow2) + #geom_point(aes(x = age, y = frac_pred, color = priorfrac)) +\n  geom_smooth(method = \"loess\", aes(x = age, y = frac_pred_log, color = priorfrac)) +\n  theme(legend.position = \"right\", \n        text = element_text(size=20), \n        title = element_text(size=16)) +\n  scale_color_discrete(name = \"Prior Fracture\") +\n  xlab(\"Age (years)\") + ylab(\"Log-Odds of Fracture\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nage = 55:90\nlogit_OR = glow_m3$coefficients[2] + glow_m3$coefficients[4]*age\nOR_data = data.frame(Age = age, OR = exp(logit_OR))\nggplot(OR_data) + \n  geom_hline(yintercept=1, color = \"#65A43D\", size = 1.5) +\n  geom_smooth(method = \"loess\", aes(x = Age, y = OR), color = \"black\", size = 2) +\n  theme(legend.position = \"right\", \n        text = element_text(size=20), \n        title = element_text(size=16)) +\n  xlab(\"Age (years)\") + ylab(\"Odds Ratio\") +\n  ylim(0, 10)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\nInteraction model - Poll Everywhere\n\ninter_bc = glm(Late_stage_diag ~ Race_Ethnicity + Age + Race_Ethnicity*Age, data = bc, \n               family = binomial())\nsummary(inter_bc)\n\n\nCall:\nglm(formula = Late_stage_diag ~ Race_Ethnicity + Age + Race_Ethnicity * \n    Age, family = binomial(), data = bc)\n\nCoefficients:\n                                                     Estimate Std. Error\n(Intercept)                                         -4.540270   0.242635\nRace_EthnicityHispanic-Latino                        0.317162   0.733038\nRace_EthnicityNH American Indian/Alaskan Native     -4.104745   5.085605\nRace_EthnicityNH Asian/Pacific Islander              0.532264   0.719711\nRace_EthnicityNH Black                              -0.337456   0.657331\nAge                                                  0.056762   0.003806\nRace_EthnicityHispanic-Latino:Age                   -0.005279   0.011561\nRace_EthnicityNH American Indian/Alaskan Native:Age  0.062026   0.077179\nRace_EthnicityNH Asian/Pacific Islander:Age         -0.006314   0.011338\nRace_EthnicityNH Black:Age                           0.011035   0.010350\n                                                    z value Pr(&gt;|z|)    \n(Intercept)                                         -18.712   &lt;2e-16 ***\nRace_EthnicityHispanic-Latino                         0.433    0.665    \nRace_EthnicityNH American Indian/Alaskan Native      -0.807    0.420    \nRace_EthnicityNH Asian/Pacific Islander               0.740    0.460    \nRace_EthnicityNH Black                               -0.513    0.608    \nAge                                                  14.913   &lt;2e-16 ***\nRace_EthnicityHispanic-Latino:Age                    -0.457    0.648    \nRace_EthnicityNH American Indian/Alaskan Native:Age   0.804    0.422    \nRace_EthnicityNH Asian/Pacific Islander:Age          -0.557    0.578    \nRace_EthnicityNH Black:Age                            1.066    0.286    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 11861  on 9999  degrees of freedom\nResidual deviance: 11481  on 9990  degrees of freedom\nAIC: 11501\n\nNumber of Fisher Scoring iterations: 4\n\nwald_test = summary(inter_bc)$coefficients\nrownames(wald_test) = c(\"Intercept\", \"R_E: HL\", \"R_E: NH AIAN\",\n                        \"R_E: NH API\",\"R_E: NH B\", \"Age\", \n                        \"R_E: HL * Age\", \"R_E: NH AIAN * Age\",\n                        \"R_E: NH API * Age\",\"R_E: NH B * Age\")\nround(wald_test, 3)\n\n                   Estimate Std. Error z value Pr(&gt;|z|)\nIntercept            -4.540      0.243 -18.712    0.000\nR_E: HL               0.317      0.733   0.433    0.665\nR_E: NH AIAN         -4.105      5.086  -0.807    0.420\nR_E: NH API           0.532      0.720   0.740    0.460\nR_E: NH B            -0.337      0.657  -0.513    0.608\nAge                   0.057      0.004  14.913    0.000\nR_E: HL * Age        -0.005      0.012  -0.457    0.648\nR_E: NH AIAN * Age    0.062      0.077   0.804    0.422\nR_E: NH API * Age    -0.006      0.011  -0.557    0.578\nR_E: NH B * Age       0.011      0.010   1.066    0.286"
  },
  {
    "objectID": "lectures/12_Numeric_Problems/12_Numeric_problems.html#section-1",
    "href": "lectures/12_Numeric_Problems/12_Numeric_problems.html#section-1",
    "title": "Lesson 12: Numeric Problems",
    "section": "",
    "text": "Lesson 12: Numeric Problems"
  },
  {
    "objectID": "project/Missing_data.html",
    "href": "project/Missing_data.html",
    "title": "Missing Data",
    "section": "",
    "text": "library(here)\nlibrary(tidyverse)\nlibrary(mice)\nlibrary(naniar)\nlibrary(gtsummary)\nload(here(\"project\", \"data\", \"wbns_data.rda\"))\nMy variables:\nYou should only need to replace my dataset name with yours and my use of variable names with your project’s variables.\nThere are two blank sections that I invite you to try to complete. These are both important parts of quality assurance. It’s good to check that the distribution of fully observed variables are not different in the missing vs. observed groups of our variables with missing data. Also, it is important to see how our imputed data look if we place it back with the observed data. Do they stand out? I hope not! (I was going to provide code for this, but it’s not new stuff. You can visualize these! … And I really ran out of time this week :( )"
  },
  {
    "objectID": "project/Missing_data.html#citizenship-had-missing-data-that-was-not-coded-as-na",
    "href": "project/Missing_data.html#citizenship-had-missing-data-that-was-not-coded-as-na",
    "title": "Missing Data",
    "section": "Citizenship had missing data that was not coded as NA",
    "text": "Citizenship had missing data that was not coded as NA\nI’m using the replace_with_na function to change the missing category to NA.\n\nsummary(wbns)\n\n      ID            FOOD_INSEC      PPAGE         LGBT      XACSNET   \n Length:8142        No  :6015   Min.   :18.00   No  :6852   No : 378  \n Class :character   Yes :2069   1st Qu.:36.00   Yes : 621   Yes:7764  \n Mode  :character   NA's:  58   Median :47.00   NA's: 669             \n                                Mean   :45.92                         \n                                3rd Qu.:57.00                         \n                                Max.   :64.00                         \n                                                                      \n PARTNER       FAMSIZE     INSURED     UNMETCARE_Y      PPP20197   \n No :7326   2      :3009   No  : 856   No :6725    Yes      :6825  \n Yes: 816   1      :2753   Yes :7167   Yes:1417    Missing  :   5  \n            3      : 986   NA's: 119               No       : 470  \n            4      : 880                           Not asked: 842  \n            5      : 336                                           \n            6      : 114                                           \n            (Other):  64                                           \n                 PPINCIMP                       PPETHM    \n Less than $5,000    : 717   White, Non-Hispanic   :4738  \n $100,000 to $124,999: 641   2+ Races, Non-Hispanic: 238  \n $15,000 to $19,999  : 511   Black, Non-Hispanic   :1023  \n $20,000 to $24,999  : 499   Hispanic              :1473  \n $60,000 to $74,999  : 443   Other, Non-Hispanic   : 670  \n $150,000 to $174,999: 423                                \n (Other)             :4908                                \n                        PPEDUCAT   \n Less than high school      : 613  \n Bachelor's degree or higher:3041  \n High school                :1940  \n Some college               :2548  \n                                   \n                                   \n                                   \n\nwbns_miss = wbns %&gt;% \n  replace_with_na(replace = list(PPP20197 = \"Missing\"))\n  # If Missing, replace with NA, if not then keep value as is\n\nsummary(wbns_miss)\n\n      ID            FOOD_INSEC      PPAGE         LGBT      XACSNET   \n Length:8142        No  :6015   Min.   :18.00   No  :6852   No : 378  \n Class :character   Yes :2069   1st Qu.:36.00   Yes : 621   Yes:7764  \n Mode  :character   NA's:  58   Median :47.00   NA's: 669             \n                                Mean   :45.92                         \n                                3rd Qu.:57.00                         \n                                Max.   :64.00                         \n                                                                      \n PARTNER       FAMSIZE     INSURED     UNMETCARE_Y      PPP20197   \n No :7326   2      :3009   No  : 856   No :6725    Yes      :6825  \n Yes: 816   1      :2753   Yes :7167   Yes:1417    Missing  :   0  \n            3      : 986   NA's: 119               No       : 470  \n            4      : 880                           Not asked: 842  \n            5      : 336                           NA's     :   5  \n            6      : 114                                           \n            (Other):  64                                           \n                 PPINCIMP                       PPETHM    \n Less than $5,000    : 717   White, Non-Hispanic   :4738  \n $100,000 to $124,999: 641   2+ Races, Non-Hispanic: 238  \n $15,000 to $19,999  : 511   Black, Non-Hispanic   :1023  \n $20,000 to $24,999  : 499   Hispanic              :1473  \n $60,000 to $74,999  : 443   Other, Non-Hispanic   : 670  \n $150,000 to $174,999: 423                                \n (Other)             :4908                                \n                        PPEDUCAT   \n Less than high school      : 613  \n Bachelor's degree or higher:3041  \n High school                :1940  \n Some college               :2548"
  },
  {
    "objectID": "project/Missing_data.html#look-at-the-missing-data-pattern",
    "href": "project/Missing_data.html#look-at-the-missing-data-pattern",
    "title": "Missing Data",
    "section": "Look at the missing data pattern",
    "text": "Look at the missing data pattern\nNow I can look at the missing data pattern.\n\nmd.pattern(wbns_miss, rotate.names = T)\n\n\n\n\n\n\n\n\n     ID PPAGE XACSNET PARTNER FAMSIZE UNMETCARE_Y PPINCIMP PPETHM PPEDUCAT\n7331  1     1       1       1       1           1        1      1        1\n644   1     1       1       1       1           1        1      1        1\n88    1     1       1       1       1           1        1      1        1\n16    1     1       1       1       1           1        1      1        1\n38    1     1       1       1       1           1        1      1        1\n5     1     1       1       1       1           1        1      1        1\n14    1     1       1       1       1           1        1      1        1\n1     1     1       1       1       1           1        1      1        1\n2     1     1       1       1       1           1        1      1        1\n3     1     1       1       1       1           1        1      1        1\n      0     0       0       0       0           0        0      0        0\n     PPP20197 FOOD_INSEC INSURED LGBT    \n7331        1          1       1    1   0\n644         1          1       1    0   1\n88          1          1       0    1   1\n16          1          1       0    0   2\n38          1          0       1    1   1\n5           1          0       1    0   2\n14          1          0       0    1   2\n1           1          0       0    0   3\n2           0          1       1    1   1\n3           0          1       1    0   2\n            5         58     119  669 851\n\n\nLooks like citizenship, food insecurity, insurance, and identifying as LGBT have missing data."
  },
  {
    "objectID": "project/Missing_data.html#check-the-distributions-of-the-variables-from-the-observed-vs.-missing-data",
    "href": "project/Missing_data.html#check-the-distributions-of-the-variables-from-the-observed-vs.-missing-data",
    "title": "Missing Data",
    "section": "Check the distributions of the variables from the observed vs. missing data",
    "text": "Check the distributions of the variables from the observed vs. missing data"
  },
  {
    "objectID": "project/Missing_data.html#use-mice-to-impute-data",
    "href": "project/Missing_data.html#use-mice-to-impute-data",
    "title": "Missing Data",
    "section": "Use mice to impute data",
    "text": "Use mice to impute data\nNow we can use the mice() function to impute the data.\nLet’s break down what I’m using in the function below:\n\nI set m=5 as the number of imputations that I will be running\nI set print = F so I don’t get the message as it imputes\nI set defaultMethod = c(\"norm\", \"logreg\", \"polyreg\", \"polr\")\n\nThis means mice will determine which method to use based on the type of variable\nFor binary variables it will use “logreg.” Thus, it will use logistic regression to predict the missing values. This will include food insecurity, insured, and LGBT variables in my data.\nFor continuous variables, it will use “norm.predict” which will consist of predicting the missing values from linear regression. I don’t have any missing continuous values, so this one is not used.\nFor multi-level categorical varaibles, we use “polyreg” which is a type of logistic regression that is expanded to handle multiple levels of outcomes. This will include citizenship in my data.\nFor ordinal variables, we use “polr” which is a proportional odds model, another type of generalized linear regression model that handles ordered categories. I don’t have any missing ordinal variables, so this one is not used.\n\n\n\nwbns_MI_5 = mice(wbns_miss, m=5, print = F, \n                 defaultMethod = c(\"norm.predict\", \"logreg\", \"polyreg\", \"polr\"))\n\nWarning: Number of logged events: 76"
  },
  {
    "objectID": "project/Missing_data.html#complete-data",
    "href": "project/Missing_data.html#complete-data",
    "title": "Missing Data",
    "section": "Complete data",
    "text": "Complete data\nIt’s hard to parse through the output of mice, but we can take a look at one of the imputation’s complete (aka filled in) data. Below I will look at the first imputation (hence the 1). You can change the 1 to anything up to 5 to see the different imputations.\n\nd1 = complete(wbns_MI_5, 1)\nsummary(d1)\n\n      ID            FOOD_INSEC     PPAGE        LGBT      XACSNET    PARTNER   \n Length:8142        No :6049   Min.   :18.00   No :7455   No : 378   No :7326  \n Class :character   Yes:2093   1st Qu.:36.00   Yes: 687   Yes:7764   Yes: 816  \n Mode  :character              Median :47.00                                   \n                               Mean   :45.92                                   \n                               3rd Qu.:57.00                                   \n                               Max.   :64.00                                   \n                                                                               \n    FAMSIZE     INSURED    UNMETCARE_Y      PPP20197   \n 2      :3009   No : 877   No :6725    Yes      :6827  \n 1      :2753   Yes:7265   Yes:1417    Missing  :   0  \n 3      : 986                          No       : 472  \n 4      : 880                          Not asked: 843  \n 5      : 336                                          \n 6      : 114                                          \n (Other):  64                                          \n                 PPINCIMP                       PPETHM    \n Less than $5,000    : 717   White, Non-Hispanic   :4738  \n $100,000 to $124,999: 641   2+ Races, Non-Hispanic: 238  \n $15,000 to $19,999  : 511   Black, Non-Hispanic   :1023  \n $20,000 to $24,999  : 499   Hispanic              :1473  \n $60,000 to $74,999  : 443   Other, Non-Hispanic   : 670  \n $150,000 to $174,999: 423                                \n (Other)             :4908                                \n                        PPEDUCAT   \n Less than high school      : 613  \n Bachelor's degree or higher:3041  \n High school                :1940  \n Some college               :2548"
  },
  {
    "objectID": "project/Missing_data.html#see-how-the-imputed-values-look-within-the-observed-values",
    "href": "project/Missing_data.html#see-how-the-imputed-values-look-within-the-observed-values",
    "title": "Missing Data",
    "section": "See how the imputed values look within the observed values",
    "text": "See how the imputed values look within the observed values"
  },
  {
    "objectID": "project/Missing_data.html#fit-regression-with-imputations",
    "href": "project/Missing_data.html#fit-regression-with-imputations",
    "title": "Missing Data",
    "section": "Fit regression with imputations",
    "text": "Fit regression with imputations\nBelow is how we run a regression model with the output from mice (wbns_MI_5). We use the with function that can be read as: With wbns_MI_5, run the following glm function. It will automatically know to know 5 different regressions, one with each imputed dataset.\n\nreg = with(wbns_MI_5, glm(FOOD_INSEC ~ PPAGE + LGBT + XACSNET + PARTNER + \n                                   FAMSIZE + INSURED + UNMETCARE_Y + PPP20197 +\n                            PPETHM, family = binomial))\n\nOnce I have fit the regression model with each imputed dataset, I can pool the results. The mice function will use Rubin’s Rules to pool the estimates and variances. Note the use of the pool function!\n\npooled_reg_MI = pool(reg)\nmi_reg = summary(pooled_reg_MI, conf.int = TRUE)\nmi_reg\n\n                           term    estimate   std.error  statistic        df\n1                   (Intercept) -0.94092399 0.191125980 -4.9230565  371.1422\n2                         PPAGE  0.00433198 0.002352706  1.8412753 4257.8618\n3                       LGBTYes  0.29504789 0.094069406  3.1364915 7823.2657\n4                    XACSNETYes -0.53596317 0.120983545 -4.4300502 2633.9023\n5                    PARTNERYes  0.74171351 0.091820246  8.0778864 7755.2850\n6                      FAMSIZE2 -0.66386257 0.071469877 -9.2887045 7184.0725\n7                      FAMSIZE3 -0.30985352 0.093877706 -3.3006082 8042.7947\n8                      FAMSIZE4 -0.37783269 0.100154973 -3.7724806 8113.8532\n9                      FAMSIZE5 -0.09145848 0.140895783 -0.6491215 7209.2753\n10                     FAMSIZE6 -0.13370972 0.233548879 -0.5725128 8115.1279\n11                     FAMSIZE7 -0.17014962 0.383547409 -0.4436208 8119.0239\n12             FAMSIZE8 or more  0.71359901 0.454899462  1.5686961 8117.4492\n13                   INSUREDYes -0.26630423 0.090125051 -2.9548303  267.5774\n14               UNMETCARE_YYes  1.63076485 0.065345023 24.9562213 6570.3110\n15                   PPP20197No  0.16420384 0.126611682  1.2969091 2663.4593\n16            PPP20197Not asked  0.64393756 0.084100957  7.6567210 5487.8370\n17 PPETHM2+ Races, Non-Hispanic  0.62960454 0.150679742  4.1784286 8107.6586\n18    PPETHMBlack, Non-Hispanic  0.48429429 0.082295957  5.8847884 6012.6999\n19               PPETHMHispanic  0.57000522 0.076440622  7.4568365 6839.4712\n20    PPETHMOther, Non-Hispanic -0.53485283 0.129266329 -4.1376036 6390.7763\n         p.value         2.5 %      97.5 %\n1   1.284335e-06 -1.3167495965 -0.56509839\n2   6.565074e-02 -0.0002805507  0.00894451\n3   1.716164e-03  0.1106467165  0.47944907\n4   9.805811e-06 -0.7731955792 -0.29873077\n5   7.585129e-16  0.5617210473  0.92170598\n6   2.031468e-20 -0.8039645609 -0.52376058\n7   9.689561e-04 -0.4938781410 -0.12582891\n8   1.627826e-04 -0.5741621197 -0.18150327\n9   5.162805e-01 -0.3676555070  0.18473855\n10  5.669905e-01 -0.5915253974  0.32410595\n11  6.573286e-01 -0.9220008165  0.58170157\n12  1.167577e-01 -0.1781205135  1.60531853\n13  3.406969e-03 -0.4437486779 -0.08885979\n14 2.056988e-131  1.5026673651  1.75886234\n15  1.947748e-01 -0.0840633202  0.41247099\n16  2.240009e-14  0.4790663555  0.80880877\n17  2.966107e-05  0.3342335827  0.92497551\n18  4.200523e-09  0.3229647002  0.64562387\n19  9.956707e-14  0.4201578390  0.71985261\n20  3.554574e-05 -0.7882581666 -0.28144748"
  },
  {
    "objectID": "project/Missing_data.html#we-can-do-an-informal-sensitivity-analysis",
    "href": "project/Missing_data.html#we-can-do-an-informal-sensitivity-analysis",
    "title": "Missing Data",
    "section": "We can do an informal sensitivity analysis",
    "text": "We can do an informal sensitivity analysis\nHere I am simply running a logistic regression model using our complete data. We can compare the estimates from this model to the pooled estimates from the multiply imputed datasets.\n\nwbns_cc = wbns_miss %&gt;% drop_na()\nreg_cc = glm(FOOD_INSEC ~ PPAGE + LGBT + XACSNET + PARTNER + \n               FAMSIZE + INSURED + UNMETCARE_Y + PPP20197 + \n               PPETHM, family = binomial, data = wbns_cc)\n\ncc_reg = summary(reg_cc)\n\nI will put the estimates side-by-side now.\n\nestimates = cbind(Mult_imp = mi_reg$estimate, \n              Complete_case = cc_reg$coefficients[,1])\nestimates\n\n                                Mult_imp Complete_case\n(Intercept)                  -0.94092399  -0.923507128\nPPAGE                         0.00433198   0.003963706\nLGBTYes                       0.29504789   0.253429229\nXACSNETYes                   -0.53596317  -0.537266195\nPARTNERYes                    0.74171351   0.756557601\nFAMSIZE2                     -0.66386257  -0.733347778\nFAMSIZE3                     -0.30985352  -0.431279163\nFAMSIZE4                     -0.37783269  -0.469643349\nFAMSIZE5                     -0.09145848  -0.136786735\nFAMSIZE6                     -0.13370972  -0.333273866\nFAMSIZE7                     -0.17014962  -0.158287657\nFAMSIZE8 or more              0.71359901   0.698218128\nINSUREDYes                   -0.26630423  -0.242185879\nUNMETCARE_YYes                1.63076485   1.739880532\nPPP20197No                    0.16420384   0.239127321\nPPP20197Not asked             0.64393756   0.596022881\nPPETHM2+ Races, Non-Hispanic  0.62960454   0.641231577\nPPETHMBlack, Non-Hispanic     0.48429429   0.457402801\nPPETHMHispanic                0.57000522   0.535396844\nPPETHMOther, Non-Hispanic    -0.53485283  -0.541533105\n\n\nThe most important thing I am noting is that:\n\nThe complete case estimates have the same signs as the pooled.\nFor the most part, the magnitude of the estimates have not changed by much. Family size seems to be the most impacted."
  },
  {
    "objectID": "project/Missing_data.html#now-we-can-proceed-with-the-other-parts-of-lab-3",
    "href": "project/Missing_data.html#now-we-can-proceed-with-the-other-parts-of-lab-3",
    "title": "Missing Data",
    "section": "Now we can proceed with the other parts of Lab 3",
    "text": "Now we can proceed with the other parts of Lab 3\nBelow is some sample code for how I would make a tidy dataset or an odds ratio table from the pooled results! From reg and pooled_reg_MI you should be able to do everything else in Lab 3!\n\ntidy(pooled_reg_MI, exponentiate=T)\n\n                           term  estimate   std.error  statistic       p.value\n1                   (Intercept) 0.3902671 0.191125980 -4.9230565  1.284335e-06\n2                         PPAGE 1.0043414 0.002352706  1.8412753  6.565074e-02\n3                       LGBTYes 1.3431907 0.094069406  3.1364915  1.716164e-03\n4                    XACSNETYes 0.5851055 0.120983545 -4.4300502  9.805811e-06\n5                    PARTNERYes 2.0995300 0.091820246  8.0778864  7.585129e-16\n6                      FAMSIZE2 0.5148588 0.071469877 -9.2887045  2.031468e-20\n7                      FAMSIZE3 0.7335544 0.093877706 -3.3006082  9.689561e-04\n8                      FAMSIZE4 0.6853452 0.100154973 -3.7724806  1.627826e-04\n9                      FAMSIZE5 0.9125992 0.140895783 -0.6491215  5.162805e-01\n10                     FAMSIZE6 0.8748440 0.233548879 -0.5725128  5.669905e-01\n11                     FAMSIZE7 0.8435386 0.383547409 -0.4436208  6.573286e-01\n12             FAMSIZE8 or more 2.0413248 0.454899462  1.5686961  1.167577e-01\n13                   INSUREDYes 0.7662060 0.090125051 -2.9548303  3.406969e-03\n14               UNMETCARE_YYes 5.1077799 0.065345023 24.9562213 2.056988e-131\n15                   PPP20197No 1.1784545 0.126611682  1.2969091  1.947748e-01\n16            PPP20197Not asked 1.9039631 0.084100957  7.6567210  2.240009e-14\n17 PPETHM2+ Races, Non-Hispanic 1.8768682 0.150679742  4.1784286  2.966107e-05\n18    PPETHMBlack, Non-Hispanic 1.6230292 0.082295957  5.8847884  4.200523e-09\n19               PPETHMHispanic 1.7682763 0.076440622  7.4568365  9.956707e-14\n20    PPETHMOther, Non-Hispanic 0.5857555 0.129266329 -4.1376036  3.554574e-05\n              b        df dfcom          fmi       lambda m          riv\n1  3.078828e-03  371.1422  8122 0.1059458931 1.011410e-01 5 1.125215e-01\n2  9.635094e-08 4257.8618  8122 0.0213478239 2.088824e-02 5 2.133387e-02\n3  3.010394e-05 7823.2657  8122 0.0043368335 4.082327e-03 5 4.099061e-03\n4  3.876201e-04 2633.9023  8122 0.0325129738 3.177861e-02 5 3.282164e-02\n5  3.212251e-05 7755.2850  8122 0.0048286900 4.572080e-03 5 4.593080e-03\n6  3.305960e-05 7184.0725  8122 0.0080427492 7.766633e-03 5 7.827426e-03\n7  1.426055e-05 8042.7947  8122 0.0021898414 1.941747e-03 5 1.945525e-03\n8  3.446834e-06 8113.8532  8122 0.0006586404 4.123410e-04 5 4.125111e-04\n9  1.264598e-04 7209.2753  8122 0.0079194852 7.644300e-03 5 7.703185e-03\n10 1.594018e-05 8115.1279  8122 0.0005969622 3.506864e-04 5 3.508094e-04\n11 1.093794e-05 8119.0239  8122 0.0003354454 8.922336e-05 5 8.923132e-05\n12 3.757543e-05 8117.4492  8122 0.0004641367 2.178986e-04 5 2.179461e-04\n13 8.119475e-04  267.5774  8122 0.1264598795 1.199549e-01 5 1.363055e-01\n14 3.747975e-05 6570.3110  8122 0.0108340745 1.053302e-02 5 1.064514e-02\n15 4.209953e-04 2663.4593  8122 0.0322409563 3.151454e-02 5 3.254002e-02\n16 8.913799e-05 5487.8370  8122 0.0154818829 1.512315e-02 5 1.535537e-02\n17 1.237177e-05 8107.6586  8122 0.0009003157 6.538879e-04 5 6.543157e-04\n18 7.276280e-05 6012.6999  8122 0.0132205733 1.289240e-02 5 1.306078e-02\n19 4.556804e-05 6839.4712  8122 0.0096477741 9.358217e-03 5 9.446621e-03\n20 1.573331e-04 6390.7763  8122 0.0116080206 1.129875e-02 5 1.142787e-02\n           ubar\n1  3.283455e-02\n2  5.419605e-06\n3  8.812928e-03\n4  1.417187e-02\n5  8.392411e-03\n6  5.068272e-03\n7  8.795911e-03\n8  1.002688e-02\n9  1.969987e-02\n10 5.452595e-02\n11 1.470955e-01\n12 2.068884e-01\n13 7.148188e-03\n14 4.224996e-03\n15 1.552532e-02\n16 6.966005e-03\n17 2.268954e-02\n18 6.685309e-03\n19 5.788487e-03\n20 1.652098e-02\n\ntbl_regression(reg, exponentiate = T) # Note I am not using the pooled results\n\npool_and_tidy_mice(): Tidying mice model with\n`mice::pool(x) %&gt;% mice::tidy(conf.int = TRUE, conf.level = 0.95, exponentiate = TRUE)`\n\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    PPAGE\n1.00\n1.00, 1.01\n0.066\n    LGBT\n\n\n\n        No\n—\n—\n\n        Yes\n1.34\n1.12, 1.62\n0.002\n    XACSNET\n\n\n\n        No\n—\n—\n\n        Yes\n0.59\n0.46, 0.74\n&lt;0.001\n    PARTNER\n\n\n\n        No\n—\n—\n\n        Yes\n2.10\n1.75, 2.51\n&lt;0.001\n    FAMSIZE\n\n\n\n        1\n—\n—\n\n        2\n0.51\n0.45, 0.59\n&lt;0.001\n        3\n0.73\n0.61, 0.88\n&lt;0.001\n        4\n0.69\n0.56, 0.83\n&lt;0.001\n        5\n0.91\n0.69, 1.20\n0.5\n        6\n0.87\n0.55, 1.38\n0.6\n        7\n0.84\n0.40, 1.79\n0.7\n        8 or more\n2.04\n0.84, 4.98\n0.12\n    INSURED\n\n\n\n        No\n—\n—\n\n        Yes\n0.77\n0.64, 0.91\n0.003\n    UNMETCARE_Y\n\n\n\n        No\n—\n—\n\n        Yes\n5.11\n4.49, 5.81\n&lt;0.001\n    PPP20197\n\n\n\n        Yes\n—\n—\n\n        No\n1.18\n0.92, 1.51\n0.2\n        Not asked\n1.90\n1.61, 2.25\n&lt;0.001\n    PPETHM\n\n\n\n        White, Non-Hispanic\n—\n—\n\n        2+ Races, Non-Hispanic\n1.88\n1.40, 2.52\n&lt;0.001\n        Black, Non-Hispanic\n1.62\n1.38, 1.91\n&lt;0.001\n        Hispanic\n1.77\n1.52, 2.05\n&lt;0.001\n        Other, Non-Hispanic\n0.59\n0.45, 0.75\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n    # tble_regression will automatically pool them!"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "",
    "text": "Use test statistics of goodness-of-fit to determine if our preliminary final model fits the data well\n\nUsing Pearson residual statistic (𝑋^2) ~ 𝜒_(𝐽−(𝑝+1))^2\nUsing Hosmer and Lemeshow goodness-of-fit statistic (𝐶 ̂) ~ 𝜒_(𝑔−2)^2\n\nApply ROC AUC to determine how well model predicts binary outcome\nApply AIC and BIC as a summary measure to make additional comparisons between potential models\n\n\n\n\n\n\n\nOnce the preliminary final model has been determined, we need to assess the fit of the model\nVariable selection is no longer our focus at this stage\n\nWe want to find answer to whether the model fits the data adequately\n\nAssessing the Goodness of Fit or Assessing model fit\n\nAssess how well our fitted logistic regression model predicts/estimates the observed outcomes\nComparison: fitted/estimated outcome vs. observed outcome\n\n\n\n\n\n\nThe model building strategies we have discussed so far only assess the importance of covariates\n\nIt did not assess model fit\n\nPrevious in model building, we made relative comparisons between models\n\nOur conclusions were limited to: Model 1 (full model) fits data better than Model 2 (reduced model)\n\nAssessing goodness of fit is\n\nNot a relative comparison\nIt is an absolute comparison\nTo compare the fitted model to the largest possible model (saturated model)\nModel adequacy vs. Model comparison\n\n\n\n\n\n\n\n\n\nThe model fits the data well if\n\nSummary measures of the distance between the predicted/estimated/fitted and observed Y are small\nThe contribution of each pair (predicted and observed) to these summary measures is unsystematic and is small relative to the error structure of the model\nIt is possible to see a “good” summary measure of the distance between predicted and observed Y with some substantial deviation from fit for a few subjects\n\n\n\n\n\n\nComputation and evaluation of overall measures of fit\nExamination of other measures of the difference or distance between the observed and fitted/predicted values\nExamination of the individual components of the summary statistics (will do in Lesson 14: Model Diagnostics)\n\n\nToday we focus on #1 and #2"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-need-to-revisit",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-need-to-revisit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (NEED TO REVISIT!!)",
    "text": "Overview (NEED TO REVISIT!!)\n\nOnce the preliminary final model has been determined, we need to assess the fit of the model\nVariable selection is no longer our focus at this stage\n\nWe want to find answer to whether the model fits the data adequately\n\nAssessing the Goodness of Fit or Assessing model fit\n\nAssess how well our fitted logistic regression model predicts/estimates the observed outcomes\nComparison: fitted/estimated outcome vs. observed outcome"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-need-to-revisit-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-need-to-revisit-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (NEED TO REVISIT!!)",
    "text": "Overview (NEED TO REVISIT!!)\n\nThe model building strategies we have discussed so far only assess the importance of covariates\n\nIt did not assess model fit\n\nPrevious in model building, we made relative comparisons between models\n\nOur conclusions were limited to: Model 1 (full model) fits data better than Model 2 (reduced model)\n\nAssessing goodness of fit is\n\nNot a relative comparison\nIt is an absolute comparison\nTo compare the fitted model to the largest possible model (saturated model)\nModel adequacy vs. Model comparison"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Components to Assess Model Fit",
    "text": "Components to Assess Model Fit\n\nThe model fits the data well if\n\nSummary measures of the distance between the predicted/estimated/fitted and observed Y are small\n\nToday’s lecture!!\n\nThe contribution of each pair (predicted and observed) to these summary measures is unsystematic and is small relative to the error structure of the model\n\nModel Diagnostics that will be covered in another lecture!\n\n\n\n \n\nNeed both components\n\nIt is possible to see a “good” summary measure of the distance between predicted and observed Y with some substantial deviation from fit for a few subjects"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#components-to-assess-model-fit-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Components to Assess Model Fit",
    "text": "Components to Assess Model Fit\n\nComputation and evaluation of overall measures of fit\nExamination of other measures of the difference or distance between the observed and fitted/predicted values\nExamination of the individual components of the summary statistics (will do in Lesson 14: Model Diagnostics)\n\n\nToday we focus on #1 and #2"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#comparing-fitted-outcome-to-observed-outcome",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Comparing fitted outcome to observed outcome",
    "text": "Comparing fitted outcome to observed outcome\n\nIn logistic regression model, we estimate \\(\\pi(\\mathbf{X}) = P(Y=1|\\mathbf{X})\\)\n\nPredicted value, \\(\\widehat\\pi(\\mathbf{X})\\), is between 0 and 1 for each subject\n\nHowever, we always observe \\(Y=1\\) or \\(Y=0\\)\n\nNot an observed \\(\\pi(\\mathbf{X})\\)\n\n\n \n\nWe can deterimine the fitted outcome by sampling Y’s from a Bernoulli distribution with the fitted probability\n\n\\(\\widehat{Y} \\sim \\text{Bernoulli}(\\widehat\\pi(\\mathbf{X}))\\)\n\nIf there are groups of individuals that share the same covariate observations, then we can use the same \\(\\widehat\\pi(\\mathbf{X})\\)\n\n\\(\\sum_j \\widehat{Y} \\sim \\text{Binomial}(\\sum_j, \\widehat\\pi(\\mathbf{X}))\\)\n\n\n \n\nInstead of comparing the expected vs. observed at individual level, we can compare them at “group” level"
  },
  {
    "objectID": "lectures/12_Assessing_fit/old_work/Class_14_code.html",
    "href": "lectures/12_Assessing_fit/old_work/Class_14_code.html",
    "title": "Class 14 Code",
    "section": "",
    "text": "library(tidyverse)\nlibrary(epiDisplay)\nlibrary(kableExtra)\nlibrary(ggplot2)\nlibrary(aplore3)\nlibrary(GGally)\nlibrary(mfp)\nsource(\"Logistic_Dx_Functions.R\")"
  },
  {
    "objectID": "lectures/12_Assessing_fit/old_work/Class_14_code.html#glow-study-example",
    "href": "lectures/12_Assessing_fit/old_work/Class_14_code.html#glow-study-example",
    "title": "Class 14 Code",
    "section": "GLOW study example",
    "text": "GLOW study example\nThe GLOW study data is already a part of the aplore3 package (stands for the “Applied Logistic Regression 3rd Edition).\n\nglow = glow500"
  },
  {
    "objectID": "lectures/12_Assessing_fit/old_work/Class_14_code.html#pearson-residuals",
    "href": "lectures/12_Assessing_fit/old_work/Class_14_code.html#pearson-residuals",
    "title": "Class 14 Code",
    "section": "Pearson Residuals",
    "text": "Pearson Residuals\n\nglow2 = glow %&gt;% mutate(raterisk2 = factor(raterisk, levels = c(\"Less\", \"Same\", \"Greater\"), \n                                            labels = c(\"Less and Same\", \"Less and Same\", \"Greater\")), \n                        fracture1 = as.numeric(fracture)-1) \ncat_model = glm(fracture1 ~ priorfrac + momfrac + armassist + raterisk2 + momfrac*armassist, \n                 data = glow2, family = \"binomial\")\nsummary(cat_model)\n\n\nCall:\nglm(formula = fracture1 ~ priorfrac + momfrac + armassist + raterisk2 + \n    momfrac * armassist, family = \"binomial\", data = glow2)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              -1.8570     0.1823 -10.186  &lt; 2e-16 ***\npriorfracYes              0.8948     0.2338   3.828 0.000129 ***\nmomfracYes                0.9969     0.3724   2.677 0.007426 ** \narmassistYes              0.6723     0.2400   2.802 0.005086 ** \nraterisk2Greater          0.3510     0.2314   1.517 0.129324    \nmomfracYes:armassistYes  -0.9101     0.5932  -1.534 0.124987    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 523.94  on 494  degrees of freedom\nAIC: 535.94\n\nNumber of Fisher Scoring iterations: 4\n\n#gof(cat_model)$chiSq\n\n## Still can't find good replacement for LogisticDx package"
  },
  {
    "objectID": "lectures/12_Assessing_fit/old_work/Class_14_code.html#hosmer-lemeshow-test",
    "href": "lectures/12_Assessing_fit/old_work/Class_14_code.html#hosmer-lemeshow-test",
    "title": "Class 14 Code",
    "section": "Hosmer-Lemeshow Test",
    "text": "Hosmer-Lemeshow Test\n\nglow2 = glow %&gt;% mutate(raterisk2 = factor(raterisk, levels = c(\"Less\", \"Same\", \"Greater\"), \n                                            labels = c(\"Less and Same\", \"Less and Same\", \"Greater\"))) \n\n\nprelim_final = glm(fracture ~ age + height + priorfrac + momfrac + armassist + raterisk2 +\n              age*priorfrac + momfrac*armassist, \n                 data = glow2, family = binomial)\nsummary(prelim_final)\n\n\nCall:\nglm(formula = fracture ~ age + height + priorfrac + momfrac + \n    armassist + raterisk2 + age * priorfrac + momfrac * armassist, \n    family = binomial, data = glow2)\n\nCoefficients:\n                        Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)              1.71724    3.32173   0.517 0.605175    \nage                      0.05731    0.01650   3.473 0.000515 ***\nheight                  -0.04674    0.01833  -2.550 0.010781 *  \npriorfracYes             4.61229    1.88018   2.453 0.014163 *  \nmomfracYes               1.24664    0.39296   3.172 0.001512 ** \narmassistYes             0.64416    0.25193   2.557 0.010561 *  \nraterisk2Greater         0.46897    0.24078   1.948 0.051449 .  \nage:priorfracYes        -0.05527    0.02593  -2.132 0.033044 *  \nmomfracYes:armassistYes -1.28054    0.62299  -2.055 0.039834 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 562.34  on 499  degrees of freedom\nResidual deviance: 500.50  on 491  degrees of freedom\nAIC: 518.5\n\nNumber of Fisher Scoring iterations: 4\n\n\nNeed to make sure that the observed outcome, fracture, is in a numeric scale (so takes 0 or 1 values). In the above code, I am using the outcome as a factor, so I will change it here.\n\nglow2 = glow2 %&gt;% mutate(frac_num = as.numeric(fracture)-1)\n\nlibrary(ResourceSelection)\n\nResourceSelection 0.3-6      2023-06-27\n\nhoslem.test(glow2$frac_num, fitted(prelim_final), g = 10)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  glow2$frac_num, fitted(prelim_final)\nX-squared = 6.3919, df = 8, p-value = 0.6034"
  },
  {
    "objectID": "lectures/12_Assessing_fit/old_work/Class_14_code.html#roc-curve-and-auc",
    "href": "lectures/12_Assessing_fit/old_work/Class_14_code.html#roc-curve-and-auc",
    "title": "Class 14 Code",
    "section": "ROC Curve and AUC",
    "text": "ROC Curve and AUC\n\nlibrary(ggplot2)\nlibrary(pROC)\n\n\npredicted &lt;- predict(prelim_final, glow2, type=\"response\")\n\n#define object to plot and calculate AUC\nrocobj &lt;- roc(glow2$fracture, predicted)\nauc &lt;- round(auc(glow2$fracture, predicted),4)\n\n#create ROC plot\nggroc(rocobj, colour = 'steelblue', size = 2) +\n  ggtitle(paste0('ROC Curve ', '(AUC = ', auc, ')')) +\n  theme(text = element_text(size = 16)) +\n  xlab(\"False Positive Rate (1 - Specificity)\") +\n  ylab(\"True Positive Rate (Sensitivity)\")"
  },
  {
    "objectID": "lectures/12_Assessing_fit/old_work/Class_14_code.html#aic-and-bic",
    "href": "lectures/12_Assessing_fit/old_work/Class_14_code.html#aic-and-bic",
    "title": "Class 14 Code",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nprelim_final = glm(fracture ~ age + height + priorfrac + \n                     momfrac + armassist + raterisk2 + \n                     age*priorfrac + momfrac*armassist, \n                 data = glow2, family = binomial)\n\n\nAIC(prelim_final)\n\n[1] 518.4966\n\nBIC(prelim_final)\n\n[1] 556.4281\n\nprelim_final2 = glm(fracture ~ age + height + priorfrac + \n                      momfrac + armassist + raterisk2 + \n                      age*height, \n                 data = glow2, family = binomial)\n\n\nAIC(prelim_final2)\n\n[1] 525.6843\n\nBIC(prelim_final2)\n\n[1] 559.4012"
  },
  {
    "objectID": "lectures/11_Interactions/11_Interactions.html",
    "href": "lectures/11_Interactions/11_Interactions.html",
    "title": "Lesson 11: Interactions",
    "section": "",
    "text": "Connect understanding of confounding and interactions from linear regression to logistic regression.\nDetermine if an additional independent variable is a not a confounder nor effect modifier, is a confounder but not effect modifier, or is an effect modifier.\nCalculate and interpret fitted interactions, including plotting the log-odds, predicted probability, and odds ratios."
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-measures-of-goodness-of-fit",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary Measures of Goodness of Fit",
    "text": "Summary Measures of Goodness of Fit\n\nAka overall measure of fit\n\n \n\nWhat do we need to calculate them?\n\nNeed to define what the fitted outcome is\nNeed to calculate how close the fitted outcome is to the observed outcome\nSummarize across all observations (or individuals’ data)\n\n\n \n\nTwo tests of goodness-of-fit\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: GLOW Study with interactions",
    "text": "Last Class: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nToday: determine the overall fit of this model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#last-class-reporting-results-of-glow-study-with-interactions",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Last Class: Reporting results of GLOW Study with interactions",
    "text": "Last Class: Reporting results of GLOW Study with interactions\n\nRemember our main covariate is prior fracture, so we want to focuse on how age changes the relationship between prior fracture and a new fracture!\n\n\n\nFor individuals 69 years old, the estimated odds of a new fracture for individuals with prior fracture is 2.72 times the estimated odds of a new fracture for individuals with no prior fracture (95% CI: 1.70, 4.35). As seen in Figure 1 (a), the odds ratio of a new fracture when comparing prior fracture status decreases with age, indicating that the effect of prior fractures on new fractures decreases as individuals get older. In Figure 1 (b), it is evident that for both prior fracture statuses, the predict probability of a new fracture increases as age increases. However, the predicted probability of new fracture for those without a prior fracture increases at a higher rate than that of individuals with a prior fracture. Thus, the predicted probabilities of a new fracture converge at age [insert age here].\n\n\n\n\n\n\n\n\n(a) Odds ratio of fracture outcome comparing prior fracture to no prior fracture\n\n\n\n\n\n\n\n(b) Predicted probability of fracture\n\n\n\n\nFigure 1: Plots of odds ratio and predicted probability from fitted interaction model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#today-a-jump-forward",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#today-a-jump-forward",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Today: a jump forward",
    "text": "Today: a jump forward\n\nGLOW study still\n\n \n\nAdding more variables\n\n \n\nWe’re jumping to our “final model”\n\nWe can use the Purposeful Selection from last quarter to arrive at this model!!\n\n\n \n\nToday we will be assessing the overall fit of the final model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#some-good-measurements-for-our-final-models",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Some good measurements for our final model(s)",
    "text": "Some good measurements for our final model(s)\n\nPearson residual statistic\n\n \n\nHosmer-Lemeshaw goodness-of-fit statistic\n\n \n\nAUC-ROC (area under the curve of the receiver operating characteristic)\n\n \n\nAIC/BIC"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview",
    "text": "Overview\n\nOnce a potential final model has been determined, we need to assess the fit of the model\nVariable selection is no longer our focus at this stage\n\nWe want to find answer to whether the model fits the data adequately\n\nAssessing the Goodness of Fit or Assessing model fit\n\nAssess how well our fitted logistic regression model predicts/estimates the observed outcomes\nComparison: fitted/estimated outcome vs. observed outcome"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview",
    "text": "Overview\n\nTo assess the fit of the model, it is good to have a mixture of measurements\nWe want to measure the absolute fit: not comparing to any models, but determining if the model fits the data well\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic\nAUC-ROC (kind of, often do not use a hypothesis test but you can!)\n\nWe want comparable measures of fit: if we have candidate models that are not nested\n\nAUC-ROC\nAIC/BIC"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#number-of-covariate-patterns",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Number of Covariate Patterns",
    "text": "Number of Covariate Patterns\n\nWhen the logistic regression model contains only categorical covariates, we can think of the number of covariate patterns\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\nWe can then compute the predicted number of individuals with Y=1 in each group, and compare that with the actual observed number of individuals with Y=1 in that group\n\nWe don’t need to sample this\nWe use the expected value (mean) of the Binomial to determine the \\(\\widehat{Y}\\) for each covariate pattern\nFor covariate pattern \\(j\\) with \\(m_j\\) observations: \\[\\widehat{Y}_j = m_j \\widehat\\pi(\\mathbf{X_j}) = m_j{\\hat{\\pi}}_j\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual",
    "text": "Pearson Residual\n\nIn logistic regression model, can use Pearson residual for summary measure of goodness-of-fit Uses the \\(\\widehat{Y}_j\\) fitted value from previous slide\nPearson residual for jth covariate pattern is: \\[r\\left(Y_j,{\\hat{\\pi}}_j\\right)=\\frac{(Y_j-m_j{\\hat{\\pi}}_j)}{\\sqrt{m_j{\\hat{\\pi}}_j(1-{\\hat{\\pi}}_j)}}=\\frac{(Y_j-{\\hat{Y}}_i)}{\\sqrt{{\\hat{Y}}_i(1-{\\hat{\\pi}}_j)}}\\]\nThe summary statistics of Pearson residual is thus: \\[X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#example",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#example",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Example",
    "text": "Example"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#issues-with-pearson-residuals",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Issues with Pearson Residuals",
    "text": "Issues with Pearson Residuals\n\nAssume current model has p covariates…\n\nthen \\(X^2\\) (Pearson residual) follows a chi-squared distribution\n\nunder the null hypothesis based on large sample theory\n\nOnly appropriate if the number of covariate patterns is less than the number of observations\n\n\n \n\nWhen the logistic regression model contains one or more continuous covariates, it is likely that the number of covariate patterns equals to the sample size n\n\n \n\nWe should not use Pearson Residuals to evaluate goodness-of-fit test when the fitted model contains one or more continuous variables"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-testing",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-testing",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual: testing",
    "text": "Pearson Residual: testing\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#not-going-to-bother-going-through-an-example",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Not going to bother going through an example",
    "text": "Not going to bother going through an example\n\nWe can calculate this by hand and test against a chi-squared distribution\n\n \n\nNo set R code to do this\n\n \n\nI do not see this as the main way to determine goodness of fit… for a binary outcome!\n\nOften because of the bigger issues with it…"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nIf number of covariate patterns is roughly same as the number of observations\n\nWhenever you include a continuous variable in your model\nHosmer-Lemeshow (HL) goodness-of-fit test should be used instead\n\n\n \n\nHowever, HL test does not work well if the number of covariate patterns is small\n\nHL test should not be used if the number of covariate patterns ≤ 6\n\nFor reference: 3 binary predictors makes 8 covariate patterns\n\nPearson residuals \\(X^2\\) should be used when the number of covariate patterns is small\n\n\n \n\nA large p-value from HL test suggests the model fits well"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-pearson-residual",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-pearson-residual",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Pearson residual",
    "text": "GLOW Study: Pearson residual\n\nOkay, so let’s look at a model with only prior fracture: \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF})\\]\nWe need to fit the model and use a new command:\n\n\nlibrary(aplore3)\nglow=glow500\nsource(here(\"lectures\", \"12_Assessing_fit\", \"Logistic_Dx_Functions.R\"))\nglow2 = glow %&gt;% mutate(frac_num = as.numeric(fracture)-1) \nglow2 = glow %&gt;% mutate(raterisk2 = factor(raterisk, levels = c(\"Less\", \"Same\", \"Greater\"), \n                                            labels = c(\"Less and Same\", \"Less and Same\", \"Greater\")), \n                        frac_num = as.numeric(fracture)-1) \nglow_m1 = glm(fracture ~ priorfrac + momfrac + \n                  armassist + raterisk2 + \n                  momfrac*armassist, , \n              data = glow2, family = binomial)\n\nanova(glow_m1,\nupdate(glow_m1, ~ 1),\ntest= \"Chisq\")\n\nAnalysis of Deviance Table\n\nModel 1: fracture ~ priorfrac + momfrac + armassist + raterisk2 + momfrac * \n    armassist\nModel 2: fracture ~ 1\n  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    \n1       494     523.94                          \n2       499     562.34 -5  -38.396 3.142e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ngof(glow_m1)\n\n\n$ct\n        n     y1hat    y1      y0hat    y0\n    &lt;int&gt;     &lt;num&gt; &lt;num&gt;      &lt;num&gt; &lt;num&gt;\n 1:     6  1.929020     2   4.070980     4\n 2:    11  4.129453     4   6.870547     7\n 3:    82 19.204752    20  62.795248    62\n 4:   169 22.823424    21 146.176576   148\n 5:    54  9.802732    11  44.197268    43\n 6:     6  3.221457     3   2.778543     3\n 7:    33 17.003775    16  15.996225    17\n 8:    38 10.504263     9  27.495737    29\n 9:    10  2.501341     3   7.498659     7\n10:     4  2.380969     2   1.619031     2\n11:    20  5.946266     7  14.053734    13\n12:    26 11.128460    13  14.871540    13\n13:    22  6.663013     5  15.336987    17\n14:     5  2.543312     2   2.456688     3\n15:     3  1.348182     1   1.651818     2\n16:    11  3.869581     6   7.130419     5\n\n$chiSq\n     test      chiSq    df      pVal\n   &lt;char&gt;      &lt;num&gt; &lt;int&gt;     &lt;num&gt;\n1:    PrI 497.234325   494 0.4507076\n2:    drI 523.939507   494 0.1697839\n3:    PrG   4.756822    10 0.9068193\n4:    drG   4.686232    10 0.9111289\n5:   PrCT   4.756822    10 0.9068193\n6:   drCT   4.686232    10 0.9111289\n\n$ctHL\n        P    y1     y1hat    y0     y0hat     n      Pbar\n   &lt;fctr&gt; &lt;num&gt;     &lt;num&gt; &lt;num&gt;     &lt;num&gt; &lt;int&gt;     &lt;num&gt;\n1:  0.135    21 22.823424   148 146.17658   169 0.1350498\n2:  0.182    11  9.802732    43  44.19727    54 0.1815321\n3:   0.25    23 21.706092    69  70.29391    92 0.2359358\n4:  0.303    21 23.113542    59  56.88646    80 0.2889193\n5:  0.428    25 21.056515    29  32.94348    54 0.3899355\n6:  0.595    24 26.497695    27  24.50230    51 0.5195626\n\n$gof\n         test   stat        val    df      pVal\n       &lt;char&gt; &lt;char&gt;      &lt;num&gt; &lt;num&gt;     &lt;num&gt;\n1:         HL  chiSq  2.4204611     8 0.9653368\n2:        mHL      F  2.1434000     5 0.1427335\n3:       OsRo      Z -0.9746271    NA 0.3297453\n4: SstPgeq0.5      Z  0.4922364    NA 0.6225522\n5:   SstPl0.5      Z  0.7472311    NA 0.4549241\n6:    SstBoth  chiSq  0.8006510     2 0.6701019\n7: SllPgeq0.5  chiSq  0.2381638     1 0.6255355\n8:   SllPl0.5  chiSq  0.5572551     1 0.4553683\n9:    SllBoth  chiSq  0.6056863     2 0.7387150\n\n$R2\n   method         R2\n   &lt;char&gt;      &lt;num&gt;\n1:    ssI 0.07826505\n2:    ssG 0.98817727\n3:    llI 0.06827892\n4:    llG 0.89122495\n\n$auc\n         auc lower 95% CI upper 95% CI \n    67.79413     62.41380     73.17447 \nattr(,\"interpret\")\n[1] \"auc = 0.5       --&gt; useless\"   \"0.7 &lt; auc &lt; 0.8 --&gt; good\"     \n[3] \"0.8 &lt; auc &lt; 0.9 --&gt; excellent\"\n\nattr(,\"link\")\n[1] \"logit\"\nattr(,\"class\")\n[1] \"gof.glm\" \"list\"   \n\n\n\n\nLesson 12: Assessing Model Fit"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test",
    "text": "Hosmer-Lemeshow test\n\nHL test uses groupings from percentiles to basically measure what Pearson residual measures\n\n \n\nSteps to compute HL test statistic:\n\nCompute estimated probability \\(\\widehat\\pi(\\mathbf{X}))\\) for all n subjects (\\(n=1, 2, ..., n\\))\nOrder \\(\\widehat\\pi(\\mathbf{X}))\\) from largest to smallest values\nDivide ordered values into g percentile grouping (usually \\(g = 10\\) based on H-L’s suggestion)\nForm table of observed and expected counts\nCalculate HL test statistic from table\nCompare HL test statistic to chi-squared distribution (\\(\\chi^2_{g-2}\\))"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everwhere-question-2",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everwhere question 2",
    "text": "Poll Everwhere question 2"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-statistic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test statistic",
    "text": "Hosmer-Lemeshow test statistic\n\nThe test statistic of Hosmer-Lemeshow goodness-of-fit test is denoted by \\(\\widehat{C}\\), which is obtained by calculating the Pearson chi-squared statistic from the \\(g \\times 2\\) table of observed and estimated expected frequencies \\[\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\]\n\nwhere \\(n'_k\\) is the total number of subjects in the \\(k\\)th group\n\nLet \\(c_k\\) be the number of covariate patterns in the \\(k\\)th decile: \\[o_k=\\sum_{j=1}^{c_k}y_j\\] and \\[{\\bar{\\pi}}_k=\\sum_{j=1}^{c_k}\\frac{m_j{\\hat{\\pi}}_j}{n_k^\\prime}\\]"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#hosmer-lemeshow-test-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Hosmer-Lemeshow test procedure",
    "text": "Hosmer-Lemeshow test procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\n\nNote: \\(\\widehat{C} \\sim \\chi^2_{df=g-2}\\)\n\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nOkay, so let’s look at the interaction model from last class \\[\\text{logit}\\left(\\pi(\\mathbf{X})\\right) = \\beta_0 + \\beta_1\\cdot I(\\text{PF}) +\\beta_2\\cdot Age + \\beta_3 \\cdot I(\\text{PF}) \\cdot Age\\]\nWe need to fit the model and use a new command:\n\n\nglow_m3 = glm(fracture ~ priorfrac + age_c + priorfrac*age_c, \n              data = glow, family = binomial)\nlibrary(ResourceSelection)\nobs_vals = as.numeric(glow$fracture) -1\nfit_vals = fitted(glow_m3)\nhoslem.test(obs_vals, fit_vals, g = 10)\n\n\n    Hosmer and Lemeshow goodness of fit (GOF) test\n\ndata:  obs_vals, fit_vals\nX-squared = 6.778, df = 8, p-value = 0.5608\n\n\nNote to Nicky: do NOT make conclusion yet! In the poll everywhere!"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-3",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere question 3",
    "text": "Poll Everywhere question 3"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-hosmer-lemeshow-test-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: Hosmer-Lemeshow test",
    "text": "GLOW Study: Hosmer-Lemeshow test\n\nConclusion: The p-value is 0.5608, so we fail to reject the null hypothesis that the model fits the data well. Thus, the preliminary final model for the GLOW dataset fits the data well\n\n \n\nDon’t forget that we still need to check individual observations (Model Diagnostics!)\n\n \n\nR may give results for the HL test even if it is not appropriate to use it!\n\nIf number of covariate patterns ≤ 6, do not use HL test"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#big-data-issue-in-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Big Data Issue in Goodness-of-fit Test",
    "text": "Big Data Issue in Goodness-of-fit Test\n\nWhen the sample size is really big (&gt; 1000), it is much more likely to find the H-L reject the model fit (even when the expected vs. observed in each decile categories looks pretty similar)\n\n \n\nThis is due to “too much” power in hypothesis testing.\n\nPaul et al. (2012) for samples sizes from 1000 to 25,000, the number of groups g should be equal to \\[g=\\max{\\left(10,\\min{\\left\\{\\frac{n_1}{2},\\ \\frac{n-n_1}{2},\\ 2+8\\left(\\frac{n}{1000}\\right)^2\\right\\}}\\right)}\\]\n\n\n \n\nFor example, if one has a sample with \\(n=10, 000\\) (sample size) and \\(n_1=1,000\\) (number of events) then \\(g=500\\) groups are suggested\nFor n &gt; 25000, other methods, such as partitioning data into a developmental data set (with smaller n) and a validation set"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#final-notes-on-goodness-of-fit-test",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Final Notes on Goodness-of-fit Test",
    "text": "Final Notes on Goodness-of-fit Test\n\nThey should not be used for variable selection\n\nThe likelihood ratio tests for significance of coefficients are much more powerful and appropriate (when nested)\n\n\n \n\nThey are not for model comparison\n\nOne should not use the p-value from goodness of fit tests of different models to decide which model is better than the other\nSomething like AUC-ROC, AIC, or BIC can be used"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (1/2)",
    "text": "ROC Curve and AUC (1/2)\n\n\n\nReceiver Operating Characteristics (ROC) curve is useful tool to quantify how good is our model predicting binary outcome\n\n \n\nIt is a plot of sensitivity (true positive rate) versus (1-specificity) or false positive rate of fitted binary values\n\nTrue Positive Rate \\(= \\dfrac{TP}{TP + FN}\\)\nFalse Positive Rate \\(= \\dfrac{FP}{FP + TN}\\)\n\n\n \n\nThe ROC curve shows the tradeoff between sensitivity and specificity"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (2/2)",
    "text": "ROC Curve and AUC (2/2)\n\n\n\nArea under the ROC curve (AUC ROC) is a reasonable summary of the overall predictive accuracy of the test\n\nAccuracy means how well the predicted value matches the observed value\n\n\n \n\nThe closer the curve follows the left-hand border and top border of the ROC space, the more accurate the test\n\nAn AUC =1 represents 100% accuracy\n\n\n \n\nThe closer the curve comes to the 45-degree diagonal line, the less accurate the test\n\nAn AUC = 0.5 represents an unhelpful model\n\nRandom predictions"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-4",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#roc-curve-and-auc-33",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "ROC Curve and AUC (3/3)",
    "text": "ROC Curve and AUC (3/3)\n\nOften only report the AUC\n\n \n\nSuggestions of how to interpret model fit through AUC values:"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#glow-study-roc-of-interaction-model",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "GLOW Study: ROC of interaction model",
    "text": "GLOW Study: ROC of interaction model\n\n\n\nlibrary(pROC)\npredicted &lt;- predict(glow_m3, glow, type=\"response\")\n\n# define object to plot and calculate AUC\nrocobj &lt;- roc(glow$fracture, predicted)\nauc &lt;- round(auc(glow$fracture, predicted),4)\n\n#create ROC plot\nggroc(rocobj, colour = 'steelblue', \n      size = 2, legacy.axes = TRUE) +\n  ggtitle(paste0('ROC Curve ','(AUC = ',auc,')')) +\n  theme(text = element_text(size = 23)) +\n  xlab(\"False Positive Rate (1 - Specificity)\") +\n  ylab(\"True Positive Rate (Sensitivity)\")\n\n\n\n\n\n\n\n\n\n\nWe have a poorly fitting model\nWe can take auc and compare it to other models: good way to pick a model based on predictive power"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#another-way-to-think-about-auc",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Another way to think about AUC",
    "text": "Another way to think about AUC\n\nGLOW Study: Consider the situation in which the fracture status of each individual is known\n\n \n\nRandomly pick one individual from fractured group and one from non-fractured outcome group\n\nBased on their age, height, prior fracture, and all other covariates, we will correctly predict which is from fractured group\n\n\n \n\nThe AUC is the percentage of randomly drawn pairs for which we predict the pair correctly\n\n \n\nTherefore, AUC represents the ability of our covariates to discriminate between individuals with the outcome (fracture) and those without the outcome"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nTwo widely used non-hypothesis testing based measurements that helps select a good model\n\nAkaike Information Criterion (AIC)\nBayesian Information Criterion (BIC)\n\n\n \n\nUnlike likelihood ratio test which is only suitable for nested model, AIC and BIC are suitable for both nested and non-nested model\n\n \n\nThere is no hypothesis/conclusion testing for the comparison between two models\n\nSo not the best for selecting covariates to include in model\nBUT helpful if you have a few preliminary final models that you want to compare"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#poll-everywhere-question-5",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Poll Everywhere Question 5",
    "text": "Poll Everywhere Question 5"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-1",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC",
    "text": "AIC and BIC\n\nBoth AIC and BIC penalize a model for having many parameters\n\n \n\n\n\n\n\n\n\n\nMeasure of fit\nEquation\nR code\n\n\n\n\nAkaike information criterion (AIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBayesian information criterion (BIC)\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\n \n\nWhere q is the number of parameters in the model and n is the sample size\nBoth AIC and BIC can only be used to compare models fitting the same data set\nIn comparing two models, the model with smaller AIC and/or BIC is preferred\n\nWhen the difference in AIC between two models exceeds 3, the difference is viewed as “meaningful”"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\nSpecial notes:\n\nUse Hosmer-Lemshow test over Pearson residual unless number of covariate patterns is less than 6\nCannot use Pearson residual when there is a continuous variable in the model\n\n\n\nLesson 12: Assessing Model Fit"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#aic-and-bic-in-r",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "AIC and BIC in R",
    "text": "AIC and BIC in R\n\nAfter fitting the logistic regression model, can calculate AIC and BIC\nLet’s look at the AIC and BIC of our interaction model:\n\n\nAIC(glow_m3)\n\n[1] 531.2716\n\nBIC(glow_m3)\n\n[1] 548.13"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (1/2)",
    "text": "Summary (1/2)\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)\n\n\n\nSpecial notes:\n\nUse Hosmer-Lemshow test over Pearson residual unless number of covariate patterns is less than 6\nCannot use Pearson residual when there is a continuous variable in the model"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#summary-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Summary (2/2)",
    "text": "Summary (2/2)\n\nFor our interaction model: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\nWe can examine the overall model fit using:\n\nNot comparing to any other models:\n\nPearson residual: Not appropriate for this model\nHosmer-Lemeshow: \\(\\hat{C}=6.778\\), p-value = 0.56\nAUC-ROC: 0.6819\n\nCan be used to compare to other models:\n\nAUC-ROC: 0.6819\nAIC: 531.27\nBIC: 548.13\n\n\n\n\n\nLesson 12: Assessing Model Fit"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-12",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (1/2)",
    "text": "Overview (1/2)\n\nOnce a potential final model has been determined, we need to assess the fit of the model\n\n \n\nVariable selection is no longer our focus at this stage\n\nWe want to find answer to whether the model fits the data adequately\n\n\n \n\nAssessing the Goodness of Fit or Assessing model fit\n\nAssess how well our fitted logistic regression model predicts/estimates the observed outcomes\nComparison: fitted/estimated outcome vs. observed outcome"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#overview-22",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Overview (2/2)",
    "text": "Overview (2/2)\n\nTo assess the fit of the model, it is good to have a mixture of measurements\n\n \n\nWe want to measure the absolute fit: not comparing to any models, but determining if the model fits the data well\n\nPearson residual statistic\nHosmer-Lemeshaw goodness-of-fit statistic\nAUC-ROC (kind of, often do not use a hypothesis test but you can!)\n\n\n \n\nWe want comparable measures of fit: if we have candidate models that are not nested\n\nAUC-ROC\nAIC/BIC"
  },
  {
    "objectID": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "href": "lectures/12_Assessing_fit/12_Assessing_fit.html#pearson-residual-procedure",
    "title": "Lesson 12: Assessing Model Fit",
    "section": "Pearson Residual procedure",
    "text": "Pearson Residual procedure\n\nSet the level of significance \\(\\alpha\\)\nSpecify the null ( \\(H_0\\) ) and alternative ( \\(H_A\\) ) hypotheses: same for all data\n\n\\(H_0\\): model fits well\n\\(H_1\\): model does not fits well\n\nCalculate the test statistic and p-value\nWrite a conclusion to the hypothesis test\n\nDo we reject or fail to reject \\(H_0\\)?\nWrite a conclusion in the context of the problem"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html",
    "title": "Lesson 12: Numeric Problems",
    "section": "",
    "text": "Identify and troubleshoot logistic regression analysis when there are zero observations for the cross section of the outcome and a predictor\nIdentify and troubleshoot logistic regression analysis when there is complete separation between the two outcome groups\nIdentify and troubleshoot logistic regression analysis when there is collinearity between variables"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#section-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#section-1",
    "title": "Lesson 12: Numeric Problems",
    "section": "",
    "text": "library(broom)\nlibrary(boot)\naug_m3 = augment(ex1_cont_glm) %&gt;% mutate(pred_prob = inv.logit(.fitted))\nnewdata = data.frame(x = c(1, 2, 3)) \npred = predict(ex1_cont_glm, newdata, se.fit=T, type = \"link\")\nLL_CI1 = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI1 = pred$fit + qnorm(1-0.05/2) * pred$se.fit\npred_link = cbind(Pred = pred$fit, LL_CI1, UL_CI1) %&gt;% inv.logit()\n\npred_prob = as.data.frame(pred_link) %&gt;% mutate(x = c(\"One\", \"Two\", \"Three\"))"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#three-numerical-problems",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#three-numerical-problems",
    "title": "Lesson 13: Numerical Problems",
    "section": "Three Numerical Problems",
    "text": "Three Numerical Problems\n\nIssues that may cause numerical problems:\n \n\nZero cell count\n\n \n\nComplete separation\n\n \n\nMulticollinearity\n\n\n \n\nAll may cause large estimated coefficients and/or large estimated standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table",
    "title": "Lesson 12: Numerical Problems",
    "section": "Zero cell in a contingency table",
    "text": "Zero cell in a contingency table\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-1",
    "title": "Lesson 12: Numerical Problems",
    "section": "Zero cell in a contingency table",
    "text": "Zero cell in a contingency table\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n    xTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n    xThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n  \n  \n  \n\n\n\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two\n2.79\n0.79, 10.5\n0.12\n        Three\n583,822,601\n0.00, NA\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-2",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-2",
    "title": "Lesson 12: Numerical Problems",
    "section": "Zero cell in a contingency table",
    "text": "Zero cell in a contingency table\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n    xTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n    xThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n  \n  \n  \n\n\n\n\n \n\nCoefficient estimate is large and standard error is large! Estimated odds ratio is very large and confidence interval cannot be computed.\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two\n2.79\n0.79, 10.5\n0.12\n        Three\n583,822,601\n0.00, NA\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-3",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-3",
    "title": "Lesson 12: Numerical Problems",
    "section": "Zero cell in a contingency table",
    "text": "Zero cell in a contingency table\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\nCoefficient estimates:\n\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n    xTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n    xThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n  \n  \n  \n\n\n\n\n\nCoefficient estimate is large and standard error is large! Estimated odds ratio is very large and confidence interval cannot be computed.\n\n\nOdds ratio:\n\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two\n2.79\n0.79, 10.5\n0.12\n        Three\n583,822,601\n0.00, NA\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#ways-to-address-zero-cell",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#ways-to-address-zero-cell",
    "title": "Lesson 13: Numerical Problems",
    "section": "Ways to address zero cell",
    "text": "Ways to address zero cell\n\nAdd one-half to each of the cell counts\n\nTechnically works, but not the best option\nRarely useful with a more complex analysis: may work for simple logistic regression\nNicky would say worst option because manipulating the data that does not work on individual level\n\nCollapse the categories to remove the 0 cells\n\nWe could collapse groups 2 and 3 together if it makes clinical sense\nGood idea if this makes clinical sense OR there is no difference between groups\n\nRemove the category with 0 cells\n\nThis would mean we reduce the total sample size as well\nNot a good idea: we would remove people from our dataset. Why would we do that?\n\nIf the variable is in ordinal scale, treat it as continuous\n\nGood idea if you have seen evidence that there is a linear trend on log-odds scale"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-1",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-interaction",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-interaction",
    "title": "Lesson 12: Numeric Problems",
    "section": "Zero cell in a contingency table: interaction",
    "text": "Zero cell in a contingency table: interaction\n\nOften when we add in interactions, we don’t look at the contingency table first\nWe are looking at the results of a fit model with the interaction\nInteraction term for x and third z group looks suspicious"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-interaction-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-interaction-1",
    "title": "Lesson 12: Numeric Problems",
    "section": "Zero cell in a contingency table: interaction",
    "text": "Zero cell in a contingency table: interaction"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-4",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-4",
    "title": "Lesson 12: Numeric Problems",
    "section": "Zero cell in a contingency table",
    "text": "Zero cell in a contingency table"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation",
    "text": "Complete Separation\n\nComplete separation: occurs when a collection of the covariates completely separates the outcome groups\n\nExample: Outcome is “gets senior discount at iHop” and the only covariate you measure is age\nAge will completely separate the outcome\nNo overlap in distribution of covariates between two outcome groups\n\n\n \n\nProblem: the maximum likelihood estimates do not exist\n\nLikelihood function is monotone\nIn order to have finite maximum likelihood estimates we must have some overlap in the distribution of the covariates in the model"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-2",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-2",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example",
    "title": "Lesson 12: Numeric Problems",
    "section": "Complete Separation: example",
    "text": "Complete Separation: example\n\n\n\nWe get a warning when we have complete separation\n\n\ny = c(0,0,0,0,1,1,1,1)\nx1 = c(1,2,3,3,5,6,10,11)\nx2 = c(3,2,-1,-1,2,4,1,0)\nex3 = data.frame(outcome = y, x1 = x1, x2= x2)\nex3\n\n\n\n\n  outcome x1 x2\n1       0  1  3\n2       0  2  2\n3       0  3 -1\n4       0  3 -1\n5       1  5  2\n6       1  6  4\n7       1 10  1\n8       1 11  0\n\n\n\n\n \n\nm1 = glm(outcome ~ x1 + x2, data = ex3, family=binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\nOutcomes of 0 and 1 are completely separated by x2\n\nIf x2 &gt; 4 then outcome is 1\nIf x2 &lt; 4 then outcome is 0"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-1",
    "title": "Lesson 12: Numeric Problems",
    "section": "Complete Separation: example",
    "text": "Complete Separation: example\n\n\nCoefficient estimates:\n\ntidy(m1, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−66.10\n183,471.72\n0.00\n1.00\n−10,644.72\n10,512.52\n    x1\n15.29\n27,362.84\n0.00\n1.00\n−3,122.69\nNA\n    x2\n6.24\n81,543.72\n0.00\n1.00\n−12,797.28\nNA"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-2",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-2",
    "title": "Lesson 12: Numeric Problems",
    "section": "Complete Separation: example",
    "text": "Complete Separation: example\n\n\nCoefficient estimates:\n\ntidy(m1, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−66.10\n183,471.72\n0.00\n1.00\n−10,644.72\n10,512.52\n    x1\n15.29\n27,362.84\n0.00\n1.00\n−3,122.69\nNA\n    x2\n6.24\n81,543.72\n0.00\n1.00\n−12,797.28\nNA\n  \n  \n  \n\n\n\n\n\n\n\nCoefficient estimate of x1 is large\nStandard error of x1’s coefficient is large\nBut also the coefficients and standard errors for the intercept and x2 is large!"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-1",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation",
    "text": "Complete Separation\n\nThe occurrence of complete separation in practice depends on\n\nSample size\nNumber of subjects with the outcome present\nNumber of variables included in the model\n\n\n \n\nExample: 25 observations and only 5 have “success” outcome\n\n1 variable in model may not lead to complete separation\nMore variables = more dimensions that can completely separate the observations\n\n\n \n\nIn most cases, the occurrence of complete separation is not bad for clinical importance\n\nBut rather a numerical coincidence that causing problem for model fitting"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-3",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-3",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-2",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-2",
    "title": "Lesson 12: Numeric Problems",
    "section": "Complete Separation",
    "text": "Complete Separation"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-3",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-3",
    "title": "Lesson 12: Numeric Problems",
    "section": "Complete Separation",
    "text": "Complete Separation"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity\n\nCollinearity happens when one or more of the covariates in a model can be predicted from other covariates in the same model\n\n \n\nThis will cause unreliable coefficient estimates for some covariates in logistic regression, as in an ordinary linear regression\n\n \n\nLooking at correlations among pairs of variables is helpful but not enough to identify collinearity problem\n\nBecause collinearity problems may involve relationships among more than two covariates"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-1",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity\n\nTable below is a simulated data with \\(x_1 \\sim \\text{Normal}(0,1)\\), \\(x_2 = x_1 + \\text{Uniform}(0,0.1)\\) , and \\(x_3 = 1 + \\text{Uniform}(0, 0.01)\\)\nTherefore, \\(x_1\\) and \\(x_2\\) are highly correlated, and \\(x_3\\) is nearly collinear with the constant term."
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-2",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-2",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity\n\nFour logistic regression models using data in the previous slide\nConsequence of collinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-3",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-3",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity\n\nFour logistic regression models using data in the previous slide\nConsequence of collinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-4",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-4",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity\n\nFour logistic regression models using data in the previous slide\nConsequence of collinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-5",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-5",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity\n\nCollinearity only involves the covariates\n\nNo specific issues to logistic regression (vs. linear regression)\nTechniques from 512/612 work well for logistic regression model\n\n\n \n\nIn more complicated dataset/analysis, we may not be able to detect collinearity using the coefficient estimates/SE\n\n \n\nVariance inflation factor (VIF) approach: well-known approach to detect collinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#variance-inflation-factor-vif-approach",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#variance-inflation-factor-vif-approach",
    "title": "Lesson 13: Numerical Problems",
    "section": "Variance Inflation Factor (VIF) Approach",
    "text": "Variance Inflation Factor (VIF) Approach\n\nComputed by regressing each variable on all the other explanatory variables\n\nFor example: \\(E(x_1│x_2,x_3,…)=\\alpha_0+\\alpha_1 x_2+\\alpha_2 x_3\\)\n\nCalculate the coefficient of determination, \\(R^2\\)\n\nProportion of the variation in \\(x_1\\) that is predicted from \\(x_2\\), \\(x_3\\),… \\[VIF = \\dfrac{1}{1=R^2}\\]\n\nEach covariate has its own VIF computed\nGet worried for multicollinearity if VIF &gt; 10\nSometimes VIF approach may miss serious multicollinearity\n\nSame multicollinearity we wish to detect using VIF can cause numerical problems in reliably estimating \\(R^2\\)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#glow-study-height-weight-bmi",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#glow-study-height-weight-bmi",
    "title": "Lesson 12: Numeric Problems",
    "section": "GLOW study: height, weight, BMI",
    "text": "GLOW study: height, weight, BMI"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-6",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-6",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-4",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#poll-everywhere-question-4",
    "title": "Lesson 13: Numerical Problems",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-7",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-7",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-8",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-8",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#wrap-up",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#wrap-up",
    "title": "Lesson 12: Numeric Problems",
    "section": "Wrap-up",
    "text": "Wrap-up\n\nWith experience and time, you won’t need to rely so heavily on the systematic approaches to model building and numerical problems\n\n\n\nLesson 12: Numeric Problems"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collapse-the-categories-of-predictor",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collapse-the-categories-of-predictor",
    "title": "Lesson 13: Numerical Problems",
    "section": "Collapse the categories of predictor",
    "text": "Collapse the categories of predictor\nCombine groups 2 and 3:\n\nex1_23 = ex1 %&gt;% \n            mutate(x = factor(x, levels = c(\"One\", \"Two\", \"Three\"), \n                                 labels = c(\"One\", \"Two-Three\", \"Two-Three\")))\nex1_23_glm = glm(outcome ~ x, data = ex1_23, family = binomial)\ntbl_regression(ex1_23_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two-Three\n7.43\n2.32, 26.3\n0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n\nBased on our previous visual, I don’t think this is a good idea\nLook at the estimated OR comparing group 2 to group 1 from our original model: 2.79 (95% CI: 0.79, 10.5)\n\nLooks different than the estimated OR in the above table"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#remove-the-category-with-0-cells",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#remove-the-category-with-0-cells",
    "title": "Lesson 13: Numerical Problems",
    "section": "Remove the category with 0 cells",
    "text": "Remove the category with 0 cells\nRemove group 3 from the data:\n\nex1_two = ex1 %&gt;% filter(x != \"Three\")\nex1_two_glm = glm(outcome ~ x, data = ex1_two, family = binomial())\ntbl_regression(ex1_two_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two\n2.79\n0.79, 10.5\n0.12\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval\n    \n  \n\n\n\n\n \n\nNot a good idea because we lose information (sample size goes down!)\nAnd really bad when we have other predictors!!"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous",
    "title": "Lesson 13: Numerical Problems",
    "section": "Treat predictor as continuous",
    "text": "Treat predictor as continuous\n\nWhen we treat a predictor as continuous, we need to make sure we have linearty between continuous predictor and log-odds\nCannot test this before fitting the logistic regression with the continuous predictor\n\nTry taking the logit of a probability of 1… it’s infinity!\n\n\n\nex1_cont = ex1 %&gt;% mutate(x = as.numeric(x))\nex1_cont_glm = glm(outcome ~ x, data = ex1_cont, family = binomial())\ntbl_regression(ex1_cont_glm, exponentiate=T) %&gt;% as_gt() %&gt;% \n  tab_options(table.font.size = 38)\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n6.22\n2.63, 18.0\n&lt;0.001\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell",
    "title": "Lesson 12: Numerical Problems",
    "section": "Decide on how to address zero cell",
    "text": "Decide on how to address zero cell\n\nLook at the proportions across the predictor, X:\n\n\nggplot(data = ex1, aes(x = x, fill = outcome)) + \n      geom_bar(stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\")\n\n\n\n\n\n\n\n\n\n \n\n\nCombining groups 2 and 3 together may not be a good idea.\nTheir proportions of the outcome do not look similar.\nThe predictor has an ordinal quality, so this is making me think a continuous approach might be good."
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#predict-probabilities-from-fitted-model-with-continuous-x",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#predict-probabilities-from-fitted-model-with-continuous-x",
    "title": "Lesson 12: Numeric Problems",
    "section": "Predict probabilities from fitted model with continuous x",
    "text": "Predict probabilities from fitted model with continuous x\n\nlibrary(broom)\nlibrary(boot)\naug_m3 = augment(ex1_cont_glm) %&gt;% mutate(pred_prob = inv.logit(.fitted))\nnewdata = data.frame(x = c(1, 2, 3)) \npred = predict(ex1_cont_glm, newdata, se.fit=T, type = \"link\")\nLL_CI1 = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI1 = pred$fit + qnorm(1-0.05/2) * pred$se.fit\npred_link = cbind(Pred = pred$fit, LL_CI1, UL_CI1) %&gt;% inv.logit()\n\npred_prob = as.data.frame(pred_link) %&gt;% mutate(x = c(\"One\", \"Two\", \"Three\"))"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous-check-linearity-assumption",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#treat-predictor-as-continuous-check-linearity-assumption",
    "title": "Lesson 13: Numerical Problems",
    "section": "Treat predictor as continuous: check linearity assumption",
    "text": "Treat predictor as continuous: check linearity assumption\n\nnewdata = data.frame(x = c(1, 2, 3)) \npred = predict(ex1_cont_glm, newdata, se.fit=T, type = \"link\")\nLL_CI1 = pred$fit - qnorm(1-0.05/2) * pred$se.fit\nUL_CI1 = pred$fit + qnorm(1-0.05/2) * pred$se.fit\n\npred_link = cbind(Pred = pred$fit, LL_CI1, UL_CI1) %&gt;% inv.logit()\npred_prob = as.data.frame(pred_link) %&gt;% mutate(x = c(\"One\", \"Two\", \"Three\"))\n\n\n\n\n\nPlotting sample and predicted probabilities\nggplot() + \n      geom_bar(data = ex1, aes(x = x, fill = outcome), stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\") +\n      scale_fill_manual(values=c(\"#D6295E\", \"#ED7D31\")) +\n      geom_point(data = pred_prob, aes(x = x, y=Pred), size=3) +\n      geom_errorbar(data = pred_prob, aes(x = x, y=Pred, ymin = LL_CI1, ymax = UL_CI1), width = 0.25)\n\n\n\n\n\n\n \n\nThis looks pretty good. We’ve mostly captured the trend of the outcome proportion!"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#summary-for-one-predictor",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#summary-for-one-predictor",
    "title": "Lesson 12: Numeric Problems",
    "section": "Summary for one predictor",
    "text": "Summary for one predictor\n\nMy suggestion is to try possible solutions in this order\n\nFor group with zero cell count, see if there is an adjacent group that makes sense to combine it with\n\n \n\nIf that does not make sense (or obscures your data) AND your data has an inherent order, then you can try treating it as continuous.\n\n \n\nRemove the zero count group and all the observations in it (not a very good solution)\n\n \n\nAdd a half count to each cell (only works for a single predictor)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-summary-for-one-predictor",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-summary-for-one-predictor",
    "title": "Lesson 12: Numeric Problems",
    "section": "Zero cell count: summary for one predictor",
    "text": "Zero cell count: summary for one predictor\nMy suggestion is to try possible solutions in this order\n\nFor group with zero cell count, see if there is an adjacent group that makes sense to combine it with\n\n \n\nIf that does not make sense (or obscures your data) AND your data has an inherent order, then you can try treating it as continuous.\n\n \n\nRemove the zero count group and all the observations in it (not a very good solution)\n\n \n\nAdd a half count to each cell (only works for a single predictor)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-when-we-have-multiple-predictors",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-when-we-have-multiple-predictors",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count when we have multiple predictors",
    "text": "Zero cell count when we have multiple predictors\n\nNote that we may not see the zero count cells in a single predictor\n \n\nBut we may have issues if there is an interaction!\n\n \n\nThis is why I suggested we keep an eye out for cell counts below 10 in our lab!\n\n\n \n\nIf you see a big coefficient estimate with a big standard deviation for a specific category or interaction…\n \n\n…this may mean that a low cell count for that category is causing you issues!"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-summary",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-summary",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: summary",
    "text": "Zero cell count: summary\nMy suggestion is to try possible solutions in this order\n\nFor group with zero cell count, see if there is an adjacent group that makes sense to combine it with\n\n \n\nIf that does not make sense (or obscures your data) AND your data has an inherent order, then you can try treating it as continuous.\n\n \n\nRemove the zero count group and all the observations in it (not a very good solution)\n\n \n\nAdd a half count to each cell (only works for a single predictor)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-13",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-13",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (1/3)",
    "text": "Complete Separation: example (1/3)\n\n\n\nWe get a warning when we have complete separation\n\n\ny = c(0,0,0,0,1,1,1,1)\nx1 = c(1,2,3,3,5,6,10,11)\nx2 = c(3,2,-1,-1,2,4,1,0)\nex3 = data.frame(outcome = y, x1 = x1, x2= x2)\nex3\n\n\n\n\n  outcome x1 x2\n1       0  1  3\n2       0  2  2\n3       0  3 -1\n4       0  3 -1\n5       1  5  2\n6       1  6  4\n7       1 10  1\n8       1 11  0\n\n\n\n\n \n\nm1 = glm(outcome ~ x1 + x2, data = ex3, family=binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n\n\nOutcomes of 0 and 1 are completely separated by x2\n\nIf x2 &gt; 4 then outcome is 1\nIf x2 &lt; 4 then outcome is 0"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-23",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-23",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (2/3)",
    "text": "Complete Separation: example (2/3)\n\n\n \nCoefficient estimates:\n\ntidy(m1, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−66.10\n183,471.72\n0.00\n1.00\n−10,644.72\n10,512.52\n    x1\n15.29\n27,362.84\n0.00\n1.00\n−3,122.69\nNA\n    x2\n6.24\n81,543.72\n0.00\n1.00\n−12,797.28\nNA"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-33",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-example-33",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: example (3/3)",
    "text": "Complete Separation: example (3/3)\n\n\n \nCoefficient estimates:\n\ntidy(m1, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−66.10\n183,471.72\n0.00\n1.00\n−10,644.72\n10,512.52\n    x1\n15.29\n27,362.84\n0.00\n1.00\n−3,122.69\nNA\n    x2\n6.24\n81,543.72\n0.00\n1.00\n−12,797.28\nNA\n  \n  \n  \n\n\n\n\n\n \n\n\nCoefficient estimate of x1 is large\nStandard error of x1’s coefficient is large\nBut also the coefficients and standard errors for the intercept and x2 are large!"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-ways-to-address-issue",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-ways-to-address-issue",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: Ways to address issue",
    "text": "Complete Separation: Ways to address issue\n\nCollapse categorical variables in a meaningful way\n\nEasiest and best if stat methods are restricted (common for collaborations)\n\n\n \n\nExclude x1 from the model\n\nNot ideal because this could lead to biased estimates for the other predicted variables in the model\n\n\n \n\nFirth logistic regression\n\nUses penalized likelihood estimation method\nBasically takes the likelihood (that has no maximum) and adds a penalty that makes the MLE estimatable"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-firth-logistic-regression",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#complete-separation-firth-logistic-regression",
    "title": "Lesson 13: Numerical Problems",
    "section": "Complete Separation: Firth logistic regression",
    "text": "Complete Separation: Firth logistic regression\n\nlibrary(logistf)\nm1_f = logistf(outcome ~ x1 + x2, data = ex3, family=binomial)\nsummary(m1_f) # Cannot use tidy on this :(\n\nlogistf(formula = outcome ~ x1 + x2, data = ex3, family = binomial)\n\nModel fitted by Penalized ML\nCoefficients:\n                  coef  se(coef)   lower 0.95 upper 0.95     Chisq          p\n(Intercept) -2.9748898 1.7244237 -15.47721665 -0.1208883 4.2179522 0.03999841\nx1           0.4908484 0.2745754   0.05268216  2.1275832 5.0225056 0.02501994\nx2           0.4313732 0.4988396  -0.65793078  4.4758930 0.7807099 0.37692411\n            method\n(Intercept)      2\nx1               2\nx2               2\n\nMethod: 1-Wald, 2-Profile penalized log-likelihood, 3-None\n\nLikelihood ratio test=5.505687 on 2 df, p=0.06374636, n=8\nWald test = 3.624899 on 2 df, p = 0.1632538"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-9",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-9",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-10",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-10",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity",
    "text": "Collinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-ways-to-address-the-issue",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#collinearity-ways-to-address-the-issue",
    "title": "Lesson 12: Numeric Problems",
    "section": "Collinearity: Ways to address the issue",
    "text": "Collinearity: Ways to address the issue\n\nExclude the redundant variable from the model\nScaling and centering variables\n\nWhen you have transformed a continuous variable\n\nOther modeling approach (outside scope of this class)\n\nRidge regression\nPrinciple component analysis\n\n\n \n\nPlease take a look at the BSTA 512/612 lesson that included multicollinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity happens when one or more of the covariates in a model can be predicted from other covariates in the same model\n\n \n\nThis will cause unreliable coefficient estimates for some covariates in logistic regression, as in an ordinary linear regression\n\n \n\nLooking at correlations among pairs of variables is helpful but not enough to identify multicollinearity problem\n\nBecause multicollinearity problems may involve relationships among more than two covariates"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-1",
    "title": "Lesson 12: Numerical Problems",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity only involves the covariates\n\nNo specific issues to logistic regression (vs. linear regression)\nTechniques from 512/612 work well for logistic regression model\n\n\n \n\nIn more complicated dataset/analysis, we may not be able to detect multicollinearity using the coefficient estimates/SE\n\n \n\nVariance inflation factor (VIF) approach: well-known approach to detect multicollinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-2",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-2",
    "title": "Lesson 12: Numerical Problems",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-3",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-3",
    "title": "Lesson 12: Numerical Problems",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity only involves the covariates\n\nNo specific issues to logistic regression (vs. linear regression)\nTechniques from 512/612 work well for logistic regression model\n\n\n \n\nIn more complicated dataset/analysis, we may not be able to detect multicollinearity using the coefficient estimates/SE\n\n \n\nVariance inflation factor (VIF) approach: well-known approach to detect multicollinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-4",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-4",
    "title": "Lesson 12: Numerical Problems",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-5",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-5",
    "title": "Lesson 12: Numerical Problems",
    "section": "Multicollinearity",
    "text": "Multicollinearity\n\nMulticollinearity only involves the covariates\n\nNo specific issues to logistic regression (vs. linear regression)\nTechniques from 512/612 work well for logistic regression model\n\n\n \n\nIn more complicated dataset/analysis, we may not be able to detect multicollinearity using the coefficient estimates/SE\n\n \n\nVariance inflation factor (VIF) approach: well-known approach to detect multicollinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-ways-to-address-the-issue",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-ways-to-address-the-issue",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: Ways to address the issue",
    "text": "Multicollinearity: Ways to address the issue\n\nExclude the redundant variable from the model\nScaling and centering variables\n\nWhen you have transformed a continuous variable\n\nOther modeling approach (outside scope of this class)\n\nRidge regression\nPrinciple component analysis\n\n\n \n\nPlease take a look at the BSTA 512/612 lesson that included multicollinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-in-a-contingency-table",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-in-a-contingency-table",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count in a contingency table",
    "text": "Zero cell count in a contingency table\n\nIf no observations at any intersection of the covariate and outcome\n\n \n\nZero cell in a contingency table should be detected in descriptive statistical analysis stage\n\n \n\nExample of one covariate with outcome:"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-example-13",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-example-13",
    "title": "Lesson 12: Numerical Problems",
    "section": "Zero cell in a contingency table: example (1/3)",
    "text": "Zero cell in a contingency table: example (1/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-example-23",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-example-23",
    "title": "Lesson 12: Numerical Problems",
    "section": "Zero cell in a contingency table: example (2/3)",
    "text": "Zero cell in a contingency table: example (2/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n    xTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n    xThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n  \n  \n  \n\n\n\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two\n2.79\n0.79, 10.5\n0.12\n        Three\n583,822,601\n0.00, NA\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-example-33",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-in-a-contingency-table-example-33",
    "title": "Lesson 12: Numerical Problems",
    "section": "Zero cell in a contingency table: example (3/3)",
    "text": "Zero cell in a contingency table: example (3/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n    xTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n    xThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n  \n  \n  \n\n\n\n\n \n\nCoefficient estimate is large and standard error is large! Estimated odds ratio is very large and confidence interval cannot be computed.\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two\n2.79\n0.79, 10.5\n0.12\n        Three\n583,822,601\n0.00, NA\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-13",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-13",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (1/3)",
    "text": "Zero cell count: example (1/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-23",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-23",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (2/3)",
    "text": "Zero cell count: example (2/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n    xTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n    xThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n  \n  \n  \n\n\n\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two\n2.79\n0.79, 10.5\n0.12\n        Three\n583,822,601\n0.00, NA\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-33",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#zero-cell-count-example-33",
    "title": "Lesson 13: Numerical Problems",
    "section": "Zero cell count: example (3/3)",
    "text": "Zero cell count: example (3/3)\n\n\n\nExample of logistic regression with one covariate:\n\n\nex1_m = glm(outcome ~ x, data = ex1, \n              family = binomial())\n\n\n\n\n\n\n\n\n\nCoefficient estimates\ntidy(ex1_m, conf.int=T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 32) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−0.62\n0.47\n−1.32\n0.19\n−1.60\n0.27\n    xTwo\n1.02\n0.65\n1.57\n0.12\n−0.23\n2.35\n    xThree\n20.19\n2,404.67\n0.01\n0.99\n−119.00\nNA\n  \n  \n  \n\n\n\n\n \n\nCoefficient estimate is large and standard error is large! Estimated odds ratio is very large and confidence interval cannot be computed.\n\n\n\n\nOdds ratio\ntbl_regression(ex1_m, exponentiate=T) %&gt;%\n  as_gt() %&gt;% # allows us to use tab_options()\n  tab_options(table.font.size = 38)\n\n\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      95% CI1\n      p-value\n    \n  \n  \n    x\n\n\n\n        One\n—\n—\n\n        Two\n2.79\n0.79, 10.5\n0.12\n        Three\n583,822,601\n0.00, NA\n&gt;0.9\n  \n  \n  \n    \n      1 OR = Odds Ratio, CI = Confidence Interval"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-12",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-12",
    "title": "Lesson 13: Numerical Problems",
    "section": "Decide on how to address zero cell (1/2)",
    "text": "Decide on how to address zero cell (1/2)\n\nLook at the proportions across the predictor, X:\n\n\nggplot(data = ex1, aes(x = x, fill = outcome)) + \n      geom_bar(stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\")"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-22",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#decide-on-how-to-address-zero-cell-22",
    "title": "Lesson 13: Numerical Problems",
    "section": "Decide on how to address zero cell (2/2)",
    "text": "Decide on how to address zero cell (2/2)\n\nLook at the proportions across the predictor, X:\n\n\nggplot(data = ex1, aes(x = x, fill = outcome)) + \n      geom_bar(stat = \"count\", position = \"fill\") +\n      labs(y = \"Proportion of Outcome\")\n\n\n\n\n\n\n\n\n\n \n\n\nCombining groups 2 and 3 together may not be a good idea.\nTheir proportions of the outcome do not look similar.\nThe predictor has an ordinal quality, so this is making me think a continuous approach might be good."
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-13",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-13",
    "title": "Lesson 12: Numerical Problems",
    "section": "Multicollinearity: example (1/3)",
    "text": "Multicollinearity: example (1/3)\n\nTable below is a simulated data with\n\n\\(x_1 \\sim \\text{Normal}(0,1)\\)\n\\(x_2 = x_1 + \\text{Uniform}(0,0.1)\\)\n\\(x_3 = 1 + \\text{Uniform}(0, 0.01)\\)\n\nTherefore, \\(x_1\\) and \\(x_2\\) are highly correlated, and \\(x_3\\) is nearly collinear with the constant term"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-13-1",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-13-1",
    "title": "Lesson 12: Numerical Problems",
    "section": "Multicollinearity: example (1/3)",
    "text": "Multicollinearity: example (1/3)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-14",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-14",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (1/4)",
    "text": "Multicollinearity: example (1/4)\n\nTable below is a simulated data with\n\n\\(x_1 \\sim \\text{Normal}(0,1)\\)\n\\(x_2 = x_1 + \\text{Uniform}(0,0.1)\\)\n\\(x_3 = 1 + \\text{Uniform}(0, 0.01)\\)\n\nTherefore, \\(x_1\\) and \\(x_2\\) are highly correlated, and \\(x_3\\) is nearly collinear with the constant term"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-24",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-24",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (2/4)",
    "text": "Multicollinearity: example (2/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-34",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-34",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (3/4)",
    "text": "Multicollinearity: example (3/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-44",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-example-44",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: example (4/4)",
    "text": "Multicollinearity: example (4/4)\n\nFour logistic regression models using data in the previous slide\nConsequence of multicollinearity: large coefficient estimates and/or standard errors"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-how-to-address-the-issue",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-how-to-address-the-issue",
    "title": "Lesson 12: Numerical Problems",
    "section": "Multicollinearity: how to address the issue",
    "text": "Multicollinearity: how to address the issue\n\nMulticollinearity only involves the covariates\n\nNo specific issues to logistic regression (vs. linear regression)\nTechniques from 512/612 work well for logistic regression model\n\n\n \n\nIn more complicated dataset/analysis, we may not be able to detect multicollinearity using the coefficient estimates/SE\n\n \n\nVariance inflation factor (VIF) approach: well-known approach to detect multicollinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-how-to-detect",
    "href": "lectures/13_Numeric_Problems/13_Numeric_problems.html#multicollinearity-how-to-detect",
    "title": "Lesson 13: Numerical Problems",
    "section": "Multicollinearity: how to detect",
    "text": "Multicollinearity: how to detect\n\nMulticollinearity only involves the covariates\n\nNo specific issues to logistic regression (vs. linear regression)\nTechniques from 512/612 work well for logistic regression model\n\n\n \n\nIn more complicated dataset/analysis, we may not be able to detect multicollinearity using the coefficient estimates/SE\n\n \n\nVariance inflation factor (VIF) approach: well-known approach to detect multicollinearity"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html",
    "href": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html",
    "title": "Class 13 Code",
    "section": "",
    "text": "library(car)\nlibrary(mctest)\nlibrary(epiDisplay)\nlibrary(kableExtra)\nlibrary(dplyr)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html#set-up-for-the-code",
    "href": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html#set-up-for-the-code",
    "title": "Class 13 Code",
    "section": "",
    "text": "library(car)\nlibrary(mctest)\nlibrary(epiDisplay)\nlibrary(kableExtra)\nlibrary(dplyr)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html#zero-cell-in-contingency-table",
    "href": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html#zero-cell-in-contingency-table",
    "title": "Class 13 Code",
    "section": "Zero cell in contingency table",
    "text": "Zero cell in contingency table\n\nOne covariate\n\nex1 = as.data.frame(rbind(cbind(outcome = rep(1, 39), x = c(rep(\"One\", 7), rep(\"Two\", 12), rep(\"Three\", 20))), \n      cbind(outcome = rep(0, 21), x = c(rep(\"One\", 13), rep(\"Two\", 8))))) %&gt;%\n  mutate(outcome = as.numeric(outcome), \n         x = factor(x, levels = c(\"One\", \"Two\", \"Three\")))\n\nRunning glm from the data\n\nex1_glm = glm(outcome ~ x, data = ex1, family = binomial())\nsummary(ex1_glm)\n\n\nCall:\nglm(formula = outcome ~ x, family = binomial(), data = ex1)\n\nCoefficients:\n             Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)   -0.6190     0.4688  -1.320    0.187\nxTwo           1.0245     0.6543   1.566    0.117\nxThree        20.1851  2404.6705   0.008    0.993\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.694  on 59  degrees of freedom\nResidual deviance: 52.818  on 57  degrees of freedom\nAIC: 58.818\n\nNumber of Fisher Scoring iterations: 18\n\nlogistic.display(ex1_glm)\n\n\nLogistic regression predicting outcome \n \n            OR(95%CI)            P(Wald's test) P(LR-test)\nx: ref.=One                                     &lt; 0.001   \n   Two      2.79 (0.77,10.04)    0.117                    \n   Three    583822600.8 (0,Inf)  0.993                    \n                                                          \nLog-likelihood = -26.4092\nNo. of observations = 60\nAIC value = 58.8183\n\n\nCombine groups 2 and 3:\n\nex1_23 = ex1 %&gt;% mutate(x = factor(x, levels = c(\"One\", \"Two\", \"Three\"), labels = c(\"One\", \"Two\", \"Two\")))\nex1_23_glm = glm(outcome ~ x, data = ex1_23, family = binomial())\nsummary(ex1_23_glm)\n\n\nCall:\nglm(formula = outcome ~ x, family = binomial(), data = ex1_23)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)   \n(Intercept)  -0.6190     0.4688   -1.32  0.18668   \nxTwo          2.0053     0.6132    3.27  0.00107 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.694  on 59  degrees of freedom\nResidual deviance: 65.930  on 58  degrees of freedom\nAIC: 69.93\n\nNumber of Fisher Scoring iterations: 4\n\n\nEliminate the third group:\n\nex1_two = ex1 %&gt;% filter(x != \"Three\")\nex1_two_glm = glm(outcome ~ x, data = ex1_two, family = binomial())\nsummary(ex1_two_glm)\n\n\nCall:\nglm(formula = outcome ~ x, family = binomial(), data = ex1_two)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)  -0.6190     0.4688  -1.320    0.187\nxTwo          1.0245     0.6543   1.566    0.117\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 55.352  on 39  degrees of freedom\nResidual deviance: 52.818  on 38  degrees of freedom\nAIC: 56.818\n\nNumber of Fisher Scoring iterations: 4\n\n\nTreat covariate as continuous:\n\nex1_cont = ex1 %&gt;% mutate(x = as.numeric(x))\ntable(ex1_cont$outcome, ex1_cont$x)\n\n   \n     1  2  3\n  0 13  8  0\n  1  7 12 20\n\nex1_cont_glm = glm(outcome ~ x, data = ex1_cont, family = binomial())\nsummary(ex1_cont_glm)\n\n\nCall:\nglm(formula = outcome ~ x, family = binomial(), data = ex1_cont)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -2.7201     0.8755  -3.107  0.00189 ** \nx             1.8285     0.4843   3.775  0.00016 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.694  on 59  degrees of freedom\nResidual deviance: 56.883  on 58  degrees of freedom\nAIC: 60.883\n\nNumber of Fisher Scoring iterations: 5\n\nlogistic.display(ex1_cont_glm)\n\n\nLogistic regression predicting outcome \n \n               OR(95%CI)          P(Wald's test) P(LR-test)\nx (cont. var.) 6.22 (2.41,16.08)  &lt; 0.001        &lt; 0.001   \n                                                           \nLog-likelihood = -28.4417\nNo. of observations = 60\nAIC value = 60.8834\n\n\n\n\nInteraction\n\noutcome = c(rep(1, 35), rep(0, 25))\nx1 = c(rep(1, 5), rep(0, 2), rep(1, 10), rep(0, 2), rep(1, 15), rep(0, 1),\n       rep(1, 5), rep(0, 8), rep(1, 2), rep(0, 6), rep(0, 4))\nx2 = c(rep(1, 7), rep(2, 12), rep(3, 16), \n       rep(1, 13), rep(2, 8), rep(3, 4))\nex2 = data.frame(outcome = outcome, x1 = x1, x2 = x2)"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html#complete-separation",
    "href": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html#complete-separation",
    "title": "Class 13 Code",
    "section": "Complete Separation",
    "text": "Complete Separation\n\ny = c(0,0,0,0,1,1,1,1)\nx1 = c(1,2,3,3,5,6,10,11)\nx2 = c(3,2,-1,-1,2,4,1,0)\nex3 = data.frame(outcome = y, x1 = x1, x2= x2)\nex3\n\n  outcome x1 x2\n1       0  1  3\n2       0  2  2\n3       0  3 -1\n4       0  3 -1\n5       1  5  2\n6       1  6  4\n7       1 10  1\n8       1 11  0\n\nm1 = glm(y ~ x1 + x2, family=binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(m1)\n\n\nCall:\nglm(formula = y ~ x1 + x2, family = binomial)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)\n(Intercept)    -66.098 183471.723   0.000        1\nx1              15.288  27362.843   0.001        1\nx2               6.241  81543.720   0.000        1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1.1090e+01  on 7  degrees of freedom\nResidual deviance: 4.5454e-10  on 5  degrees of freedom\nAIC: 6\n\nNumber of Fisher Scoring iterations: 24\n\nlibrary(logistf)\nm1_f = logistf(y ~ x1 + x2, family=binomial)\nsummary(m1_f)\n\nlogistf(formula = y ~ x1 + x2, family = binomial)\n\nModel fitted by Penalized ML\nCoefficients:\n                  coef  se(coef)   lower 0.95 upper 0.95     Chisq          p\n(Intercept) -2.9748898 1.7244237 -15.47721665 -0.1208883 4.2179522 0.03999841\nx1           0.4908484 0.2745754   0.05268216  2.1275832 5.0225056 0.02501994\nx2           0.4313732 0.4988396  -0.65793078  4.4758930 0.7807099 0.37692411\n            method\n(Intercept)      2\nx1               2\nx2               2\n\nMethod: 1-Wald, 2-Profile penalized log-likelihood, 3-None\n\nLikelihood ratio test=5.505687 on 2 df, p=0.06374636, n=8\nWald test = 3.624899 on 2 df, p = 0.1632538"
  },
  {
    "objectID": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html#collinearity",
    "href": "lectures/13_Numeric_Problems/old_work/Class_13_Code.html#collinearity",
    "title": "Class 13 Code",
    "section": "Collinearity",
    "text": "Collinearity\nBring in GLOW study again\n\nlibrary(aplore3)\nglow = glow500\nglow2 = glow %&gt;% mutate(raterisk2 = factor(raterisk, levels = c(\"Less\", \"Same\", \"Greater\"), \n                                            labels = c(\"Less and Same\", \"Less and Same\", \"Greater\"))) \n\nVIF\n\nmain_eff = glm(fracture ~ age + height + priorfrac + momfrac + \n                 armassist + raterisk2, \n                 data = glow2, family = binomial)\n\nglow_model = glm(fracture ~ age + height + priorfrac + momfrac + \n                   armassist + raterisk2 + weight + bmi, \n                 data = glow2, family = binomial)\n\nlibrary(car)\nvif(main_eff)\n\n      age    height priorfrac   momfrac armassist raterisk2 \n 1.169168  1.067010  1.118853  1.021981  1.103161  1.067098 \n\nvif(glow_model)\n\n       age     height  priorfrac    momfrac  armassist  raterisk2     weight \n  1.368259  19.561368   1.134230   1.028988   1.300983   1.120429 166.214541 \n       bmi \n152.406849 \n\n\n\nglow3 = glow2 %&gt;% mutate(height_sq = height^2)\nheight2 = glm(fracture ~ age + height + priorfrac + momfrac + \n                 armassist + raterisk2 + height_sq, \n                 data = glow3, family = binomial)\n# summary(height2)\nvif(height2)\n\n       age     height  priorfrac    momfrac  armassist  raterisk2  height_sq \n  1.173304 571.542009   1.120032   1.023053   1.101958   1.069178 571.172305 \n\n\nCentering height helps with the VIF:\n\nglow4 = glow2 %&gt;% mutate(height_c = (height - mean(height)), \n                         height_c_sq = height_c^2)\nheight_c_2 = glm(fracture ~ age + height_c + priorfrac + momfrac + \n                 armassist + raterisk2 + height_c_sq, \n                 data = glow4, family = binomial)\n# summary(height_c_2)\nvif(height_c_2)\n\n        age    height_c   priorfrac     momfrac   armassist   raterisk2 \n   1.173304    1.072721    1.120032    1.023053    1.101958    1.069178 \nheight_c_sq \n   1.006067"
  },
  {
    "objectID": "homework/HW4.html#question-2",
    "href": "homework/HW4.html#question-2",
    "title": "Homework 4",
    "section": "Question 2",
    "text": "Question 2\nThis question is adapted from the Hosmer and Lemeshow textbook, page 47. (Note that the parts are sited differently in the textbook.)\nUse the Myopia Study data described in Section 1.6.6 and use MYOPIC as the outcome and as possible variables for a model: AGE, GENDER, family history of myopia (MOMMY and DADMY), number of hours playing sports (SPORTHR) and number of hours watching television (TVHR).\n\nPart a\nWrite down the equation for the logistic regression model of MYOPIC on AGE, GENDER, MOMMY, DADMY, SPORTHR, and TVHR. How many parameters does this model contain?\n\n\nPart b\nUsing glm(), obtain the maximum likelihood estimates of the parameters of the logistic regression model. Using these estimates write down the equation for the fitted values (i.e., the estimated logistic probabilities).\n\n\nPart c\nAssess the significance of the coefficient corresponding to number of hours playing sports. You may use the Wald test or the likelihood ratio test. Please interpret the odds ratio for the coefficient (in this part, you do not need to calculate the confidence interval.)"
  },
  {
    "objectID": "homework/HW4.html#directions-1",
    "href": "homework/HW4.html#directions-1",
    "title": "Homework 4",
    "section": "Directions",
    "text": "Directions\n\nDownload the .qmd file here.\nYou will need to download the datasets. Use this link to download the homework datasets needed in this assignment. If you do not want to make changes to the paths set in this document, then make sure the files are stored in a folder named “data” that is housed in the same location as this homework .qmd file.\nPlease upload your homework to Sakai. Upload both your .qmd code file and the rendered .html file\n\nPlease rename you homework as Lastname_Firstinitial_HW0.qmd. This will help organize the homeworks when the TAs grade them.\nPlease also add the following line under subtitle: \"BSTA 512/612\": author: First-name Last-name with your first and last name so it is attached to the viewable document.\n\nFor each question, make sure to include all code and resulting output in the html file to support your answers.\nShow the work of your calculations using R code within a code chunk. Make sure that both your code and output are visible in the rendered html file. This is the default setting.\nIf you are computing something by hand, you may take a picture of your work and insert the image in this file. You may also use LaTeX to write it inline.\nWrite all answers in complete sentences as if communicating the results to a collaborator. This means including a sentence summarizing results in the context of the research study.\n\n\n\n\n\n\n\nTip\n\n\n\nIt is a good idea to try rendering your document from time to time as you go along! Note that rendering automatically saves your qmd file and rendering frequently helps you catch your errors more quickly."
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html",
    "href": "lectures/14_Model_building/14_Model_Building.html",
    "title": "Categorical Data Analysis",
    "section": "",
    "text": "https://github.com/tidyverse/datascience-box/tree/main/course-materials/_slides/u4-d07-prediction-overfitting\nhttps://datasciencebox.org/02-making-rigorous-conclusions"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#model-complexity-vs.-parsimony",
    "href": "lectures/14_Model_building/14_Model_Building.html#model-complexity-vs.-parsimony",
    "title": "Lesson 14: Model Building",
    "section": "Model Complexity vs. Parsimony",
    "text": "Model Complexity vs. Parsimony\n\n\nSuppose we have \\(p = 30\\) covariates (in the true model) and n = 50 observations. We could consider the following two alternatives:\n\nWe could fit a model using all of the covariates.\n\nIn this case, \\(\\widehat\\beta\\) is unbiased for \\(\\beta\\) (in a linear model fit using OLS). But \\(\\widehat\\beta\\) has very high variance.\n\nWe could fit a model using only the five strongest covariates.\n\nIn this case, \\(\\widehat\\beta\\) will be biased for \\(\\beta\\), but it will have lower variance (compared to the estimate including all covariates)\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#some-important-definitions",
    "href": "lectures/14_Model_building/14_Model_Building.html#some-important-definitions",
    "title": "Lesson 14: Model Building",
    "section": "Some important definitions",
    "text": "Some important definitions\n\nModel selection: picking the “best” model from a set of possible models\n\nModels will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n\n\n \n\nModel selection strategies: a process or framework that helps us pick our “best” model\n\nThese strategies often differ by the approach and criteria used to the determine the “best” model\n\n\n \n\nOverfitting: result of fitting a model so closely to our particular sample data that it cannot be generalized to other samples (or the population)"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#the-goals-of-association-vs.-prediction",
    "href": "lectures/14_Model_building/14_Model_Building.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 14: Model Building",
    "section": "The goals of association vs. prediction",
    "text": "The goals of association vs. prediction\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret odds ratios of the variable that is the focus of the study\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#hfnekl",
    "href": "lectures/14_Model_building/14_Model_Building.html#hfnekl",
    "title": "Lesson 14: Model Building",
    "section": "hfnekl",
    "text": "hfnekl\nhttps://github.com/tidyverse/datascience-box/tree/main/course-materials/_slides/u4-d07-prediction-overfitting\nhttps://datasciencebox.org/02-making-rigorous-conclusions\n\n\nLesson 14: Model Building"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#model-selection-strategies-for-continuous-outcomes",
    "href": "lectures/14_Model_building/14_Model_Building.html#model-selection-strategies-for-continuous-outcomes",
    "title": "Lesson 14: Model Building",
    "section": "Model selection strategies for continuous outcomes",
    "text": "Model selection strategies for continuous outcomes\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\nAutomated strategies\n\nStepwise selection (forward/backward)\n\nYou’ll see these a lot, but they’re not really good methods\n\nBest subset\nRegularization techniques (LASSO, Ridge, Elastic net)\n\n\n\n\n\n\n\nFor categorical outcomes, there are more prediction model selection strategies (will learn more in BSTA 513)\n\nExamples: Decision trees, Random forest, Neural networks, K-means"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#model-selection-strategies-for-categorical-outcomes",
    "href": "lectures/14_Model_building/14_Model_Building.html#model-selection-strategies-for-categorical-outcomes",
    "title": "Lesson 14: Model Building",
    "section": "Model selection strategies for categorical outcomes",
    "text": "Model selection strategies for categorical outcomes\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\n\n \n\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\n\n \n\nLogistic regression with more refined model selection\n\nRegularization techniques (LASSO, Ridge, Elastic net)\n\nMachine learning realm\n\nDecision trees, random forest, k-nearest neighbors, Neural networks"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#before-i-move-on",
    "href": "lectures/14_Model_building/14_Model_Building.html#before-i-move-on",
    "title": "Lesson 14: Model Building",
    "section": "Before I move on…",
    "text": "Before I move on…\n\nWe CAN use purposeful selection from last quarter in any type of generalized linear model (GLM)\n\nThis includes logistic regression!\n\n\n \n\nThe best documented information on purposeful selection is in the Hosmer-Lemeshow textbook on logistic regression\n\nTextbook in student files is linked here\nPurposeful selection starts on page 89 (or page 101 in the pdf)\n\n\n \n\nI will not discuss purposeful selection in this course\n\nBe aware that this is a tool that you can use in any regression!"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#okay-so-prediction-of-categorical-outcomes",
    "href": "lectures/14_Model_building/14_Model_Building.html#okay-so-prediction-of-categorical-outcomes",
    "title": "Lesson 14: Model Building",
    "section": "Okay, so prediction of categorical outcomes",
    "text": "Okay, so prediction of categorical outcomes\n\nClassification: process of predicting categorical responses/outcomes\n\nAssigning a category outcome based on an observation’s predictors\n\nCommon classification methods (good site on brief explanation of each)\n\nLogistic regression\nNaive Bayes\nk-Nearest Neighbor (KNN)\nDecision Trees\nSupport Vector Machines (SVMs)\nNeural Networks"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#bias-variance-trade-off",
    "href": "lectures/14_Model_building/14_Model_Building.html#bias-variance-trade-off",
    "title": "Lesson 14: Model Building",
    "section": "Bias-variance trade off",
    "text": "Bias-variance trade off\n\n\n\nRecall from 512/612: MSE can be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\n\nWe no longer use MSE in logistic regression to find the best fit model, BUT the idea between the bias and variance trade off holds!\n\nFor the same data:\n\nMore covariates in model: less bias, more variance\n\nPotential overfitting: with new data does our model still hold?\n\nLess covariates in model: more bias, less variance\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#logistic-regression-is-a-classification-method",
    "href": "lectures/14_Model_building/14_Model_Building.html#logistic-regression-is-a-classification-method",
    "title": "Lesson 14: Model Building",
    "section": "Logistic regression is a classification method",
    "text": "Logistic regression is a classification method\n\nBut to be a good classifier, our model needs to built a certain way\nPrediction depends on type of variable/model selection!\n\nThis is when it can become machine learning\n\nSo the big question is: how do we select this model??"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#poll-everywhere-question-1",
    "href": "lectures/14_Model_building/14_Model_Building.html#poll-everywhere-question-1",
    "title": "Lesson 14: Model Building",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#overview-of-the-process",
    "href": "lectures/14_Model_building/14_Model_Building.html#overview-of-the-process",
    "title": "Lesson 14: Model Building",
    "section": "Overview of the process",
    "text": "Overview of the process"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#splitting-data",
    "href": "lectures/14_Model_building/14_Model_Building.html#splitting-data",
    "title": "Lesson 14: Model Building",
    "section": "Splitting data",
    "text": "Splitting data\n\nLet’s keep this in context of the outcome\n\n\nggplot(glow1, aes(x = fracture)) + geom_bar()\n\nglow = glow1 %&gt;%\n    dplyr::select(-sub_id, -site_id, -phy_id, -age)"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#splitting-data-1",
    "href": "lectures/14_Model_building/14_Model_Building.html#splitting-data-1",
    "title": "Lesson 14: Model Building",
    "section": "Splitting data",
    "text": "Splitting data\n\nWant to split our data into training and testing sets\nStratify by fracture: because we have imbalanced data\n\n\nglow_split = initial_split(glow, strata = fracture)\nglow_split\n\n&lt;Training/Testing/Total&gt;\n&lt;374/126/500&gt;\n\nglow_train = training(glow_split)\nglow_test = testing(glow_split)"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#fitting-the-logistic-regression-model",
    "href": "lectures/14_Model_building/14_Model_Building.html#fitting-the-logistic-regression-model",
    "title": "Lesson 14: Model Building",
    "section": "Fitting the logistic regression model",
    "text": "Fitting the logistic regression model\n\nUse Lasso\n\nLasso with interactions!! Using interactions??\n\nlasso_mod = logistic_reg(penalty = 0.001, mixture = 1) %&gt;%\n            set_engine(\"glmnet\")"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#build-recipe",
    "href": "lectures/14_Model_building/14_Model_Building.html#build-recipe",
    "title": "Lesson 14: Model Building",
    "section": "build recipe??",
    "text": "build recipe??\n\nglow_rec = recipe(fracture ~ ., data = glow_train) %&gt;%\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk) \n\nglow_workflow = workflow() %&gt;%\n      add_model(lasso_mod) %&gt;% add_recipe(glow_rec)\n\n\nglow_folds = vfold_cv(glow_train, v = 5, strata = fracture)\n\nglow_fit_rs = glow_workflow %&gt;% \n      fit_resamples(glow_folds, control = control_resamples(save_pred=T))\n\nglow_train_metrics = collect_metrics(glow_fit_rs)\nglow_train_metrics\n\n# A tibble: 3 × 6\n  .metric     .estimator  mean     n std_err .config             \n  &lt;chr&gt;       &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    binary     0.754     5 0.0206  Preprocessor1_Model1\n2 brier_class binary     0.176     5 0.00902 Preprocessor1_Model1\n3 roc_auc     binary     0.675     5 0.0357  Preprocessor1_Model1\n\nglow_train_pred = collect_predictions(glow_fit_rs)\n\nglow_train_pred %&gt;%\n    group_by(id) %&gt;%\n    roc_curve(truth = fracture, .pred_No) %&gt;%\n    autoplot()\n\n\n\nglow_fit = glow_workflow %&gt;% fit(data = glow_train)\n\nglow_test_pred = predict(glow_fit, new_data = glow_test, type = \"prob\") %&gt;%\n    bind_cols(glow_test)\n\nglow_test_pred %&gt;% \n    roc_curve(truth = fracture, .pred_No) %&gt;%\n    autoplot()\n\n\n\nglow_test_pred %&gt;% \n    roc_auc(truth = fracture, .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.652\n\nglow_test_pred %&gt;% filter(fracture == \"No\", .pred_Yes &gt; 0.5)\n\n# A tibble: 7 × 14\n  .pred_No .pred_Yes priorfrac weight height   bmi premeno momfrac armassist\n     &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    \n1    0.493     0.507 Yes         54.4    158  21.8 Yes     No      No       \n2    0.488     0.512 Yes         58.5    155  24.3 No      No      Yes      \n3    0.298     0.702 Yes         54.9    159  21.7 No      Yes     Yes      \n4    0.433     0.567 No          99.8    153  42.6 Yes     No      Yes      \n5    0.499     0.501 No          50.8    150  22.6 No      No      Yes      \n6    0.368     0.632 Yes         55.3    152  23.9 No      No      Yes      \n7    0.429     0.571 No         113.     152  49.1 Yes     No      Yes      \n# ℹ 5 more variables: smoke &lt;fct&gt;, raterisk &lt;fct&gt;, fracscore &lt;int&gt;,\n#   fracture &lt;fct&gt;, age_c &lt;dbl&gt;\n\nglow_test_pred %&gt;% filter(fracture == \"Yes\", .pred_No &gt; 0.5)\n\n# A tibble: 27 × 14\n   .pred_No .pred_Yes priorfrac weight height   bmi premeno momfrac armassist\n      &lt;dbl&gt;     &lt;dbl&gt; &lt;fct&gt;      &lt;dbl&gt;  &lt;int&gt; &lt;dbl&gt; &lt;fct&gt;   &lt;fct&gt;   &lt;fct&gt;    \n 1    0.768     0.232 No          49.9    151  21.9 No      No      No       \n 2    0.598     0.402 Yes         88.5    158  35.5 No      No      Yes      \n 3    0.850     0.150 Yes         94.3    173  31.5 No      No      Yes      \n 4    0.551     0.449 No          60.3    148  27.5 Yes     No      Yes      \n 5    0.527     0.473 Yes         78.5    165  28.8 No      Yes     Yes      \n 6    0.822     0.178 Yes         54.4    165  20.0 No      No      No       \n 7    0.715     0.285 No          46.3    158  18.5 No      No      No       \n 8    0.579     0.421 Yes         63.5    165  23.3 Yes     No      Yes      \n 9    0.849     0.151 No          68      173  22.7 No      Yes     No       \n10    0.673     0.327 Yes         56.2    157  22.8 No      No      No       \n# ℹ 17 more rows\n# ℹ 5 more variables: smoke &lt;fct&gt;, raterisk &lt;fct&gt;, fracscore &lt;int&gt;,\n#   fracture &lt;fct&gt;, age_c &lt;dbl&gt;"
  },
  {
    "objectID": "lectures/14_Model_building/14_Model_Building.html#other-stufs",
    "href": "lectures/14_Model_building/14_Model_Building.html#other-stufs",
    "title": "Lesson 14: Model Building",
    "section": "Other stufs",
    "text": "Other stufs\n\nlibrary(vip)\n\nvi_data = glow_workflow %&gt;% \n    fit(glow_train) %&gt;%\n    pull_workflow_fit() %&gt;%\n    vi(lambda = 0.001) %&gt;%\n    filter(Importance != 0)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (1/2)",
    "text": "Review of model assessment so far (1/2)\n\nOverall measurements of fit\n\nHow well does the fitted logistic regression model predict the outcome?\nDifferent ways to measure the answer to this question\n\n\n\n\n\n\n\n\n\n\n\nMeasure of fit\nHypothesis tested?\nEquation\nR code\n\n\n\n\nPearson residual\nYes\n\\(X^2=\\sum_{j=1}^{J}{r\\left(Y_j,{\\hat{\\pi}}_j\\right)^2}\\)\nNot given\n\n\nHosmer-Lemeshow test\nYes\n\\(\\hat{C}=\\sum_{k=1}^{g}\\frac{\\left(o_k-n_k^\\prime{\\bar{\\pi}}_k\\right)^2}{n_k^\\prime{\\bar{\\pi}}_k(1-{\\bar{\\pi}}_k)}\\)\nhoslem.test()\n\n\nAUC-ROC\nKinda\nNot given\nauc(observed, predicted)\n\n\nAIC\nOnly to compare models\n\\(AIC = -2 \\cdot \\text{log-likelihood} + 2q\\)\nAIC(model_name)\n\n\nBIC\nOnly to compare models\n\\(BIC = -2 \\cdot \\text{log-likelihood} + q\\text{log}(n)\\)\nBIC(model_name)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-model-assessment-so-far-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of model assessment so far (2/2)",
    "text": "Review of model assessment so far (2/2)\n\nNumerical problems\n\nAssess pre and post model fit\nNumerical problems often depend on the final model (which variables and interactions are included)\n\nDifferent numerical problems to look out for\n\nZero cell count\nComplete separation\nMulticollinearity"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#today",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Today",
    "text": "Today\n\nWe now use model diagnostics to identify any observations that the model does not fit well"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#review-of-number-of-covariate-patterns",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Review of Number of Covariate Patterns",
    "text": "Review of Number of Covariate Patterns\n\nCovariate patterns are the unique covariate combinations that are observed\n\n \n\nFor example: model contains two binary covariates (history of fracture and smoking status), there will be 4 unique combination of these factors\n\nThis model has 4 covariate patterns\nSubjects can be divided into 4 groups based on the covariates’ values\n\n\n \n\nWhen we have continuous covariates, the number of covariate patterns will be close to the number of individuals in the dataset"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#overall-measure-to-diagnostics",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#overall-measure-to-diagnostics",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Overall measure to diagnostics",
    "text": "Overall measure to diagnostics\n\nNow we need to investigate diagnostics looking at individual data or covariate pattern data\n\nMake sure the overall measure has not been influenced by certain observations\n\n\n \n\nThe key quantities from logistic regression diagnostics are the components of “residual sum-of-squares”\n\nThe same idea as in the linear regression\nAssessed for each covariate pattern \\(j\\), by computing standardized Pearson residuals and Deviance residuals\n\nStandardization using \\(h_j\\), the leverage values"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-12",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values (1/2)",
    "text": "Hat Matrix and Leverage Values (1/2)\n\nWe have learned “hat” matrix and leverage values from linear regression diagnostics\nIn linear regression, the hat matrix projects the outcome variable onto the covariate space:\n\n$ H=X(XX){-1}X; =Hy$\nThe linear regression residuals is thus \\(y=\\widehat{y}\\), or \\((I-H)y\\)\n\nThe leverage is just the diagonal elements of the hat matrix, which is proportional to the distance of \\(x_j\\) to the mean of the data \\(\\overline{x}\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-22",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values (2/2)",
    "text": "Hat Matrix and Leverage Values (2/2)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-12",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (1/2)",
    "text": "Diagnostic Statistics Computation (1/2)\n\nTwo diagnostic statistics computation approach\n\nApproach 1: computed by covariate pattern\n\nRecommendation of Hosmer-Lemeshow textbook\nR uses this approach\nIdentify outliers as group that shares the same covariate values (in the same covariate pattern)\n\nApproach 2: individual observation approach\n\nSAS uses this approach\nIdentify outliers as individual\n\n\nWhy prefer covariate patterns approach?\n\nWhen the number of covariate pattern is much smaller than n, there is risk that we may fail to identify influential and/or poorly fit covariate patterns using individual based on residual"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostic-statistics-computation-22",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostic Statistics Computation (2/2)",
    "text": "Diagnostic Statistics Computation (2/2)\nConsider a covariate pattern with \\(m_j\\) subjects, all did not have event (some \\(y_i = 0\\)). So the estimated logistic probability is \\(\\widehat\\pi_j\\)\n\nPearson residual computed by individual \\[r_i=-\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nPearson residual computed by covariate pattern \\[r_i=-\\sqrt{m_j}\\sqrt{\\frac{{\\hat{\\pi}}_j}{(1-{\\hat{\\pi}}_j)}}\\]\nDifference between aboveresiduals will be large if \\(m_j\\) is large: usually a problem if less covariate patterns\n\nResidual from covariate pattern will identify poorly fit covariate patterns"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Diagnostics of Logistic Regression",
    "text": "Diagnostics of Logistic Regression\n\nModel diagnostics of logistic regression can be assessed by checking how influential a covariate pattern is:\n \n\nLook at change in residuals if a covariate pattern is excluded\n\nStandardized Pearson residual\nStandardized Deviance residual\n\n\n \n\nLook at change in coefficients if a covariate pattern is excluded"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-standardized-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Standardized Residuals",
    "text": "Change of Standardized Residuals\n\nChange in standardized Pearson Chi-square statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta X_j^2 = r_{sj}^2 = \\dfrac{r_j^2}{1-h_j}\\]\nDon’t need to know this: change in standardized deviance statistic due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta D_j = \\dfrac{d_j^2}{1-h_j}\\]\nRefer to Lesson 12: Assessing Model Fit for expression of Pearson residual"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#change-of-estimated-coefficients",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Change of Estimated Coefficients",
    "text": "Change of Estimated Coefficients\n\nChange in estimated coefficients due to deletion of subjects with covariate pattern \\(x_j\\): \\[\\Delta \\widehat{\\beta}_j = \\dfrac{r_j^2 h_j}{(1-h_j)^2}\\]\n\n \n\nThis is the logistic regression analog of Cook’s influence statistic (in linear regression)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression-i",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression (I)",
    "text": "Visual Assessment for Diagnostics of Logistic Regression (I)\n\nIn logistic regression, we mainly rely on graphical methods\n\nBecause the distribution of diagnostic measures under null hypothesis (that the model fits) is only known in certain limited settings\n\n\n \n\nFour plots for analysis of diagnostics in logistic regression:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(h_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-linear-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Linear regression",
    "text": "Hat Matrix and Leverage Values: Linear regression\n\nWe have learned “hat” matrix and leverage values from linear regression diagnostics\n\n \n\nIn linear regression, the hat matrix projects the outcome variable onto the covariate space:\n \n\n\\(H=X\\left(X^\\prime X\\right)^{-1}X^\\prime\\) and \\(\\hat{y}=Hy\\)\n\n \n\nThe linear regression residuals is thus \\(y - \\widehat{y}\\), or \\((I-H)y\\)\n\n\n \n\nThe leverage is just the diagonal elements of the hat matrix, which is proportional to the distance of \\(x_j\\) to the mean of the data \\(\\overline{x}\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#hat-matrix-and-leverage-values-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Hat Matrix and Leverage Values: Logistic regression",
    "text": "Hat Matrix and Leverage Values: Logistic regression\n\nIn logistic regression model, the hat matrix is: \\[H=V^\\frac{1}{2}X\\left(X^\\prime V\\ X\\right)^{-1}X^\\prime V^\\frac{1}{2}\\]\nThe leverage is \\[h_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\textbf{x}_j^\\prime\\left(\\textbf{X}^\\prime\\textbf{VX}\\right)^{-1}\\textbf{x}_j=v_j\\cdot b_j\\]\n\n\\(b\\): weighted distance of \\(x_j\\) from \\(\\overline{x}\\)\n\\(v_j\\): model based estimator of the variance of \\(y_j\\)\n\n\\(v_j=m_j\\cdot\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\left[1-\\hat{\\pi}\\left(\\textbf{x}_j\\right)\\right]\\)\n\n\n\\(h_j\\) reflects the relative influence of each covariate pattern on the model’s fit"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Recall the model we fit",
    "text": "Recall the model we fit"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#last-class-glow-study-with-interactions",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#last-class-glow-study-with-interactions",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Last Class: GLOW Study with interactions",
    "text": "Last Class: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nLesson 12: determined the overall fit of this model\nToday: determine the if any observations/covariate patterns that model does not fit well"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-do-we-get-these-values-in-r",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How do we get these values in R?",
    "text": "How do we get these values in R?\n\nNice function in the R script Logistic_Dx_Functions.R\n\nHighly suggest you save this R script for future use!!\n\n\n\nsource(here(\"lectures\", \"14_Model_diagnostics\", \"Logistic_Dx_Functions.R\"))\ndx_glow = dx(glow_m3)\nglimpse(dx_glow)\n\nRows: 71\nColumns: 16\n$ `(Intercept)`        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ priorfracYes         &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0…\n$ age_c                &lt;dbl&gt; 1, -7, 7, -2, 10, 20, 1, -2, 2, 8, 18, -8, 11, 10…\n$ `priorfracYes:age_c` &lt;dbl&gt; 1, 0, 7, 0, 10, 0, 0, -2, 2, 0, 0, 0, 0, 0, -3, 0…\n$ y                    &lt;dbl&gt; 2, 2, 3, 2, 2, 1, 3, 3, 1, 5, 1, 3, 2, 1, 1, 4, 1…\n$ P                    &lt;dbl&gt; 0.4088354, 0.1402159, 0.4162991, 0.1822879, 0.420…\n$ n                    &lt;int&gt; 5, 15, 7, 10, 5, 2, 12, 8, 3, 15, 2, 18, 7, 4, 3,…\n$ yhat                 &lt;dbl&gt; 2.0441770, 2.1032389, 2.9140936, 1.8228786, 2.100…\n$ Pr                   &lt;dbl&gt; -0.04018670, -0.07677228, 0.06586860, 0.14507476,…\n$ dr                   &lt;dbl&gt; -0.04023255, -0.07730975, 0.06577949, 0.14332786,…\n$ h                    &lt;dbl&gt; 0.008844090, 0.003811004, 0.008725450, 0.00290085…\n$ sPr                  &lt;dbl&gt; -0.04036559, -0.07691899, 0.06615786, 0.14528564,…\n$ sdr                  &lt;dbl&gt; -0.04041165, -0.07745749, 0.06606836, 0.14353620,…\n$ dChisq               &lt;dbl&gt; 0.001629381, 0.005916530, 0.004376863, 0.02110791…\n$ dDev                 &lt;dbl&gt; 0.001633102, 0.005999662, 0.004365028, 0.02060264…\n$ dBhat                &lt;dbl&gt; 1.453897e-05, 2.263418e-05, 3.852626e-05, 6.14091…"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-3",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#visual-assessment-for-diagnostics-of-logistic-regression",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Visual Assessment for Diagnostics of Logistic Regression",
    "text": "Visual Assessment for Diagnostics of Logistic Regression\n\nThe plots allow us to identify those covariate patterns that are…\n\nPoorly fit\n\nLarge values of \\(\\Delta X_j^2\\) (and/or \\(\\Delta D_j\\) if we looked at those)\n\nInfluential on estimated coefficients\n\nLarge values of \\(\\Delta\\widehat{\\beta}_j\\)\n\n\nIf you are interested to look at the contribution of leverage (ℎ_𝑗) to the values of the diagnostic statistic, you may also look at plots of:\n\n\\(\\Delta X_j^2\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta D_j\\) vs. \\({\\hat{\\pi}}_j\\)\n\\(\\Delta\\widehat{\\beta}_j\\) vs. \\({\\hat{\\pi}}_j\\)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-pearson-residuals-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Pearson residuals",
    "text": "GLOW study: standardized Pearson residuals\n\n\n\nGenerally, the points that curve from top left to bottom right of plot correspond to covariate patterns with \\(y_j = 1\\)\n\nOpposite corresponds to \\(y_j = 0\\)\n\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nWe may use 4 as a crude approximation to the upper 95th percentile for \\(\\Delta X_j^2\\)\n\n95th percentile of chi-squared distribution is 3.84\n\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dChisq &gt; 4) %&gt;% select(priorfracYes, age_c, P, dChisq)\n\n   priorfracYes age_c         P   dChisq\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0    -4 0.1643855 4.413937\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dChisq), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Pearson Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-standardized-deviance-residuals",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW study: standardized Deviance residuals",
    "text": "GLOW study: standardized Deviance residuals\n\n\n\nSame investigation as Pearson residuals\nPoints in the top left or top right corners identify the covariate patterns that are poorly fit\nUse 4 as a crude approximation to the upper 95th percentile\nWhich point is over 4?\n\n\ndx_glow %&gt;% filter(dDev &gt; 4) %&gt;% \n  select(priorfracYes, age_c, P, dDev)\n\n   priorfracYes age_c         P     dDev\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;    &lt;num&gt;\n1:            0   -10 0.1190935 4.841217\n2:            0     7 0.2812460 5.313540\n3:            1     6 0.4150524 4.325664\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dDev), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Std. Deviance Residual\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-change-in-coefficient-estimates",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Change in coefficient estimates",
    "text": "GLOW Study: Change in coefficient estimates\n\n\n\nBook recommends flagging certain covariate patterns if change in coefficient estimates are greater than 1\nAll values of \\(\\Delta\\widehat{\\beta}_j\\) are below 0.09\n\n\ndx_glow %&gt;% filter(dBhat &gt; 0.075) %&gt;% \n  select(priorfracYes, age_c, P, dBhat)\n\n   priorfracYes age_c         P      dBhat\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            1    20 0.4325984 0.08926472\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=dBhat), size = 3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Change in Coefficient Estimates\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#glow-study-leverage",
    "title": "Lesson 14: Model Diagnostics",
    "section": "GLOW Study: Leverage",
    "text": "GLOW Study: Leverage\n\n\n\nWe can use the same rule as linear regression: \\(h_j &gt; 3p/n\\)\n\nFlag these points as high leverage\n\nPoints with high leverage\n\n\\(p=4\\): four regression coefficients\n\\(n=500\\): 500 total observations\nLook for \\(h_j &gt; 3p/n = 3\\cdot4 /500 = 0.024\\)\n\n\n\ndx_glow %&gt;% filter(h &gt; 3*4/500) %&gt;% \n  select(priorfracYes, age_c, P, h) %&gt;% \n  head()\n\n   priorfracYes age_c         P          h\n          &lt;num&gt; &lt;num&gt;     &lt;num&gt;      &lt;num&gt;\n1:            0    20 0.4686423 0.02688958\n2:            1   -12 0.3928116 0.03186122\n3:            0    19 0.4531105 0.02451738\n4:            1   -11 0.3940365 0.02900675\n5:            1    19 0.4313389 0.02895824\n6:            1    18 0.4300804 0.02621708\n\n\n\n\n\nTo make the plot\nggplot(dx_glow) + geom_point(aes(x=P, y=h), size=3) + \n  xlab(\"Estimated/Predicted Probability of Fracture\") +\n  ylab(\"Leverage\") +\n  theme(text = element_text(size = 26))"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#find-out-the-influential-observation-from-the-data-set",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Find Out the “Influential” Observation From the Data Set",
    "text": "Find Out the “Influential” Observation From the Data Set\n\n\n\nWe identified covariate patterns that may be poorly fit or influential\n\n \n\nLet’s identify the covariate patterns that were not fit well\n\n\n\ndx_glow %&gt;% mutate(Cov_patt = 1:nrow(.)) %&gt;%\n  filter(dChisq &gt; 4 | dDev &gt; 4 | dBhat &gt; 1 | \n          h &gt; 3*4/500) %&gt;%\n  select(Cov_patt, y, P, h, dChisq, dDev, dBhat, h) %&gt;%\n  round(., 3)\n\n    Cov_patt     y     P     h dChisq  dDev dBhat\n       &lt;num&gt; &lt;num&gt; &lt;num&gt; &lt;num&gt;  &lt;num&gt; &lt;num&gt; &lt;num&gt;\n 1:        6     1 0.469 0.027  0.008 0.008 0.000\n 2:       22     1 0.393 0.032  0.046 0.047 0.002\n 3:       36     1 0.453 0.025  0.178 0.183 0.004\n 4:       43     0 0.119 0.005  2.581 4.841 0.012\n 5:       45     6 0.164 0.003  4.414 3.554 0.014\n 6:       47     0 0.281 0.006  3.148 5.314 0.018\n 7:       48     0 0.394 0.029  0.670 1.032 0.020\n 8:       49     2 0.431 0.029  0.698 0.693 0.021\n 9:       50     0 0.430 0.026  0.775 1.155 0.021\n10:       53     0 0.415 0.008  2.862 4.326 0.024\n11:       57     2 0.395 0.026  0.949 0.924 0.026\n12:       63     0 0.484 0.029  0.967 1.364 0.029\n13:       69     0 0.434 0.035  1.588 2.358 0.058\n14:       70     1 0.392 0.035  1.610 1.943 0.058\n15:       71     2 0.433 0.032  2.710 3.462 0.089"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#key-to-the-values",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Key to the values",
    "text": "Key to the values\n\n\n\ncolnames(dx_glow)\n\n [1] \"(Intercept)\"        \"priorfracYes\"       \"age_c\"             \n [4] \"priorfracYes:age_c\" \"y\"                  \"P\"                 \n [7] \"n\"                  \"yhat\"               \"Pr\"                \n[10] \"dr\"                 \"h\"                  \"sPr\"               \n[13] \"sdr\"                \"dChisq\"             \"dDev\"              \n[16] \"dBhat\"             \n\n\n\nFor each covariate pattern (which is each row) …\n\ny: Number of events\nP: Estimated probability of events\nn: Number of observations\nyhat: Estimated number of events\nPr: Pearson residual\ndr: Deviance\nh: leverage\nsPr: Standardized Pearson residual\nsdr: Standardized deviance\ndChisq: Change in standardized Pearson residual\ndDev: Change in standardized deviance\ndBhat: Change in coefficient estimates"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#three-potentials-when-model-fits-poorly",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#three-potentials-when-model-fits-poorly",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Three Potentials when model fits poorly",
    "text": "Three Potentials when model fits poorly\n\nIf only a few covariate pattern does not fit well (\\(y_j\\) differs from \\(m_j\\widehat\\pi_j\\) ), we are not too worried\n\nWe had 15 out of 71 covariate patterns\n\n\n \n\nIf quite a few covariate patterns do not fit well, potential reasons can be considered:\n\nThe link used in logistic regression model is not appropriate for outcome\n\nThis is usually unlikely, since logistic regression model is very flexible (think back to why we transformed our outcome from binary form)\n\nOne or more important covariates missing in the model\n\nAt least one of the covariates in the model has been entered in the wrong scale (think age-squared vs. age)\n\n\n\n \n\nCommon to do nothing after looking at the diagnostics\n\nIf data is feasible, we leave as is…"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#after-identifying-points",
    "title": "Lesson 14: Model Diagnostics",
    "section": "After identifying points",
    "text": "After identifying points\n\nDo a data quality check\n\nUnless you have a very good reason to believe the data are not measured correctly, then we leave it in\nCommon to do nothing\n\n\n \n\nIf only a few covariate pattern does not fit well (\\(y_j\\) differs from \\(m_j\\widehat\\pi_j\\) ), we are not too worried\n\nWe had 15 out of 71 covariate patterns\n\n\n \n\nIf quite a few covariate patterns do not fit well, potential reasons can be considered:\n\nThe link used in logistic regression model is not appropriate for outcome\n\nThis is usually unlikely, since logistic regression model is very flexible (think back to why we transformed our outcome from binary form)\n\nOne or more important covariates missing in the model\n\nAt least one of the covariates in the model has been entered in the wrong scale (think age-squared vs. age)"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#how-would-i-report-this-combining-all-model-assessment",
    "title": "Lesson 14: Model Diagnostics",
    "section": "How would I report this? (Combining all model assessment)",
    "text": "How would I report this? (Combining all model assessment)\n\nAssuming I have not checked other final models (no other models to compare AIC/BIC or AUC with)\n\nMethods: To assess the overall model fit, we calculated the AUC-ROC. We also calculated several model diagnostics including standardized Pearson residual, standardized deviance, change in coefficient estimates, and leverage. We identified covariate patterns with high standardized Pearson residual (greater than 4), standardized deviance (greater than 4), change in coefficient estimates (greater than 1), and leverage (greater than 0.024).\n \nResults: Our final logistic regression model consisted of the outcome, fracture, and predictors including prior fracture, age, and their interaction. The AUC-ROC was 0.68. We identified 11 covariate patterns with high leverage and 4 with high standardized Pearson residual, standardized deviance, or change in coefficient estimates. No identified observations were omitted.\n \nDiscussion:\n\nAUC-ROC low: Included covariates were pre-determined\nInfluential points were kept in because all observations were within feasible range of the predictors and outcome. (we could try age-sqaured and see if that helps AUC and/or diagnostics)\n\n\n\nLesson 14: Model Diagnostics"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#recall-the-model-we-fit-glow-study-with-interactions",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Recall the model we fit: GLOW Study with interactions",
    "text": "Recall the model we fit: GLOW Study with interactions\n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\nFitted model with interactions: \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\n \n\nLesson 12: determined the overall fit of this model\nToday: determine the if any observations/covariate patterns that model does not fit well"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#from-overall-measure-to-diagnostics",
    "title": "Lesson 14: Model Diagnostics",
    "section": "From overall measure to diagnostics",
    "text": "From overall measure to diagnostics\n\nNow we need to investigate diagnostics looking at individual data or covariate pattern data\n\nMake sure the overall measure has not been influenced by certain observations\n\n\n \n\nThe key quantities from logistic regression diagnostics are the components of “residual sum-of-squares”\n\nThe same idea as in the linear regression\nAssessed for each covariate pattern \\(j\\), by computing standardized Pearson residuals and Deviance residuals\n\nStandardization using \\(h_j\\), the leverage values"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-1",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "href": "lectures/14_Model_diagnostics/14_Model_diagnostics.html#poll-everywhere-question-2",
    "title": "Lesson 14: Model Diagnostics",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html",
    "href": "lectures/15_Model_building/15_Model_Building.html",
    "title": "Lesson 15: Model Building",
    "section": "",
    "text": "Understand the place of LASSO regression within association and prediction modeling for binary outcomes.\nRecognize the process for tidymodels\nUnderstand how penalized regression is a form of model/variable selection.\nPerform LASSO regression on a dataset using R and the general process for classification methods."
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#some-important-definitions",
    "href": "lectures/15_Model_building/15_Model_Building.html#some-important-definitions",
    "title": "Lesson 15: Model Building",
    "section": "Some important definitions",
    "text": "Some important definitions\n\nModel selection: picking the “best” model from a set of possible models\n\nModels will have the same outcome, but typically differ by the covariates that are included, their transformations, and their interactions\n“Best” model is defined by the research question and by how you want to answer it!\n\n\n \n\nModel selection strategies: a process or framework that helps us pick our “best” model\n\nThese strategies often differ by the approach and criteria used to the determine the “best” model\n\n\n \n\nOverfitting: result of fitting a model so closely to our particular sample data that it cannot be generalized to other samples (or the population)"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#bias-variance-trade-off",
    "href": "lectures/15_Model_building/15_Model_Building.html#bias-variance-trade-off",
    "title": "Lesson 15: Model Building",
    "section": "Bias-variance trade off",
    "text": "Bias-variance trade off\n\n\n\nRecall from 512/612: MSE can be written as a function of the bias and variance\n\\[\nMSE = \\text{bias}\\big(\\widehat\\beta\\big)^2 + \\text{variance}\\big(\\widehat\\beta\\big)\n\\]\n\nWe no longer use MSE in logistic regression to find the best fit model, BUT the idea between the bias and variance trade off holds!\n\nFor the same data:\n\nMore covariates in model: less bias, more variance\n\nPotential overfitting: with new data does our model still hold?\n\nLess covariates in model: more bias, less variance\n\nMore bias bc more likely that were are not capturing the true underlying relationship with less variables\n\n\n\n\n\n\n\nSource: http://scott.fortmann-roe.com/docs/BiasVariance.html"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#the-goals-of-association-vs.-prediction",
    "href": "lectures/15_Model_building/15_Model_Building.html#the-goals-of-association-vs.-prediction",
    "title": "Lesson 15: Model Building",
    "section": "The goals of association vs. prediction",
    "text": "The goals of association vs. prediction\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nGoal: Understand one variable’s (or a group of variable’s) effect on the response after adjusting for other factors\nMainly interpret odds ratios of the variable that is the focus of the study\n\n\n\n\n\n\nPrediction\n\n\n\nGoal: to calculate the most precise prediction of the response variable\nInterpreting coefficients is not important\nChoose only the variables that are strong predictors of the response variable\n\nExcluding irrelevant variables can help reduce widths of the prediction intervals"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#model-selection-strategies-for-categorical-outcomes",
    "href": "lectures/15_Model_building/15_Model_Building.html#model-selection-strategies-for-categorical-outcomes",
    "title": "Lesson 15: Model Building",
    "section": "Model selection strategies for categorical outcomes",
    "text": "Model selection strategies for categorical outcomes\n\n\n\n\nAssociation / Explanatory / One variable’s effect\n\n\n\nSelection of potential models is tied more with the research context with some incorporation of prediction scores\n\n \n\nPre-specification of multivariable model\nPurposeful model selection\n\n“Risk factor modeling”\n\nChange in Estimate (CIE) approaches\n\nWill learn in Survival Analysis (BSTA 514)\n\n\n\n\n\n\n\nPrediction\n\n\n\nSelection of potential models is fully dependent on prediction scores\n\n \n\nLogistic regression with more refined model selection\n\nRegularization techniques (LASSO, Ridge, Elastic net)\n\nMachine learning realm\n\nDecision trees, random forest, k-nearest neighbors, Neural networks"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#before-i-move-on",
    "href": "lectures/15_Model_building/15_Model_Building.html#before-i-move-on",
    "title": "Lesson 15: Model Building",
    "section": "Before I move on…",
    "text": "Before I move on…\n\nWe CAN use purposeful selection from last quarter in any type of generalized linear model (GLM)\n\nThis includes logistic regression!\n\n\n \n\nThe best documented information on purposeful selection is in the Hosmer-Lemeshow textbook on logistic regression\n\nTextbook in student files is linked here\nPurposeful selection starts on page 89 (or page 101 in the pdf)\n\n\n \n\nI will not discuss purposeful selection in this course\n\nBe aware that this is a tool that you can use in any regression!"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#okay-so-prediction-of-categorical-outcomes",
    "href": "lectures/15_Model_building/15_Model_Building.html#okay-so-prediction-of-categorical-outcomes",
    "title": "Lesson 15: Model Building",
    "section": "Okay, so prediction of categorical outcomes",
    "text": "Okay, so prediction of categorical outcomes\n\nClassification: process of predicting categorical responses/outcomes\n\nAssigning a category outcome based on an observation’s predictors\n\n\n \n\nNote: we’ve already done a lot of work around predicting probabilities within logistic regression\n\nCan we take those predicted probabilities one step further to predict the binary outcome??\n\n\n \n\nCommon classification methods (good site on brief explanation of each)\n\nLogistic regression\nNaive Bayes\nk-Nearest Neighbor (KNN)\nDecision Trees\nSupport Vector Machines (SVMs)\nNeural Networks"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#logistic-regression-is-a-classification-method",
    "href": "lectures/15_Model_building/15_Model_Building.html#logistic-regression-is-a-classification-method",
    "title": "Lesson 15: Model Building",
    "section": "Logistic regression is a classification method",
    "text": "Logistic regression is a classification method\n\nBut to be a good classifier, our logistic regression model needs to built a certain way\n\n \n\nPrediction depends on type of variable/model selection!\n\nThis is when it can become machine learning\n\n\n \n\nSo the big question is: how do we select this model??\n\nRegularized techniques, aka penalized regression"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#poll-everywhere-question-1",
    "href": "lectures/15_Model_building/15_Model_Building.html#poll-everywhere-question-1",
    "title": "Lesson 15: Model Building",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#overview-of-the-process",
    "href": "lectures/15_Model_building/15_Model_Building.html#overview-of-the-process",
    "title": "Lesson 15: Model Building",
    "section": "Overview of the process",
    "text": "Overview of the process\n\nSplit data into training and testing datasets\n\n \n\nPerform our classification method on training set\n\nThis is where we will use penalized regression!\n\n\n \n\nMeasure predictive accuracy on testing set"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#splitting-data",
    "href": "lectures/15_Model_building/15_Model_Building.html#splitting-data",
    "title": "Lesson 15: Model Building",
    "section": "Splitting data",
    "text": "Splitting data\n\nWhen splitting data, we need to be conscious of the proportions of our outcomes\n\nIs there imbalance within our outcome?\nWe want to randomly select observations but make sure the proportions of No and Yes stay the same\nWe stratify by the outcome, meaning we pick Yes’s and No’s separately for the training set\n\n\n\nggplot(glow1, aes(x = fracture)) + geom_bar()"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#splitting-data-1",
    "href": "lectures/15_Model_building/15_Model_Building.html#splitting-data-1",
    "title": "Lesson 15: Model Building",
    "section": "Splitting data",
    "text": "Splitting data\n\nFrom package rsample within tidyverse, we can use initial_split() to create training and testing data\n\nUse strata to stratify by fracture\n\n\n\nglow_split = initial_split(glow, strata = fracture, prop = 0.8)\nglow_split\n\n&lt;Training/Testing/Total&gt;\n&lt;400/100/500&gt;\n\n\n\nThen we can pull the training and testing data into their own datasets\n\n\nglow_train = training(glow_split)\nglow_test = testing(glow_split)"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#fitting-the-logistic-regression-model",
    "href": "lectures/15_Model_building/15_Model_Building.html#fitting-the-logistic-regression-model",
    "title": "Lesson 15: Model Building",
    "section": "Fitting the logistic regression model",
    "text": "Fitting the logistic regression model\n\nUsing Lasso penalized regression!\n\nLasso with interactions!! Using interactions??\n\nlasso_mod = logistic_reg(penalty = 0.001, mixture = 1) %&gt;% #mixture =1 means lasso\n            set_engine(\"glmnet\")"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#build-recipe",
    "href": "lectures/15_Model_building/15_Model_Building.html#build-recipe",
    "title": "Lesson 15: Model Building",
    "section": "build recipe??",
    "text": "build recipe??\n\nglow_rec_main = recipe(fracture ~ ., data = glow_train) %&gt;%\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk)\n  \nglow_workflow_main = workflow() %&gt;%\n      add_model(lasso_mod) %&gt;% add_recipe(glow_rec_main)\n  \nglow_fit_main = glow_workflow_main %&gt;% \n      fit(glow_train)\ntidy(glow_fit_main)\n\n# A tibble: 13 × 3\n   term             estimate penalty\n   &lt;chr&gt;               &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)      -3.19      0.001\n 2 weight           -0.0405    0.001\n 3 height            0         0.001\n 4 bmi               0.133     0.001\n 5 fracscore         0.191     0.001\n 6 age_c             0.00813   0.001\n 7 priorfrac_Yes     0.434     0.001\n 8 premeno_Yes       0.151     0.001\n 9 momfrac_Yes       0.482     0.001\n10 armassist_Yes    -0.120     0.001\n11 smoke_Yes        -0.605     0.001\n12 raterisk_Same     0.275     0.001\n13 raterisk_Greater  0.835     0.001\n\nlibrary(vip)  \nvi_data_main = glow_fit_main %&gt;% \n    pull_workflow_fit() %&gt;%\n    vi(lambda = 0.001) %&gt;%\n    filter(Importance != 0)\nvi_data_main\n\n# A tibble: 11 × 3\n   Variable         Importance Sign \n   &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;\n 1 raterisk_Greater    0.835   POS  \n 2 smoke_Yes           0.605   NEG  \n 3 momfrac_Yes         0.482   POS  \n 4 priorfrac_Yes       0.434   POS  \n 5 raterisk_Same       0.275   POS  \n 6 fracscore           0.191   POS  \n 7 premeno_Yes         0.151   POS  \n 8 bmi                 0.133   POS  \n 9 armassist_Yes       0.120   NEG  \n10 weight              0.0405  NEG  \n11 age_c               0.00813 POS"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#other-stufs",
    "href": "lectures/15_Model_building/15_Model_Building.html#other-stufs",
    "title": "Lesson 15: Model Building",
    "section": "Other stufs",
    "text": "Other stufs"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#hfnekl",
    "href": "lectures/15_Model_building/15_Model_Building.html#hfnekl",
    "title": "Lesson 15: Model Building",
    "section": "hfnekl",
    "text": "hfnekl\nhttps://github.com/tidyverse/datascience-box/tree/main/course-materials/_slides/u4-d07-prediction-overfitting\nhttps://datasciencebox.org/02-making-rigorous-conclusions\n\n\nLesson 15: Model Building"
  },
  {
    "objectID": "weeks/week_08_sched.html#muddiest-points-questions",
    "href": "weeks/week_08_sched.html#muddiest-points-questions",
    "title": "Week 8",
    "section": "Muddiest Points / Questions",
    "text": "Muddiest Points / Questions\n\n1. How did you determine the ages for the R output on slide 24 (standardized deviance residuals)\nThe centered ages are centered around the mean age. A few classes ago I mentioned that the mean was 69 years old, might have gotten lost in this lesson. So calculating the actual ages is just adding the mean age and centered age. So centered age of 6 is 69+6 = 75. Also, very confusing because apparently I can’t add!\n\n\n2. From comment on shrinkage vs. regularization vs. penalized methods\nAll these terms are used intercahngeably!\nPenalized regression means that penalty is added to our likelihood function! This may feel like a more generic form of shrinkage or regularization. However, within statistics, I do not see penalized regression used for anything other than minimizing the coefficient values towards zero. I often see it defined as: form of regression that uses a penalty to shrink coefficients towards zero.\nDefinitions of regularized regression mirror the above for penalized regression.\nShrinkage is more the action of reducing coefficient values towards zero. Many people will refer to regularization and penalized regression as shrinkage methods.\n\nLASSO, ridge, and Elastic net are all types of penalized/regularization/shrinkage methods\n\n\n\n3. Sign column within vi() output\nThe sign column is in fact the sign of the coefficient within the model.\nSo within our interaction model, the sign for smoking status is negative. Since smoking status had many interactions, we cannot make claims about the association between smoking and fracture without considering all other variables that it interacts with. ALSO, remember that our goal here is prediction, NOT association."
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#penalized-regression",
    "href": "lectures/15_Model_building/15_Model_Building.html#penalized-regression",
    "title": "Lesson 15: Model Building",
    "section": "Penalized regression",
    "text": "Penalized regression\n\nBasic idea: We are running regression, but now we want to incentivize our model fit to have less predictors\n\nInclude a penalty to discourage too many predictors in the model\n\n\n \n\nAlso known as shrinkage or regularization methods\n\n \n\nPenalty will reduce coefficient values to zero (or close to zero) if the predictor does not contribute much information to predicting our outcome\n\n \n\nWe need a tuning parameter that determines the amount of shrinkage called lambda/\\(\\lambda\\)\n\nHow much do we want to penalize additional predictors?"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#cross-validation",
    "href": "lectures/15_Model_building/15_Model_Building.html#cross-validation",
    "title": "Lesson 15: Model Building",
    "section": "Cross validation",
    "text": "Cross validation\n\nPrevents overfitting to one set of training data\nSplit data into folds that train and validate model selection\nBasically subsection of training and testing (called validating) before truly testing on our original testing set"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#splitting-our-data",
    "href": "lectures/15_Model_building/15_Model_Building.html#splitting-our-data",
    "title": "Lesson 15: Model Building",
    "section": "Splitting our data",
    "text": "Splitting our data\n\nTraining: act of creating our prediction model based on our observed data\n\nSupervised: Means we keep information on our outcome while training\n\n\n \n\nTesting: act of measuring the predictive accuracy of our model by trying it out on new data\n\n \n\nWhen we use data to create a prediction model, we want to test our prediction model on new data\n\nHelps make sure prediction model can be applied to other data outside of the data that was used to create it!\n\n\n \n\nSo an important first step in prediction modeling is to split our data into a training set and a testing set!"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#splitting-our-data-1",
    "href": "lectures/15_Model_building/15_Model_Building.html#splitting-our-data-1",
    "title": "Lesson 15: Model Building",
    "section": "Splitting our data",
    "text": "Splitting our data\n\n\n\n\nTraining set\n\n\n\nSandbox for model building\nSpend most of your time using the training set to develop the model\nMajority of the data (usually 80%)\n\n\n\n\n\n\nTesting set\n\n\n\nHeld in reserve to determine efficacy of one or two chosen models\nCritical to look at it once, otherwise it becomes part of the modeling process\nRemainder of the data (usually 20%)\n\n\n\n\n\n     \n \n\nSlide content from Data Science in a Box"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#example-to-be-used-glow-study",
    "href": "lectures/15_Model_building/15_Model_Building.html#example-to-be-used-glow-study",
    "title": "Lesson 15: Model Building",
    "section": "Example to be used: GLOW Study",
    "text": "Example to be used: GLOW Study\n\nFrom GLOW (Global Longitudinal Study of Osteoporosis in Women) study\n\n \n\nOutcome variable: any fracture in the first year of follow up (FRACTURE: 0 or 1)\n\n \n\nRisk factor/variable of interest: history of prior fracture (PRIORFRAC: 0 or 1)\nPotential confounder or effect modifier: age (AGE, a continuous variable)\n\nCenter age will be used! We will center around the rounded mean age of 69 years old\n\n\n \n\nCrossed out because we are no longer attached to specific predictors and their association with fracture\n\nFocused on predicting fracture with whatever variables we can!"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#splitting-data-peek-at-the-split",
    "href": "lectures/15_Model_building/15_Model_Building.html#splitting-data-peek-at-the-split",
    "title": "Lesson 15: Model Building",
    "section": "Splitting data: peek at the split",
    "text": "Splitting data: peek at the split\n\n\n\nglimpse(glow_train)\n\nRows: 400\nColumns: 12\n$ priorfrac &lt;fct&gt; No, No, Yes, No, Yes, No, Yes, No, No, No, No, No, No, No, N…\n$ weight    &lt;dbl&gt; 70.3, 87.1, 50.8, 62.1, 68.0, 50.8, 40.8, 63.5, 117.9, 67.1,…\n$ height    &lt;int&gt; 158, 160, 157, 160, 161, 150, 153, 166, 167, 162, 165, 160, …\n$ bmi       &lt;dbl&gt; 28.16055, 34.02344, 20.60936, 24.25781, 26.23356, 22.57778, …\n$ premeno   &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ momfrac   &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ armassist &lt;fct&gt; No, No, Yes, No, No, No, No, No, Yes, No, Yes, No, Yes, No, …\n$ smoke     &lt;fct&gt; No, No, No, No, Yes, No, No, No, Yes, Yes, No, No, No, No, N…\n$ raterisk  &lt;fct&gt; Same, Same, Less, Less, Same, Less, Same, Less, Same, Less, …\n$ fracscore &lt;int&gt; 1, 2, 11, 5, 4, 6, 7, 0, 3, 1, 5, 1, 4, 2, 2, 7, 2, 1, 4, 5,…\n$ fracture  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ age_c     &lt;dbl&gt; -7, -4, 19, 13, -2, 15, 13, -11, -13, -10, 3, -5, -1, -2, 0,…\n\n\n\n\nglimpse(glow_test)\n\nRows: 100\nColumns: 12\n$ priorfrac &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, No, Yes, No, No, No, No…\n$ weight    &lt;dbl&gt; 68.0, 62.6, 67.6, 88.0, 58.5, 69.9, 58.5, 62.1, 93.0, 59.0, …\n$ height    &lt;int&gt; 152, 156, 153, 158, 165, 153, 175, 158, 163, 157, 152, 160, …\n$ bmi       &lt;dbl&gt; 29.43213, 25.72321, 28.87778, 35.25076, 21.48760, 29.86031, …\n$ premeno   &lt;fct&gt; No, No, No, Yes, No, No, No, No, No, Yes, No, No, No, Yes, N…\n$ momfrac   &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, No, No, No, No, No,…\n$ armassist &lt;fct&gt; No, No, No, No, No, Yes, No, No, Yes, Yes, No, No, Yes, No, …\n$ smoke     &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, Yes, No, No, No, No, No…\n$ raterisk  &lt;fct&gt; Same, Same, Less, Greater, Same, Same, Greater, Less, Less, …\n$ fracscore &lt;int&gt; 1, 7, 4, 1, 3, 8, 3, 5, 4, 5, 7, 1, 3, 1, 4, 2, 2, 4, 1, 3, …\n$ fracture  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ age_c     &lt;dbl&gt; -8, 17, -2, -8, 2, 17, 2, 11, -3, -4, 17, -9, -5, -6, 7, -7,…"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#before-i-get-really-into-things",
    "href": "lectures/15_Model_building/15_Model_Building.html#before-i-get-really-into-things",
    "title": "Lesson 15: Model Building",
    "section": "Before I get really into things!!",
    "text": "Before I get really into things!!\n\ntidymodels is a great package when we are performing prediction\nOne problem: it uses very different syntax for model fitting than we are used to…\ntidymodels syntax dictates that we need to define:\n\nA model\nA recipe\nA workflow"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#tidymodels-with-glow",
    "href": "lectures/15_Model_building/15_Model_Building.html#tidymodels-with-glow",
    "title": "Lesson 15: Model Building",
    "section": "tidymodels with GLOW",
    "text": "tidymodels with GLOW\nTo fit our logistic regression model with the interaction between age and prior fracture, we use:\n\n# model\nmodel = logistic_reg()\n# recipe\nrecipe = recipe(fracture ~ priorfrac + age_c, data = glow1) %&gt;%\n            step_dummy(priorfrac) %&gt;%\n            step_interact(terms = ~ age_c:starts_with(\"priorfrac\"))\n# workflow\nworkflow = workflow() %&gt;% add_model(model) %&gt;% add_recipe(recipe)\n\nfit = workflow %&gt;% fit(data = glow1)\n\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.376\n0.134\n−10.270\n0.000\n−1.646\n−1.120\n    age_c\n0.063\n0.015\n4.043\n0.000\n0.032\n0.093\n    priorfrac_Yes\n1.002\n0.240\n4.184\n0.000\n0.530\n1.471\n    age_c_x_priorfrac_Yes\n−0.057\n0.025\n−2.294\n0.022\n−0.107\n−0.008"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#section",
    "href": "lectures/15_Model_building/15_Model_Building.html#section",
    "title": "Lesson 15: Model Building",
    "section": "",
    "text": "glow_rec_int = recipe(fracture ~ ., data = glow_train) %&gt;%\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk) %&gt;%\n  step_interact(terms = ~ all_predictors():all_predictors())\n\nglow_workflow_int = workflow() %&gt;%\n      add_model(lasso_mod) %&gt;% add_recipe(glow_rec_int)\n  \nglow_fit_int = glow_workflow_int %&gt;% fit(glow_train)\ntidy(glow_fit_int)\n\n# A tibble: 79 × 3\n   term          estimate penalty\n   &lt;chr&gt;            &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)    -4.16     0.001\n 2 weight          0        0.001\n 3 height          0        0.001\n 4 bmi             0.165    0.001\n 5 fracscore       0        0.001\n 6 age_c           0        0.001\n 7 priorfrac_Yes   0.0371   0.001\n 8 premeno_Yes     1.21     0.001\n 9 momfrac_Yes     0        0.001\n10 armassist_Yes   0        0.001\n# ℹ 69 more rows\n\nvi_data_int = glow_fit_int %&gt;%\n    pull_workflow_fit() %&gt;%\n    vi(lambda = 0.001) %&gt;%\n    filter(Importance != 0)\nvi_data_int\n\n# A tibble: 46 × 3\n   Variable                         Importance Sign \n   &lt;chr&gt;                                 &lt;dbl&gt; &lt;chr&gt;\n 1 smoke_Yes_x_raterisk_Greater          2.54  POS  \n 2 priorfrac_Yes_x_momfrac_Yes           1.82  NEG  \n 3 momfrac_Yes_x_armassist_Yes           1.56  NEG  \n 4 smoke_Yes                             1.50  NEG  \n 5 priorfrac_Yes_x_premeno_Yes           1.25  NEG  \n 6 premeno_Yes                           1.21  POS  \n 7 premeno_Yes_x_armassist_Yes           1.18  POS  \n 8 priorfrac_Yes_x_raterisk_Greater      0.921 POS  \n 9 premeno_Yes_x_raterisk_Greater        0.792 POS  \n10 momfrac_Yes_x_raterisk_Same           0.758 POS  \n# ℹ 36 more rows"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#three-types-of-penalized-regression",
    "href": "lectures/15_Model_building/15_Model_Building.html#three-types-of-penalized-regression",
    "title": "Lesson 15: Model Building",
    "section": "Three types of penalized regression",
    "text": "Three types of penalized regression\nMain difference is the type of penalty used\n\n\n\n\nRidge regression\n\n\n\nPenalty called L2 norm, uses sqaured values\nPros\n\nReduces overfitting\nHandles \\(p&gt;n\\)\nHandles collinearity\n\nCons\n\nDoes not shrink coefficients to 0\nDifficult to interpret\n\n\n\n\n\n\n\nLasso regression\n\n\n\nPenalty called L1 norm, uses absolute values\n\n \n\nPros\n\nReduces overfitting\nShrinks coefficients to 0\n\nCons\n\nCannot handle \\(p&gt;n\\)\nDoes not handle multicollinearity well\n\n\n\n\n\n\n\nElastic net regression\n\n\n\nL1 and L2 used, best of both worlds\nPros\n\nReduces overfitting\nHandles \\(p&gt;n\\)\nHandles collinearity\nShrinks coefficients to 0\n\nCons\n\nMore difficult to do than other two"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data",
    "text": "Step 1: Splitting data\n\nTraining: act of creating our prediction model based on our observed data\n\nSupervised: Means we keep information on our outcome while training\n\n\n \n\nTesting: act of measuring the predictive accuracy of our model by trying it out on new data\n\n \n\nWhen we use data to create a prediction model, we want to test our prediction model on new data\n\nHelps make sure prediction model can be applied to other data outside of the data that was used to create it!\n\n\n \n\nSo an important first step in prediction modeling is to split our data into a training set and a testing set!"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data-1",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data-1",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data",
    "text": "Step 1: Splitting data\n\n\n\n\nTraining set\n\n\n\nSandbox for model building\nSpend most of your time using the training set to develop the model\nMajority of the data (usually 80%)\n\n\n\n\n\n\nTesting set\n\n\n\nHeld in reserve to determine efficacy of one or two chosen models\nCritical to look at it once at the end, otherwise it becomes part of the modeling process\nRemainder of the data (usually 20%)\n\n\n\n\n\n     \n \n\nSlide content from Data Science in a Box"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data-2",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data-2",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data",
    "text": "Step 1: Splitting data\n\nWhen splitting data, we need to be conscious of the proportions of our outcomes\n\nIs there imbalance within our outcome?\nWe want to randomly select observations but make sure the proportions of No and Yes stay the same\nWe stratify by the outcome, meaning we pick Yes’s and No’s separately for the training set\n\n\n\nggplot(glow1, aes(x = fracture)) + geom_bar()\n\n\n\n\n\nSide note: took out bmi and weight bc we have multicollinearity issues\n\nCombo of I hate these variables and my previous work in the LASSO identified these as not important\n\n\n\nglow = glow1 %&gt;%\n    dplyr::select(-sub_id, -site_id, -phy_id, -age, -bmi, -weight)"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data-3",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data-3",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data",
    "text": "Step 1: Splitting data\n\nFrom package rsample within tidyverse, we can use initial_split() to create training and testing data\n\nUse strata to stratify by fracture\n\n\n\nglow_split = initial_split(glow, strata = fracture, prop = 0.8)\nglow_split\n\n&lt;Training/Testing/Total&gt;\n&lt;400/100/500&gt;\n\n\n\nThen we can pull the training and testing data into their own datasets\n\n\nglow_train = training(glow_split)\nglow_test = testing(glow_split)"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data-peek-at-the-split",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-1-splitting-data-peek-at-the-split",
    "title": "Lesson 15: Model Building",
    "section": "Step 1: Splitting data: peek at the split",
    "text": "Step 1: Splitting data: peek at the split\n\n\n\nglimpse(glow_train)\n\nRows: 400\nColumns: 10\n$ priorfrac &lt;fct&gt; No, No, Yes, No, No, Yes, No, Yes, Yes, No, No, No, No, No, …\n$ height    &lt;int&gt; 158, 160, 157, 160, 152, 161, 150, 153, 156, 166, 153, 160, …\n$ premeno   &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, Yes, No, No, No,…\n$ momfrac   &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No…\n$ armassist &lt;fct&gt; No, No, Yes, No, No, No, No, No, No, No, No, No, Yes, No, No…\n$ smoke     &lt;fct&gt; No, No, No, No, No, Yes, No, No, No, No, Yes, No, No, No, No…\n$ raterisk  &lt;fct&gt; Same, Same, Less, Less, Same, Same, Less, Same, Same, Less, …\n$ fracscore &lt;int&gt; 1, 2, 11, 5, 1, 4, 6, 7, 7, 0, 4, 1, 4, 2, 2, 7, 2, 1, 4, 5,…\n$ fracture  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ age_c     &lt;dbl&gt; -7, -4, 19, 13, -8, -2, 15, 13, 17, -11, -2, -5, -1, -2, 0, …\n\n\n\n\nglimpse(glow_test)\n\nRows: 100\nColumns: 10\n$ priorfrac &lt;fct&gt; No, No, No, No, No, No, No, No, Yes, Yes, No, No, No, No, No…\n$ height    &lt;int&gt; 167, 162, 165, 158, 153, 170, 154, 171, 142, 152, 166, 154, …\n$ premeno   &lt;fct&gt; No, No, No, Yes, No, Yes, Yes, Yes, Yes, No, No, No, No, No,…\n$ momfrac   &lt;fct&gt; No, No, No, No, No, Yes, No, No, Yes, No, No, No, No, No, No…\n$ armassist &lt;fct&gt; Yes, No, Yes, No, Yes, No, Yes, No, No, No, No, No, No, No, …\n$ smoke     &lt;fct&gt; Yes, Yes, No, No, No, No, No, No, No, No, No, No, No, No, No…\n$ raterisk  &lt;fct&gt; Same, Less, Less, Greater, Same, Same, Same, Same, Same, Sam…\n$ fracscore &lt;int&gt; 3, 1, 5, 1, 8, 3, 7, 1, 6, 7, 0, 2, 0, 0, 1, 2, 2, 8, 4, 3, …\n$ fracture  &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No, No, …\n$ age_c     &lt;dbl&gt; -13, -10, 3, -8, 17, 0, 6, -5, 1, 17, -11, -6, -10, -12, -6,…"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-penalized-logistic-regression-model",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-penalized-logistic-regression-model",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO penalized logistic regression model",
    "text": "Step 2: Fit LASSO penalized logistic regression model\n\nUsing Lasso penalized regression!\nWe can simply set up a penalized regression model\n\n \n\nlasso_mod = logistic_reg(penalty = 0.001, mixture = 1) %&gt;%\n\n            set_engine(\"glmnet\")\n\n\nglmnet takes the basic fitting of glm and adds penalties!\n\nIn tidymodels we set an engine that will fit the model\n\nmixture option let’s us pick the penalty\n\nmixture = 0 for Ridge regression\nmixture = 1 for Lasso regression\n0 &lt; mixture &lt; 1 for Elastic net regression"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-3-prediction-on-testing-set",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-3-prediction-on-testing-set",
    "title": "Lesson 15: Model Building",
    "section": "Step 3: Prediction on testing set",
    "text": "Step 3: Prediction on testing set\n\nglow_test_pred = predict(glow_fit_int2, new_data = glow_test, type = \"prob\") %&gt;%\n    bind_cols(glow_test)\n\n\n\n\nglow_test_pred %&gt;% \n  roc_auc(truth = fracture, \n                  .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.644\n\n\n\n\nglow_test_pred %&gt;% \n  roc_curve(truth = fracture, .pred_No) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Main effects",
    "text": "Step 2: Fit LASSO: Main effects\n\nglow_rec_main = recipe(fracture ~ ., data = glow_train) %&gt;%\n\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk)\n\nglow_workflow_main = workflow() %&gt;%\n\n      add_model(lasso_mod) %&gt;% add_recipe(glow_rec_main)\n  \nglow_fit_main = glow_workflow_main %&gt;% fit(glow_train)"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-1",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-1",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Main effects",
    "text": "Step 2: Fit LASSO: Main effects\n\nWe want to include interactions in our regression\nThe main effect model will be our starting point\n\nOtherwise, we may drop main effects but not their interactions\nCannot do that: see hierarchy principle\n\nSo we look at the main effects and build from there"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-add-interactions",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-add-interactions",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Add interactions",
    "text": "Step 2: Fit LASSO: Add interactions\n\nglow_rec_int = recipe(fracture ~ ., data = glow_train) %&gt;%\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk) %&gt;%\n  step_interact(terms = ~ all_predictors():all_predictors())\n\nglow_workflow_int = workflow() %&gt;%\n      add_model(lasso_mod) %&gt;% add_recipe(glow_rec_int)\n  \nglow_fit_int = glow_workflow_int %&gt;% fit(glow_train)\n\nvi_data_int = glow_fit_int %&gt;%\n    pull_workflow_fit() %&gt;%\n    vi(lambda = 0.001) %&gt;%\n    filter(Importance != 0)\nvi_data_int\n\n# A tibble: 46 × 3\n   Variable                     Importance Sign \n   &lt;chr&gt;                             &lt;dbl&gt; &lt;chr&gt;\n 1 priorfrac_Yes_x_premeno_Yes        2.45 NEG  \n 2 premeno_Yes_x_smoke_Yes            2.40 NEG  \n 3 armassist_Yes_x_smoke_Yes          2.22 POS  \n 4 premeno_Yes                        2.06 POS  \n 5 momfrac_Yes_x_smoke_Yes            2.03 NEG  \n 6 smoke_Yes_x_raterisk_Greater       1.96 POS  \n 7 momfrac_Yes_x_armassist_Yes        1.43 NEG  \n 8 smoke_Yes_x_raterisk_Same          1.40 POS  \n 9 premeno_Yes_x_armassist_Yes        1.34 POS  \n10 momfrac_Yes_x_raterisk_Same        1.07 POS  \n# ℹ 36 more rows"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-2",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-2",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Main effects",
    "text": "Step 2: Fit LASSO: Main effects\n\nWe want to include interactions in our regression\nThe main effect model will be our starting point\n\nOtherwise, we may drop main effects but not their interactions\nCannot do that: see hierarchy principle\n\nSo we look at the main effects and build from there"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Identify interactions",
    "text": "Step 2: Fit LASSO: Identify interactions\n\nvi_data_int = glow_fit_int %&gt;%\n    pull_workflow_fit() %&gt;%\n    vi(lambda = 0.001) %&gt;%\n    filter(Importance != 0)\nvi_data_int\n\n# A tibble: 34 × 3\n   Variable                       Importance Sign \n   &lt;chr&gt;                               &lt;dbl&gt; &lt;chr&gt;\n 1 smoke_Yes                            4.29 NEG  \n 2 smoke_Yes_x_raterisk_Greater         3.89 POS  \n 3 smoke_Yes_x_raterisk_Same            3.14 POS  \n 4 premeno_Yes_x_smoke_Yes              3.00 NEG  \n 5 momfrac_Yes_x_armassist_Yes          2.82 NEG  \n 6 priorfrac_Yes_x_premeno_Yes          2.50 NEG  \n 7 priorfrac_Yes                        1.82 POS  \n 8 armassist_Yes_x_smoke_Yes            1.44 POS  \n 9 premeno_Yes_x_raterisk_Greater       1.31 POS  \n10 momfrac_Yes_x_smoke_Yes              1.17 POS  \n# ℹ 24 more rows\n\n\n\nThis is where things got a little annoying for me…"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-identify-variables",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-identify-variables",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Main effects: Identify variables",
    "text": "Step 2: Fit LASSO: Main effects: Identify variables\n\nlibrary(vip)  \nvi_data_main = glow_fit_main %&gt;% \n    pull_workflow_fit() %&gt;%\n    vi(lambda = 0.001) %&gt;%\n    filter(Importance != 0)\nvi_data_main\n\n# A tibble: 9 × 3\n  Variable         Importance Sign \n  &lt;chr&gt;                 &lt;dbl&gt; &lt;chr&gt;\n1 raterisk_Greater     0.559  POS  \n2 momfrac_Yes          0.542  POS  \n3 priorfrac_Yes        0.493  POS  \n4 raterisk_Same        0.438  POS  \n5 smoke_Yes            0.376  NEG  \n6 premeno_Yes          0.285  POS  \n7 fracscore            0.197  POS  \n8 armassist_Yes        0.146  POS  \n9 height               0.0382 NEG  \n\n\n\nLooks like age is removed!"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-interactions",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-main-effects-interactions",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Main effects + interactions",
    "text": "Step 2: Fit LASSO: Main effects + interactions\n\nWe want to include interactions in our regression\nThe main effect model will be our starting point\n\nOtherwise, we may drop main effects but not their interactions\nCannot do that: see hierarchy principle\n\nI remove age_c from this section because main effects did not include it\n\n\nglow_rec_int = recipe(fracture ~ ., data = glow_train) %&gt;%\n  update_role(age_c, new_role = \"dont_use\") %&gt;%\n\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk) %&gt;%\n\n  step_interact(terms = ~ all_predictors():all_predictors())\n\nglow_workflow_int = workflow() %&gt;%\n      add_model(lasso_mod) %&gt;% add_recipe(glow_rec_int)\n  \nglow_fit_int = glow_workflow_int %&gt;% fit(glow_train)"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions-2",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions-2",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Identify interactions",
    "text": "Step 2: Fit LASSO: Identify interactions\n\nI combed through the column names of the results to find the interactions\n\nI used ChatGPT to help me because I’m pretty new to tidymodels: let’s view what I asked\n\n\n\ninteractions = vi_data_int %&gt;% filter(grepl(\"_x_\", Variable)) %&gt;%\n                select(Variable) %&gt;% separate(Variable, \"_x_\")\n                \n\ninteraction_terms = ~ (all_predictors()^2) - #Makes interactions b/w all predictors\n                      fracscore:starts_with(\"premeno\") - # Removes this interaction\n                      height:starts_with(\"premeno\") - \n                      height:starts_with(\"smoke\") - \n                      height:starts_with(\"momfrac\")"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions-1",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-identify-interactions-1",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Identify interactions",
    "text": "Step 2: Fit LASSO: Identify interactions\n\nI combed through the column names of the results to find the interactions\n\n\nvi_data_int$Variable\n\n [1] \"smoke_Yes\"                        \"smoke_Yes_x_raterisk_Greater\"    \n [3] \"smoke_Yes_x_raterisk_Same\"        \"premeno_Yes_x_smoke_Yes\"         \n [5] \"momfrac_Yes_x_armassist_Yes\"      \"priorfrac_Yes_x_premeno_Yes\"     \n [7] \"priorfrac_Yes\"                    \"armassist_Yes_x_smoke_Yes\"       \n [9] \"premeno_Yes_x_raterisk_Greater\"   \"momfrac_Yes_x_smoke_Yes\"         \n[11] \"priorfrac_Yes_x_momfrac_Yes\"      \"priorfrac_Yes_x_armassist_Yes\"   \n[13] \"premeno_Yes_x_armassist_Yes\"      \"momfrac_Yes_x_raterisk_Same\"     \n[15] \"priorfrac_Yes_x_raterisk_Greater\" \"armassist_Yes_x_raterisk_Greater\"\n[17] \"fracscore_x_momfrac_Yes\"          \"priorfrac_Yes_x_smoke_Yes\"       \n[19] \"premeno_Yes_x_raterisk_Same\"      \"fracscore_x_priorfrac_Yes\"       \n[21] \"fracscore_x_premeno_Yes\"          \"raterisk_Same\"                   \n[23] \"fracscore\"                        \"fracscore_x_raterisk_Greater\"    \n[25] \"armassist_Yes_x_raterisk_Same\"    \"fracscore_x_smoke_Yes\"           \n[27] \"height\"                           \"momfrac_Yes_x_raterisk_Greater\"  \n[29] \"priorfrac_Yes_x_raterisk_Same\"    \"fracscore_x_raterisk_Same\"       \n[31] \"height_x_raterisk_Greater\"        \"height_x_premeno_Yes\"            \n[33] \"height_x_fracscore\"               \"height_x_armassist_Yes\""
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-create-recipe-and-fit-model-from-lasso",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-create-recipe-and-fit-model-from-lasso",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Create recipe and fit model (from LASSO)",
    "text": "Step 2: Fit LASSO: Create recipe and fit model (from LASSO)\n\nThis is not the typical procedure for LASSO, but the tidymodels framework for interactions did not let me keep all main effects when looking at my interactions\n\n\nglow_rec_int2 = recipe(fracture ~ ., data = glow_train) %&gt;%\n  update_role(age_c, new_role = \"dont_use\") %&gt;%\n\n  step_dummy(priorfrac, premeno, momfrac, armassist, smoke, raterisk) %&gt;%\n\n  step_interact(terms = interaction_terms)\n  \nlog_model = logistic_reg()\n\nglow_workflow_int2 = workflow() %&gt;%\n      add_model(log_model) %&gt;% add_recipe(glow_rec_int2)\n  \nglow_fit_int2 = glow_workflow_int2 %&gt;% fit(glow_train)"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-look-at-model-fit",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-2-fit-lasso-look-at-model-fit",
    "title": "Lesson 15: Model Building",
    "section": "Step 2: Fit LASSO: Look at model fit",
    "text": "Step 2: Fit LASSO: Look at model fit\n\nprint(tidy(glow_fit_int2), n=60)\n\n# A tibble: 42 × 5\n   term                              estimate std.error statistic p.value\n   &lt;chr&gt;                                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n 1 (Intercept)                        3.09      10.3       0.300   0.764 \n 2 height                            -0.0415     0.0637   -0.652   0.515 \n 3 fracscore                         -2.92       2.15     -1.36    0.175 \n 4 priorfrac_Yes                     15.1        8.61      1.75    0.0793\n 5 premeno_Yes                       -0.805      1.14     -0.709   0.478 \n 6 momfrac_Yes                       -1.71       1.74     -0.984   0.325 \n 7 armassist_Yes                     18.5       10.7       1.73    0.0838\n 8 smoke_Yes                        -22.8      838.       -0.0272  0.978 \n 9 raterisk_Same                     16.0       10.1       1.59    0.112 \n10 raterisk_Greater                   1.13       9.16      0.123   0.902 \n11 height_x_fracscore                 0.0215     0.0136    1.58    0.113 \n12 height_x_priorfrac_Yes            -0.0825     0.0531   -1.55    0.120 \n13 height_x_armassist_Yes            -0.114      0.0645   -1.77    0.0762\n14 height_x_raterisk_Same            -0.0940     0.0623   -1.51    0.131 \n15 height_x_raterisk_Greater          0.00238    0.0563    0.0423  0.966 \n16 fracscore_x_priorfrac_Yes         -0.373      0.177    -2.10    0.0353\n17 fracscore_x_momfrac_Yes            0.608      0.313     1.94    0.0520\n18 fracscore_x_armassist_Yes         -0.111      0.178    -0.626   0.531 \n19 fracscore_x_smoke_Yes              0.604      0.564     1.07    0.284 \n20 fracscore_x_raterisk_Same         -0.257      0.209    -1.23    0.217 \n21 fracscore_x_raterisk_Greater      -0.318      0.212    -1.50    0.133 \n22 priorfrac_Yes_x_premeno_Yes       -2.72       1.06     -2.56    0.0104\n23 priorfrac_Yes_x_momfrac_Yes       -1.35       1.35     -1.00    0.317 \n24 priorfrac_Yes_x_armassist_Yes      1.45       0.820     1.76    0.0779\n25 priorfrac_Yes_x_smoke_Yes         -0.329      1.68     -0.196   0.845 \n26 priorfrac_Yes_x_raterisk_Same      0.122      0.837     0.146   0.884 \n27 priorfrac_Yes_x_raterisk_Greater   0.838      0.916     0.915   0.360 \n28 premeno_Yes_x_momfrac_Yes          0.304      1.58      0.192   0.848 \n29 premeno_Yes_x_armassist_Yes        1.73       0.923     1.87    0.0615\n30 premeno_Yes_x_smoke_Yes           -3.98       1.84     -2.17    0.0300\n31 premeno_Yes_x_raterisk_Same        0.716      1.16      0.620   0.535 \n32 premeno_Yes_x_raterisk_Greater     1.71       1.19      1.44    0.150 \n33 momfrac_Yes_x_armassist_Yes       -3.60       1.43     -2.52    0.0118\n34 momfrac_Yes_x_smoke_Yes            2.73       2.67      1.02    0.307 \n35 momfrac_Yes_x_raterisk_Same        1.87       1.33      1.41    0.160 \n36 momfrac_Yes_x_raterisk_Greater     0.730      1.33      0.548   0.583 \n37 armassist_Yes_x_smoke_Yes          1.58       1.67      0.948   0.343 \n38 armassist_Yes_x_raterisk_Same      0.690      0.893     0.774   0.439 \n39 armassist_Yes_x_raterisk_Greater  -0.247      0.975    -0.253   0.800 \n40 smoke_Yes_x_raterisk_Same         19.5      838.        0.0232  0.981 \n41 smoke_Yes_x_raterisk_Greater      20.0      838.        0.0239  0.981 \n42 raterisk_Same_x_raterisk_Greater  NA         NA        NA      NA"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#step-3-prediction-on-testing-set-1",
    "href": "lectures/15_Model_building/15_Model_Building.html#step-3-prediction-on-testing-set-1",
    "title": "Lesson 15: Model Building",
    "section": "Step 3: Prediction on testing set",
    "text": "Step 3: Prediction on testing set\n\nglow_test_pred = predict(glow_fit_int2, new_data = glow_test, type = \"prob\") %&gt;%\n    bind_cols(glow_test)\n\n\n\n\nglow_test_pred %&gt;% \n  roc_auc(truth = fracture, \n                  .pred_No)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.644\n\n\n \n\nWhy is this AUC worse than the one we saw with prior fracture, age, and their interaction?\n\nOnly 1 training and testing set: can overfit training and perform poorly on testing\nWe did not tune our penalty\nOur testing set only has 100 observations!\n\n\n\n\nglow_test_pred %&gt;% \n  roc_curve(truth = fracture, .pred_No) %&gt;%\n  autoplot()"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#solutions",
    "href": "lectures/15_Model_building/15_Model_Building.html#solutions",
    "title": "Lesson 15: Model Building",
    "section": "Solutions",
    "text": "Solutions\n\nUse a tuning parameter for our penalty\n\nBasically, we need to figure out what the best penalty is for our model\nWe use the training set to determine the best penality\nVideos that include info on coding tuning parameter\n\nTidyTuesday video on LASSO with interactions\n[]\n\n\nCross-validation\n\nUnder Cross validation within Data Science in a Box\nWill discuss more on next slide\n\nFor complete video of machine learning with LASSO, cross-validation, and tuning parameters\n\nSee “Unit 5 - Deck 4: Machine learning” on this page\n\nVideo goes through an example with more complicated data, but can be followed with our work!"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#solutions-beyond-our-class-right-now",
    "href": "lectures/15_Model_building/15_Model_Building.html#solutions-beyond-our-class-right-now",
    "title": "Lesson 15: Model Building",
    "section": "Solutions (beyond our class right now)",
    "text": "Solutions (beyond our class right now)\n\nUse a tuning parameter for our penalty\n\nBasically, we need to figure out what the best penalty is for our model\nWe use the training set to determine the best penality\nVideos that include info on coding tuning parameter\n\nTidyTuesday video on LASSO with interactions\n[]\n\n\nCross-validation\n\nUnder Cross validation within Data Science in a Box\nWill discuss more on next slide\n\nFor complete video of machine learning with LASSO, cross-validation, and tuning parameters\n\nSee “Unit 5 - Deck 4: Machine learning” on this page\n\nVideo goes through an example with more complicated data, but can be followed with our work!"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#cross-validation-specifically-k-fold",
    "href": "lectures/15_Model_building/15_Model_Building.html#cross-validation-specifically-k-fold",
    "title": "Lesson 15: Model Building",
    "section": "Cross-validation (specifically k-fold)",
    "text": "Cross-validation (specifically k-fold)\n\nPrevents overfitting to one set of training data\nSplit data into folds that train and validate model selection\nBasically subsection of training and testing (called validating) before truly testing on our original testing set"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#solutions-resources-beyond-our-class-right-now",
    "href": "lectures/15_Model_building/15_Model_Building.html#solutions-resources-beyond-our-class-right-now",
    "title": "Lesson 15: Model Building",
    "section": "Solutions / Resources (beyond our class right now)",
    "text": "Solutions / Resources (beyond our class right now)\n\nUse a tuning parameter for our penalty\n\nBasically, we need to figure out what the best penalty is for our model\nWe use the training set to determine the best penality\nVideos that includes tuning parameter with LASSO\n\nTidyTuesday video on LASSO with interactions\n\n\nCross-validation\n\nUnder Cross validation within Data Science in a Box\n\nFor complete video of machine learning with LASSO, cross-validation, and tuning parameters\n\nSee “Unit 5 - Deck 4: Machine learning” on this Data Science in a Box page\n\nVideo goes through an example with more complicated data, but can be followed with our work!"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#for-your-lab-4",
    "href": "lectures/15_Model_building/15_Model_Building.html#for-your-lab-4",
    "title": "Lesson 15: Model Building",
    "section": "For your Lab 4",
    "text": "For your Lab 4\n\nYou can use purposeful selection, like we did last quarter\n\nIf you want to focus on association modeling!\nA good way to practice this again if you struggled with it previously\n\n\n \n\nYou can try out LASSO regression\n\nIf you want to focus on prediction modeling!\nAnd if you want to stretch your R coding skills"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#same-as-results-from-previous-lessons",
    "href": "lectures/15_Model_building/15_Model_Building.html#same-as-results-from-previous-lessons",
    "title": "Lesson 15: Model Building",
    "section": "Same as results from previous lessons",
    "text": "Same as results from previous lessons\n\nglow_m3 = glm(fracture ~ priorfrac + age_c + priorfrac*age_c, \n              data = glow1, family = binomial)\n\n\ntidy(glow_m3, conf.int = T) %&gt;% gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n−1.376\n0.134\n−10.270\n0.000\n−1.646\n−1.120\n    priorfracYes\n1.002\n0.240\n4.184\n0.000\n0.530\n1.471\n    age_c\n0.063\n0.015\n4.043\n0.000\n0.032\n0.093\n    priorfracYes:age_c\n−0.057\n0.025\n−2.294\n0.022\n−0.107\n−0.008\n  \n  \n  \n\n\n\n\nInteraction model:  \\[\\begin{aligned} \\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = \\widehat\\beta_0 &+ &\\widehat\\beta_1\\cdot I(\\text{PF}) & + &\\widehat\\beta_2\\cdot Age& + &\\widehat\\beta_3 \\cdot I(\\text{PF}) \\cdot Age \\\\  \n\\text{logit}\\left(\\widehat\\pi(\\mathbf{X})\\right) & = -1.376 &+ &1.002\\cdot I(\\text{PF})& + &0.063\\cdot Age& -&0.057 \\cdot I(\\text{PF}) \\cdot Age\n\\end{aligned}\\]\n\nReminder of main effects and interactions"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#poll-everywhere-question-2",
    "href": "lectures/15_Model_building/15_Model_Building.html#poll-everywhere-question-2",
    "title": "Lesson 15: Model Building",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#poll-everywhere-question-3",
    "href": "lectures/15_Model_building/15_Model_Building.html#poll-everywhere-question-3",
    "title": "Lesson 15: Model Building",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#poll-everywhere-question-4",
    "href": "lectures/15_Model_building/15_Model_Building.html#poll-everywhere-question-4",
    "title": "Lesson 15: Model Building",
    "section": "Poll Everywhere Question 4",
    "text": "Poll Everywhere Question 4"
  },
  {
    "objectID": "lectures/15_Model_building/15_Model_Building.html#summary",
    "href": "lectures/15_Model_building/15_Model_Building.html#summary",
    "title": "Lesson 15: Model Building",
    "section": "Summary",
    "text": "Summary\n\nRevisited model selection techniques and discussed how a binary outcome can be treated differently than a continuous outcome\nDiscussed association vs prediction modeling\nDiscussed classification: a type of machine learning!\nIntroduced penalized regression as a classification method\nPerformed penalized regression (specifically LASSO) to select a prediction model\nProcess presented today has major flaws\n\nWe did not tune our parameter\nWe did not perform cross validation"
  },
  {
    "objectID": "project/Lab_04_instructions.html",
    "href": "project/Lab_04_instructions.html",
    "title": "Lab 4 Instructions",
    "section": "",
    "text": "Note\n\n\n\nThis lab looks short, but there is a lot of work here!!"
  },
  {
    "objectID": "project/Lab_04_instructions.html#directions",
    "href": "project/Lab_04_instructions.html#directions",
    "title": "Lab 4 Instructions",
    "section": "1 Directions",
    "text": "1 Directions\nYou can download the .qmd file for this lab here.\nThe above link will take you to your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Purpose\nThe purpose of this lab is to fit a multiple logistic regression model and practice how we would interpret our results for this study.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback.\n\n1.2.1 Rubric\n\n\n\n\n\n\n\n\n\n\n\n\n4 points\n3 points\n2 points\n1 point\n0 points\n\n\n\n\nFormatting\nLab submitted on Sakai with .html file. Answers are written in complete sentences with no major grammatical nor spelling errors. With little editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with grammatical or spelling errors. With editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are written in complete sentences with major grammatical or spelling errors. With major editing, the answer can be incorporated into the project report.\nLab submitted on Sakai with .html file. Answers are bulletted or do not use complete sentences.\nLab not submitted on Sakai with .html file.\n\n\nCode/Work\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nAll tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMost tasks are directly followed or answered. This includes all the needed code, in code chunks, with the requested output.\nSome tasks are directly followed or answered.This includes all the needed code, in code chunks, with the requested output. In a few tasks, the code syntax or output is not quite right.\nMore than a quarter of the tasks are not completed properly.\n\n\nReasoning*\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful and can be easily integrated into the final report.\nAnswers demonstrate understanding of research context and investigation of the data. Answers are thoughtful, but lack the clarity needed to easily integrate into the final report.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers are fairly thoughtful, but lack connection to the research.\nAnswers demonstrate some understanding of research context and investigation of the data. Answers seem rushed and with minimal thought.\nAnswers lack understanding of research context and investigation of the data. Answers seem rushed and without thought.\n\n\n\n*Applies to questions with reasoning"
  },
  {
    "objectID": "project/Lab_04_instructions.html#lab-activities",
    "href": "project/Lab_04_instructions.html#lab-activities",
    "title": "Lab 4 Instructions",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\n\n\n\n\n\n\nTask\n\n\n\nPlease restate your research question below using the provided format (1 sentence). You can change the wording if you’d like, but please make sure it is still clear. It’s repetitive, but it helps me contextualize my feedback as I look through your lab.\n\n\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Build your model\nThis lab is very open-ended. You will need to build a model for your outcome. We discussed prediction modeling in Lesson 15. (We covered everything you will need in class even though we did not finish my slides.) We discussed association modeling in BSTA 512/612.\nYou may either use LASSO regression OR Purposeful model selection to build a model. I highly suggest picking the model selection strategy based on your desired learning objective. LASSO will help stretch your R coding and machine learning skills. Purposeful model selection will allow you to cement certain concepts that we learned within 512/612 and 513/613.\nI will not be taking you through step-by-step. Please follow my work from Lesson 15 in 513/613 or from Lab 4 in 512/612.\n\n\n2.3 Assess your model fit\nCheck if your model fits the data well (Hosmer-Lemeshow test). Calculate the ROC-AUC of your model.\n\n\n2.4 Perform model diagnostics\nCheck your diagnostic plots and cutoffs (change in Pearson residuals, change in coefficients, and leverage) to identify and investigate any influential or outlier observations. Are these observations feasible?"
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_04.html",
    "href": "project/LastName_FirstInit_Lab_04.html",
    "title": "Lab 4",
    "section": "",
    "text": "Note\n\n\n\nThis is the required work for Lab 3. I am working on guided instructions on incorporating the optional missing data section of the Lab."
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_04.html#directions",
    "href": "project/LastName_FirstInit_Lab_04.html#directions",
    "title": "Lab 4",
    "section": "1 Directions",
    "text": "1 Directions\nThis is your editing file. Please do not remove anything from this editing file!! You will only add your code and work to this file.\n\n1.1 Purpose\nThe purpose of this lab is to fit a multiple logistic regression model and practice how we would interpret our results for this study.\n\n\n1.2 Grading\nThis lab is graded out of 12 points. The TAs will go through and grade your lab. They will make sure each section is complete and will follow the rubric below. I have instructed them that completion and clear effort is all that is needed to receive 100%. Nicky will go through the labs to give you feedback."
  },
  {
    "objectID": "project/LastName_FirstInit_Lab_04.html#lab-activities",
    "href": "project/LastName_FirstInit_Lab_04.html#lab-activities",
    "title": "Lab 4",
    "section": "2 Lab activities",
    "text": "2 Lab activities\n\n\n\n\n\n\nNote\n\n\n\nI have left it up to you to load the needed packages for this lab.\n\n\n\n2.1 Restate research question\nIn this study, we will investigate the association between food insecurity and ________.\n\n\n2.2 Build your model\n\n\n2.3 Assess your model fit\n\n\n2.4 Perform model diagnostics"
  },
  {
    "objectID": "project/Project_template.html#statistical-methods",
    "href": "project/Project_template.html#statistical-methods",
    "title": "Project Template: Title here",
    "section": "Statistical Methods",
    "text": "Statistical Methods"
  },
  {
    "objectID": "project/Project_template.html#results",
    "href": "project/Project_template.html#results",
    "title": "Project Template: Title here",
    "section": "Results",
    "text": "Results"
  },
  {
    "objectID": "project/Project_template.html#discussion",
    "href": "project/Project_template.html#discussion",
    "title": "Project Template: Title here",
    "section": "Discussion",
    "text": "Discussion"
  },
  {
    "objectID": "project/Project_template.html#conclusion",
    "href": "project/Project_template.html#conclusion",
    "title": "Project Template: Title here",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "project/Project_template.html#references",
    "href": "project/Project_template.html#references",
    "title": "Project Template: Title here",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "project/Project_template.html#reflection",
    "href": "project/Project_template.html#reflection",
    "title": "Project Template: Title here",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poisson Distribution",
    "text": "Poisson Distribution\n\nThis distribution is often used to model count data\nExamples:\n\nDistribution of number of deaths due to lung cancer\nDistribution of number of individuals diagnosed with leukemia\nDistribution of number of hospitalizations\n\n\n \n\nThis distribution is often used to model rate data\nExamples:\n\nDistribution of number of deaths due to lung cancer per year\nDistribution of number of individuals diagnosed with leukemia over follow-up time\nDistribution of number of hospitalizations in a day"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-ii",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-ii",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poisson Distribution (II)",
    "text": "Poisson Distribution (II)\n\nThe probability function of Poisson distribution: \\[P(Y = y | \\mu) = \\dfrac{\\mu^y e^{-\\mu}}{y!}\\]\n\nWhere \\(y\\)’s are non-negative integers \\(y=0, 1, 2,...\\)\nWhere \\(\\mu\\) is the mean of \\(Y\\), that is \\(E(Y)=\\mu\\)\nAnd also, \\(\\text{var}(Y)=\\mu\\)\n\nFor a Poisson distribution, \\(Y \\sim \\text{Poisson}(\\mu)\\)\n\nRange: \\([0, \\infty)\\)\n\n\n\nggplot(data = hcrabs) + geom_bar(aes(x = num.satellites))"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-iii",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-iii",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poisson Distribution (III)",
    "text": "Poisson Distribution (III)\n\nIf we look at the probability of \\(y\\) events in a time period \\(t\\) for a Poisson random variable, we could write: \\[P(Y = y | \\mu) = \\dfrac{\\mu^y e^{-\\mu}}{y!}\\]\n\nWhere \\(y\\)’s are non-negative integers \\(y=0, 1, 2,...\\)\nWhere \\(\\mu = \\lambda t\\), where \\(\\lambda\\) is the expected number of events per unit time\nThen \\(\\mu\\) is the expected number of events over time \\(t\\)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-iv",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-iv",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poisson Distribution (IV)",
    "text": "Poisson Distribution (IV)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poll-everywhere-question-1",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poll-everywhere-question-1",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poll Everywhere Question 1",
    "text": "Poll Everywhere Question 1"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years-i",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years-i",
    "title": "Lesson 16: Poisson Regression",
    "section": "Why Person-Years? (I)",
    "text": "Why Person-Years? (I)\n\nIn the example of number of patient arrivals, an event does not conclude the study\n\nIf someone arrives within the first minute of the study, then we keep counting\nWe may be able to study the association of arrivals with qualities of the hospital, but we can’t measure qualities of the individuals arriving\n\nWhat happens if we want to measure qualities of the individual?\n\nWe can measure a hospitalization rate"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years-ii",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years-ii",
    "title": "Lesson 16: Poisson Regression",
    "section": "Why Person-Years? (II)",
    "text": "Why Person-Years? (II)\n\nIf we are measuring at the individual level and counting something that is “terminal” then our count will always be 0 or 1\n\nExample: Number of individuals diagnosed with leukemia\nThis only happens once, so how do we measure the rate here?\n\nSince rate involves the counts and time – we can use the time to diagnosis to estimate the rate\n\nOften expressed in units such as events per thousand person-years"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#calculating-person-year",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#calculating-person-year",
    "title": "Lesson 16: Poisson Regression",
    "section": "Calculating Person-Year",
    "text": "Calculating Person-Year\n\nOne person-year is a unit of time defined as one person being followed for one year\nPerson-years for a sample of n subjects is calculated as the total years followed for the n subjects, where each subject could have different follow-up time\nExample: suppose we have 5 subjects, two of the subjects were followed for 2 years, and two of them are followed for 3 years and the fifth subject was followed for 3.8 years\n\n\\[\\text{person-years} = 2 \\text{ people} \\cdot 2 \\text{ years} +2 \\text{ people} \\cdot 3 \\text{ years}+ 1 \\text{ person} \\cdot 3.8 \\text{ years} = 13.8 \\text{ person-years}\\]"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#calculating-rate-ii",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#calculating-rate-ii",
    "title": "Lesson 16: Poisson Regression",
    "section": "Calculating Rate (II)",
    "text": "Calculating Rate (II)\n\nSuppose that we observe one event during the follow-up period, then\n\n\\[\\begin{aligned}\n\\text{Rate of event} &= \\dfrac{\\# events}{\\text{person-years}}= \\dfrac{1 \\text{ event}}{13.8 \\text{person-years}} \\\\ &= 0.072 \\text{ events per person−year} \\\\ &=72 \\text{ events per } 1000 \\text{ person−years} \\end{aligned}\\]"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#review-simple-logistic-regression",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#review-simple-logistic-regression",
    "title": "Lesson 16: Poisson Regression",
    "section": "Review: Simple Logistic Regression",
    "text": "Review: Simple Logistic Regression\n\nLet Y is the dependent variable of interest and x is a predictor variable,\n\nIn simple logistic regression, we have \\[\\log\\left(\\frac{\\pi(X)}{1 - \\pi(X)}\\right) = \\beta_0 + \\beta_1 X\\]\nwhere \\(\\pi (X) =P(Y = 1 \\mid X = x)\\)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#simple-poisson-regression-model",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#simple-poisson-regression-model",
    "title": "Lesson 16: Poisson Regression",
    "section": "Simple Poisson Regression Model",
    "text": "Simple Poisson Regression Model\n\nWhat do we model in a Poisson regression?\n\n \n\nLog of conditional mean of Y given x\n\nConditional mean of Y given x is represented as \\(E(Y \\mid X) = \\mu(X)\\)\nLet Y be a Poisson count for a given unit of time, then \\(\\mu(X) =\\lambda(X)\\)\nIn a simple Poisson regression, we have\n\n\n\\[\\ln(\\mu(X)) = \\ln(\\lambda(X)) = \\beta_0 + \\beta_1 X\\]\n \n\nSo this is also called a log-linear model"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-i",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-i",
    "title": "Lesson 16: Poisson Regression",
    "section": "Parameter Interpretation (I)",
    "text": "Parameter Interpretation (I)\n\nIn simple Poisson regression: \\[\\ln(\\mu(X)) = \\ln(\\lambda(X)) = \\beta_0 + \\beta_1 X\\]\nWhen \\(X\\) is a binary variable: How do we interpret \\(\\beta_1\\)?\n\nWhen \\(X=0\\): \\[\\ln(\\mu(X = 0)) = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\\]\n\n\\(\\mu(X = 0) = \\exp (\\beta_0)\\): the mean rate of \\(Y\\) when \\(X=0\\)\n\nWhen \\(X=1\\): \\[\\ln(\\mu(X = 1)) = \\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\\]\n\n\\(\\mu(X = 1) = \\exp (\\beta_0 + \\beta_1)\\): the mean rate of \\(Y\\) when \\(X=1\\)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-ii",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-ii",
    "title": "Lesson 16: Poisson Regression",
    "section": "Parameter Interpretation (II)",
    "text": "Parameter Interpretation (II)\n\nWhen \\(X\\) is a binary variable: How do we interpret \\(\\beta_1\\)?\n\nBy subtraction, we have \\[\\beta_1 = \\ln(\\mu(X = 1)) -  \\ln(\\mu(X = 0)) = \\ln \\left( \\dfrac{\\mu(X = 1)}{\\mu(X = 0)} \\right)\\]\n\\(\\beta_1\\): log-rate ratio\n\nSince \\(\\mu(X)\\) is \\(\\lambda(X)\\) is the rate of \\(Y\\)\n\nSo \\(\\exp(\\beta_1)\\) is the rate ratio!"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#further-reading-tutorials-on-poisson-regression",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#further-reading-tutorials-on-poisson-regression",
    "title": "Lesson 16: Poisson Regression",
    "section": "Further reading / tutorials on Poisson regression",
    "text": "Further reading / tutorials on Poisson regression\n\nGood tutorial in R\nWhen people are followed for different amounts of time, we should include an offset\n\nPoisson Regression Modeling Using Rate Data: section from above site that discusses offsets\n\nWe can use Wald test and LRT in the same way as logistic regression to test our coefficients and variables\n\n\n\nLesson 16: Poisson Regression"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#model-construction-include-offset-i",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#model-construction-include-offset-i",
    "title": "Lesson 16: Poisson Regression",
    "section": "Model Construction – Include Offset (I)",
    "text": "Model Construction – Include Offset (I)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#model-construction-include-offset-ii",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#model-construction-include-offset-ii",
    "title": "Lesson 16: Poisson Regression",
    "section": "Model Construction – Include Offset (II)",
    "text": "Model Construction – Include Offset (II)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#test-for-significance-in-parameter-i",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#test-for-significance-in-parameter-i",
    "title": "Lesson 16: Poisson Regression",
    "section": "Test for Significance in Parameter (I)",
    "text": "Test for Significance in Parameter (I)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#test-for-significance-in-parameter-ii",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#test-for-significance-in-parameter-ii",
    "title": "Lesson 16: Poisson Regression",
    "section": "Test for Significance in Parameter (II)",
    "text": "Test for Significance in Parameter (II)\n\n\nLesson 16: Poisson Regression"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#what-does-lambda-represent-in-the-poisson-distribution",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#what-does-lambda-represent-in-the-poisson-distribution",
    "title": "Lesson 16: Poisson Regression",
    "section": "What does \\(\\lambda\\) represent in the Poisson distribution?",
    "text": "What does \\(\\lambda\\) represent in the Poisson distribution?\n\nWhat does \\(\\lambda\\) represent?\n\nA rate, the expected number of events in a given population over a given period time\n\n\n \n\nExample: Number of patient arrivals into the Emergency Room per hour\n\n \n\nThe Poisson distribution is the prototype for assigning probabilities of observing any number of events"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#review-generalized-linear-models-glms",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#review-generalized-linear-models-glms",
    "title": "Lesson 16: Poisson Regression",
    "section": "Review: Generalized Linear Models (GLMs)",
    "text": "Review: Generalized Linear Models (GLMs)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#glm-random-component",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#glm-random-component",
    "title": "Lesson 16: Poisson Regression",
    "section": "GLM: Random Component",
    "text": "GLM: Random Component\n\nThe random component specifies the response variable \\(Y\\) and selects a probability distribution for it\n\n \n \n\nBasically, we are just identifying the distribution for our outcome\n\nIf Y is binary: assumes a binomial distribution of Y\nIf Y is count: assumes Poisson or negative binomial distribution of Y\nIf Y is continuous: assumea Normal distribution of Y"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#glm-systematic-component",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#glm-systematic-component",
    "title": "Lesson 16: Poisson Regression",
    "section": "GLM: Systematic Component",
    "text": "GLM: Systematic Component\n\nThe systematic component specifies the explanatory variables, which enter linearly as predictors \\[\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\]\n\n \n\nAbove equation includes:\n\nCentered variables\nInteractions\nTransformations of variables (like squares)\n\n\n \n\nSystematic component is the same as what we learned in Linear Models"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#glm-link-function",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#glm-link-function",
    "title": "Lesson 16: Poisson Regression",
    "section": "GLM: Link Function",
    "text": "GLM: Link Function\n\nIf \\(\\mu = E(Y)\\), then the link function specifies a function \\(g(.)\\) that relates \\(\\mu\\) to the linear predictor as: \\[g\\left(\\mu\\right)=\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\]\n\n\\(g\\left(\\mu\\right)\\) is the transformation we make to \\(E(Y)\\) (aka \\(\\mu\\)) so that the linear predictors (right side of equation) can be linked to the outcome\n\nThe link function connects the random component with the systematic component\nCan also think of this as: \\[\\mu=g^{-1}\\left(\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\right)\\]"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#glm-link-function-1",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#glm-link-function-1",
    "title": "Lesson 16: Poisson Regression",
    "section": "GLM: Link Function",
    "text": "GLM: Link Function"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html",
    "title": "Lesson 16: Poisson Regression",
    "section": "",
    "text": "The random component specifies the response variable \\(Y\\) and selects a probability distribution for it\n\n \n \n\nBasically, we are just identifying the distribution for our outcome\n\nIf Y is binary: assumes a binomial distribution of Y\nIf Y is count: assumes Poisson or negative binomial distribution of Y\nIf Y is continuous: assumea Normal distribution of Y\n\n\n\n\n\n\nThe systematic component specifies the explanatory variables, which enter linearly as predictors \\[\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\]\n\n \n\nAbove equation includes:\n\nCentered variables\nInteractions\nTransformations of variables (like squares)\n\n\n \n\nSystematic component is the same as what we learned in Linear Models\n\n\n\n\n\nIf \\(\\mu = E(Y)\\), then the link function specifies a function \\(g(.)\\) that relates \\(\\mu\\) to the linear predictor as: \\[g\\left(\\mu\\right)=\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\]\n\n\\(g\\left(\\mu\\right)\\) is the transformation we make to \\(E(Y)\\) (aka \\(\\mu\\)) so that the linear predictors (right side of equation) can be linked to the outcome\n\nThe link function connects the random component with the systematic component\nCan also think of this as: \\[\\mu=g^{-1}\\left(\\beta_0+\\beta_1X_1+\\ldots+\\beta_kX_k\\right)\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis distribution is often used to model count data\nExamples:\n\nDistribution of number of deaths due to lung cancer\nDistribution of number of individuals diagnosed with leukemia\nDistribution of number of hospitalizations\n\n\n\n\n\n\nThe probability function of Poisson distribution: \\[P(Y = y | \\mu) = \\dfrac{\\mu^y e^{-\\mu}}{y!}\\]\n\nWhere \\(y\\)’s are non-negative integers \\(y=0, 1, 2,...\\)\nWhere \\(\\mu\\) is the mean of \\(Y\\), that is \\(E(Y)=\\mu\\)\nAnd also, \\(\\text{var}(Y)=\\mu\\)\n\nFor a Poisson distribution, \\(Y \\sim \\text{Poisson}(\\mu)\\)\n\nRange: \\([0, \\infty)\\)\n\n\n\n\n\n\nIf we look at the probability of \\(y\\) events in a time period \\(t\\) for a Poisson random variable, we could write: \\[P(Y = y | \\mu) = \\dfrac{\\mu^y e^{-\\mu}}{y!}\\]\n\nWhere \\(y\\)’s are non-negative integers \\(y=0, 1, 2,...\\)\nWhere \\(\\mu = \\lambda t\\), where \\(\\lambda\\) is the expected number of events per unit time\nThen \\(\\mu\\) is the expected number of events over time \\(t\\)\n\n\n\n\n\n\nWhat does \\(\\lambda\\) represent?\n\nA rate, the expected number of events in a given population over a given period time\n\nExample: Number of patient arrivals into the Emergency Room per hour\nThe Poisson distribution is the prototype for assigning probabilities of observing any number of events\n\n\n\n\n\n\n\n\nIn the example of number of patient arrivals, an event does not conclude the study\n\nIf someone arrives within the first minute of the study, then we keep counting\nWe may be able to study the association of arrivals with qualities of the hospital, but we can’t measure qualities of the individuals arriving\n\nWhat happens if we want to measure qualities of the individual?\n\nWe can measure a hospitalization rate\n\n\n\n\n\n\nIf we are measuring at the individual level and counting something that is “terminal” then our count will always be 0 or 1\n\nExample: Number of individuals diagnosed with leukemia\nThis only happens once, so how do we measure the rate here?\n\nSince rate involves the counts and time – we can use the time to diagnosis to estimate the rate\n\nOften expressed in units such as events per thousand person-years\n\n\n\n\n\n\nOne person-year is a unit of time defined as one person being followed for one year\nPerson-years for a sample of n subjects is calculated as the total years followed for the n subjects, where each subject could have different follow-up time\nExample: suppose we have 5 subjects, two of the subjects were followed for 2 years, and two of them are followed for 3 years and the fifth subject was followed for 3.8 years\n\n\\[\\text{person-years} = 2 \\text{ people} \\cdot 2 \\text{ years} +2 \\text{ people} \\cdot 3 \\text{ years}+ 1 \\text{ person} \\cdot 3.8 \\text{ years} = 13.8 \\text{ person-years}\\]\n\n\n\n\nSuppose that we observe one event during the follow-up period, then\n\n\\[\\begin{aligned}\n\\text{Rate of event} &= \\dfrac{\\# events}{\\text{person-years}}= \\dfrac{1 \\text{ event}}{13.8 \\text{person-years}} \\\\ &= 0.072 \\text{ events per person−year} \\\\ &=72 \\text{ events per } 1000 \\text{ person−years} \\end{aligned}\\]\n\n\n\n\nLet Y is the dependent variable of interest and x is a predictor variable,\n\nIn simple logistic regression, we have \\[\\log\\left(\\frac{\\pi(X)}{1 - \\pi(X)}\\right) = \\beta_0 + \\beta_1 X\\]\nwhere \\(\\pi (X) =P(Y = 1 \\mid X = x)\\)\n\n\n\n\n\n\nWhat do we model in a Poisson regression?\nLog of conditional mean of Y given x\n\nLet Y be a Poisson count for a given unit of time, then \\(\\mu(X) =\\lambda(X)\\)\nIn a simple Poisson regression, we have \\[\\ln(\\mu(X)) = \\ln(\\lambda(X)) = \\beta_0 + \\beta_1 X\\]\n\nSo this is also called a log-linear model\n\n\n\n\n\nIn simple Poisson regression: \\[\\ln(\\mu(X)) = \\ln(\\lambda(X)) = \\beta_0 + \\beta_1 X\\]\nWhen \\(X\\) is a binary variable: How do we interpret \\(\\beta_1\\)?\n\nWhen \\(X=0\\)\nWhen \\(X=1\\)"
  },
  {
    "objectID": "homework/HW5.html#question-1",
    "href": "homework/HW5.html#question-1",
    "title": "Homework 5",
    "section": "Question 1",
    "text": "Question 1\nThis question stems from an example from an online textbook by Dr. Ramzi W. Nahhas. The dataset for this problem includes a subset of individuals from the 2019 National Survey on Drug Use and Health (NSDUH). Overall, our study aims included investigating potential risk factors for lifetime heroin use. Lifetime heroin use is a binary outcome, which we regress on age at first use of alcohol (alc_agefirst), age with 6 categories (demog_age_cat6), and sex assigned at birth (demog_sex).\n\nload(here(\"data\", \"nsduh2019_adult_sub_rmph.RData\"))\nnsduh = nsduh_adult_sub %&gt;% \n  dplyr::select(her_lifetime, alc_agefirst, demog_age_cat6, demog_sex) %&gt;% \n  drop_na()\n\n\nPart a\nUsing the nsduh dataset from the above chunk of code, please run a regression model and present the model summary using lifetime heroin use as our outcome, and age at first use of alcohol, categorical age, and sex assigned at birth as covariates in our model. No need to write out your model, you just need to write the R code to run it.\n\n\nPart b\nAre we encountering a numerical problem with our regression? If yes, please name the numerical issue. What first clued you into that issue? Provide conclusive evidence of this numerical issue (with a contingency table), and explain which variable(s) are causing this problem.\n\n\nPart c\nWhat would you do to “fix” this numerical issue? Please apply your “fix” and rerun the regression"
  },
  {
    "objectID": "homework/HW5.html#question-2",
    "href": "homework/HW5.html#question-2",
    "title": "Homework 5",
    "section": "Question 2",
    "text": "Question 2\nThis question is taken from the Hosmer and Lemeshow textbook. The ICU study data set consists of a sample of 200 subjects who were part of a much larger study on survival of patients following admission to an adult intensive care unit (ICU). The dataset should be available within Course Materials. The major goal of this study was to develop a logistic regression model to predict the probability of survival to hospital discharge of these patients. In this question, the primary outcome variable is vital status at hospital discharge, STA. Clinicians associated with the study felt that a key determinant of survival was the patient’s age at admission, AGE. We will be building to a multivariable logistic regression model while adjusting for cancer part of the present problem (CAN), CPR prior to ICU admission (CPR), infection probable at ICU admission (INF), and level of consciousness at ICU admission (LOC).\nWe will use the model from Homework 4 Question 1a for this question: \\[\\text{logit}(\\pi(\\textbf{X}))=\\beta_0 + \\beta_1 \\cdot AGE + \\beta_2 \\cdot CAN + \\beta_3 \\cdot CPR + \\\\ \\beta_4 \\cdot INF\\]\n\nicu = read_csv(here(\"data\", \"icu.csv\"))\nicu1 = icu %&gt;% mutate(STA = as.factor(STA) %&gt;% relevel(ref = \"Lived\"))\nicu2 = icu1 %&gt;% mutate(CAN = as.factor(CAN) %&gt;% relevel(ref = \"No\"), \n                     CPR = as.factor(CPR) %&gt;% relevel(ref = \"No\"), \n                     INF = as.factor(INF) %&gt;% relevel(ref = \"No\"), \n                     LOC = as.factor(LOC) %&gt;% \n                       relevel(ref = \"No Coma or Deep Stupor\"))\n\n\nPart a\nAssess the fit of the above model. You may use Hosmer-Lemeshow test, or Pearson Residual as appropriate. Discuss your choice and interpret.\n\n\nPart b\nAssess the your models ability to discriminate vital status (STA) using AUC.\n\n\nPart c\nLet’s say a colleague found a different preliminary final model than yours. Using the below model that your colleague found, compare your model to theirs using AIC and BIC.\n\ncoll_model = glm(STA ~ SYS + AGE + CPR + INF + AGE*CPR, \n                 data = icu2, family = \"binomial\")\ncoll_model\n\n\nCall:  glm(formula = STA ~ SYS + AGE + CPR + INF + AGE * CPR, family = \"binomial\", \n    data = icu2)\n\nCoefficients:\n(Intercept)          SYS          AGE       CPRYes       INFYes   AGE:CPRYes  \n   -1.47960     -0.01343      0.02340     -3.37369      0.53449      0.08370  \n\nDegrees of Freedom: 199 Total (i.e. Null);  194 Residual\nNull Deviance:      200.2 \nResidual Deviance: 172.5    AIC: 184.5"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#our-example-horseshoe-crabs-and-satellites",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#our-example-horseshoe-crabs-and-satellites",
    "title": "Lesson 16: Poisson Regression",
    "section": "Our example: Horseshoe Crabs and Satellites",
    "text": "Our example: Horseshoe Crabs and Satellites\n\n\nEach female horseshoe crab in the study had a male crab attached to her in her nest. The study investigated factors that affect whether the female crab had any other males, called satellites, residing near her. Explanatory variables that are thought to affect this included the female crab’s color, spine condition, and carapace width, and weight. The response outcome for each female crab is the number of satellites. There are 173 females in this study.\n \n\nlibrary(rsq)\ndata(hcrabs)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-12",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-12",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poisson Distribution (1/2)",
    "text": "Poisson Distribution (1/2)\n\n\n\nThe probability function of Poisson distribution: \\[P(Y = y | \\mu) = \\dfrac{\\mu^y e^{-\\mu}}{y!}\\]\n\nWhere \\(y\\)’s are non-negative integers \\(y=0, 1, 2,...\\)\nWhere \\(\\mu\\) is the mean of \\(Y\\), that is \\(E(Y)=\\mu\\)\nAnd also, \\(\\text{var}(Y)=\\mu\\)\n\nFor a Poisson distribution, \\(Y \\sim \\text{Poisson}(\\mu)\\)\n\nRange: \\([0, \\infty)\\)\n\n\n\n\nggplot(data = hcrabs) + \n    geom_bar(aes(x = num.satellites))"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-22",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-22",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poisson Distribution (2/2)",
    "text": "Poisson Distribution (2/2)\n\nIf we look at the probability of \\(y\\) events in a time period \\(t\\) for a Poisson random variable, we could write: \\[P(Y = y | \\mu) = \\dfrac{\\mu^y e^{-\\mu}}{y!} = \\dfrac{(\\lambda t)^y e^{-\\lambda t}}{y!}\\]\n\nWhere \\(y\\)’s are non-negative integers \\(y=0, 1, 2,...\\)\nWhere \\(\\mu = \\lambda t\\), where \\(\\lambda\\) is the expected number of events per unit time (aka rate)\nThen \\(\\mu\\) is the expected number of events over time \\(t\\)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-binary-x-12",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-binary-x-12",
    "title": "Lesson 16: Poisson Regression",
    "section": "Parameter Interpretation: Binary \\(X\\) (1/2)",
    "text": "Parameter Interpretation: Binary \\(X\\) (1/2)\n\nIn simple Poisson regression: \\[\\ln(\\mu(X)) = \\ln(\\lambda(X)) = \\beta_0 + \\beta_1 X\\]\nWhen \\(X\\) is a binary variable: How do we interpret \\(\\beta_1\\)?\n\nWhen \\(X=0\\): \\[\\ln(\\mu(X = 0)) = \\beta_0 + \\beta_1 \\cdot 0 = \\beta_0\\]\n\n\\(\\mu(X = 0) = \\exp (\\beta_0)\\): the mean count or rate of \\(Y\\) when \\(X=0\\)\n\nWhen \\(X=1\\): \\[\\ln(\\mu(X = 1)) = \\beta_0 + \\beta_1 \\cdot 1 = \\beta_0 + \\beta_1\\]\n\n\\(\\mu(X = 1) = \\exp (\\beta_0 + \\beta_1)\\): the mean count or rate of \\(Y\\) when \\(X=1\\)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-binary-x-22",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-binary-x-22",
    "title": "Lesson 16: Poisson Regression",
    "section": "Parameter Interpretation: Binary \\(X\\) (2/2)",
    "text": "Parameter Interpretation: Binary \\(X\\) (2/2)\n\nWhen \\(X\\) is a binary variable: How do we interpret \\(\\beta_1\\)?\n\nBy subtraction, we have \\[\\beta_1 = \\ln(\\mu(X = 1)) -  \\ln(\\mu(X = 0)) = \\ln \\left( \\dfrac{\\mu(X = 1)}{\\mu(X = 0)} \\right)\\]\n\\(\\beta_1\\): log-count ratio or log-rate ratio\n\nSince \\(\\mu(X)\\) is \\(\\lambda(X)\\) is the rate of \\(Y\\)\n\nSo \\(\\exp(\\beta_1)\\) is the count or rate ratio!"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-continuous-x",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#parameter-interpretation-continuous-x",
    "title": "Lesson 16: Poisson Regression",
    "section": "Parameter Interpretation: Continuous \\(X\\)",
    "text": "Parameter Interpretation: Continuous \\(X\\)\n\nWhen \\(X\\) is a continuous variable: How do we interpret \\(\\beta_0\\)?\n\n\\(\\beta_0\\): log-count or log-rate when \\(X\\) is 0\nSo \\(\\exp(\\beta_0)\\) is the expected count or rate when \\(X\\) is 0\n\nWhen \\(X\\) is a continuous variable: How do we interpret \\(\\beta_1\\)?\n\n\\(\\beta_1\\): log-count ratio or log-rate ratio for every 1 unit increase in \\(X\\)\n\nSince \\(\\mu(X)\\) is \\(\\lambda(X)\\) is the rate of \\(Y\\)\n\nSo \\(\\exp(\\beta_1)\\) is the rate ratio for every 1 unit increase in \\(X\\)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years-12",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years-12",
    "title": "Lesson 16: Poisson Regression",
    "section": "Why Person-Years? (1/2)",
    "text": "Why Person-Years? (1/2)\n\nIn the example of number of patient arrivals, an event does not conclude the study\n\nIf someone arrives within the first minute of the study, then we keep counting\nWe may be able to study the association of arrivals with qualities of the hospital, but we can’t measure qualities of the individuals arriving\n\nWhat happens if we want to measure qualities of the individual?\n\nWe can measure a hospitalization rate"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years-22",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years-22",
    "title": "Lesson 16: Poisson Regression",
    "section": "Why Person-Years? (2/2)",
    "text": "Why Person-Years? (2/2)\n\nIf we are measuring at the individual level and counting something that is “terminal” then our count will always be 0 or 1\n\nExample: Number of individuals diagnosed with leukemia\nThis only happens once, so how do we measure the rate here?\n\nSince rate involves the counts and time – we can use the time to diagnosis to estimate the rate\n\nOften expressed in units such as events per thousand person-years\n\nExample: effects of radiation exposure in Chernobyl disaster\n\nIf we measure deaths or cancer incidence due to radiation exposure\nTime plays a factor in measuring these outcomes\nDistance plays a factor in the amount of radiation exposure\n\nWhich can be measured on an individual basis"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poll-everywhere-question-2",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poll-everywhere-question-2",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poll Everywhere Question 2",
    "text": "Poll Everywhere Question 2"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-1-horseshoe-crabs-and-satellites",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-1-horseshoe-crabs-and-satellites",
    "title": "Lesson 16: Poisson Regression",
    "section": "Example 1: Horseshoe Crabs and Satellites",
    "text": "Example 1: Horseshoe Crabs and Satellites\n\n\nExample of count data:\nEach female horseshoe crab in the study had a male crab attached to her in her nest. The study investigated factors that affect whether the female crab had any other males, called satellites, residing near her. Explanatory variables that are thought to affect this included the female crab’s color, spine condition, and carapace width, and weight. The response outcome for each female crab is the number of satellites. There are 173 females in this study.\n\n\n\n\n\n\n\nlibrary(rsq)\ndata(hcrabs)\nggplot(data = hcrabs) + \n  geom_bar(aes(x = num.satellites))"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-1-horseshoe-crabs-and-satellites-1",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-1-horseshoe-crabs-and-satellites-1",
    "title": "Lesson 16: Poisson Regression",
    "section": "Example 1: Horseshoe Crabs and Satellites",
    "text": "Example 1: Horseshoe Crabs and Satellites\n\n\n\nggplot(hcrabs, \n       aes(x=width, \n           y=num.satellites)) + \n  geom_point(size = 2)\n\n\n\n\n\n\ncrab_mod = glm(num.satellites ~ width, \n                family=poisson, \n                data=hcrabs)\ntidy(crab_mod, conf.int=T, \n     exponentiate=T) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 35) %&gt;%\n  fmt_number(decimals = 2)\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.04\n0.54\n−6.09\n0.00\n0.01\n0.11\n    width\n1.18\n0.02\n8.22\n0.00\n1.13\n1.23\n  \n  \n  \n\n\n\n\n\n\nInterpretation: For every 1-cm increase in carapace width, the expected number of satellites increases by 18% (95% CI: 13%, 23%)."
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#in-summary",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#in-summary",
    "title": "Lesson 16: Poisson Regression",
    "section": "In summary…",
    "text": "In summary…\n\nPoisson regression can be used to model\n\nCount data\nRate data"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-2-lung-cancer-incidence",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-2-lung-cancer-incidence",
    "title": "Lesson 16: Poisson Regression",
    "section": "Example 2: Lung Cancer Incidence",
    "text": "Example 2: Lung Cancer Incidence\nExample of rate data:\nWe can look at the lung cancer incident counts (cases) per age group for four Danish cities from 1968 to 1971. Since it’s reasonable to assume that the expected count of lung cancer incidents is proportional to the population size, we would prefer to model the rate of incidents per capita.\n \n\nlibrary(ISwR)\ndata(eba1977)\nlc_inc = eba1977 %&gt;% mutate(lpop = log(pop))"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-with-a-count",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-with-a-count",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poisson Distribution with a count",
    "text": "Poisson Distribution with a count\n\n\n\nThe probability function of Poisson distribution: \\[P(Y = y | \\mu) = \\dfrac{\\mu^y e^{-\\mu}}{y!}\\]\n\nWhere \\(y\\)’s are non-negative integers \\(y=0, 1, 2,...\\)\nWhere \\(\\mu\\) is the mean of \\(Y\\), that is \\(E(Y)=\\mu\\)\nAnd also, \\(\\text{var}(Y)=\\mu\\)\n\nFor a Poisson distribution, \\(Y \\sim \\text{Poisson}(\\mu)\\)\n\nRange: \\([0, \\infty)\\)\n\n\n\nFrom Wikipedia:"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-with-a-rate",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poisson-distribution-with-a-rate",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poisson Distribution with a rate",
    "text": "Poisson Distribution with a rate\n\nIf we look at the probability of \\(y\\) events in a time period \\(t\\) for a Poisson random variable, we could write: \\[P(Y = y | \\mu) = \\dfrac{\\mu^y e^{-\\mu}}{y!} = \\dfrac{(\\lambda t)^y e^{-\\lambda t}}{y!}\\]\n\nWhere \\(y\\)’s are non-negative integers \\(y=0, 1, 2,...\\)\nWhere \\(\\mu = \\lambda t\\), where \\(\\lambda\\) is the expected number of events per unit time (aka rate)\nThen \\(\\mu\\) is the expected number of events over time \\(t\\)"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#what-are-person-years",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#what-are-person-years",
    "title": "Lesson 16: Poisson Regression",
    "section": "What are person-years?",
    "text": "What are person-years?"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#building-towards-person-years-12",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#building-towards-person-years-12",
    "title": "Lesson 16: Poisson Regression",
    "section": "Building towards person-years (1/2)",
    "text": "Building towards person-years (1/2)\n\nIn an example of number of patient arrivals, an event does not conclude the study\n\nIf someone arrives within the first minute of the study, then we keep counting\nWe may be able to study the association of arrivals with qualities of the hospital, but we can’t measure qualities of the individuals arriving\n\n\n \n\nFor example, in the lung cancer study, we can only discuss the incidence of cancer at the city-wide level\n\n \n\nWhat happens if we want to measure qualities of the individual?\n\nWe can measure a hospitalization rate"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#what-is-a-person-year-and-how-to-calculate",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#what-is-a-person-year-and-how-to-calculate",
    "title": "Lesson 16: Poisson Regression",
    "section": "What is a Person-Year and how to calculate?",
    "text": "What is a Person-Year and how to calculate?\n\nOne person-year is a unit of time defined as one person being followed for one year\n\n \n\nPerson-years for a sample of n subjects is calculated as the total years followed for the n subjects, where each subject could have different follow-up time\n\n \n\nExample: suppose we have 5 subjects, two of the subjects were followed for 2 years, and two of them are followed for 3 years and the fifth subject was followed for 3.8 years\n\n\\[\\text{person-years} = 2 \\text{ people} \\cdot 2 \\text{ years} +2 \\text{ people} \\cdot 3 \\text{ years}+ 1 \\text{ person} \\cdot 3.8 \\text{ years} = 13.8 \\text{ person-years}\\]"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#building-towards-person-years",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#building-towards-person-years",
    "title": "Lesson 16: Poisson Regression",
    "section": "Building towards person-years",
    "text": "Building towards person-years\n\nIn an example of number of patient arrivals, an event does not conclude the study\n\nIf someone arrives within the first minute of the study, then we keep counting\nWe may be able to study the association of arrivals with qualities of the hospital, but we can’t measure qualities of the individuals arriving\n\n\n \n\nFor example, in the lung cancer study, we can only discuss the incidence of cancer at the city-wide level\n\n \n\nWhat happens if we want to measure qualities of the individual?\n\nWe can measure a hospitalization rate"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#why-person-years",
    "title": "Lesson 16: Poisson Regression",
    "section": "Why Person-Years?",
    "text": "Why Person-Years?\n\nIf we are measuring at the individual level and counting something that is “terminal” then our count will always be 0 or 1\n\nExample: Number of individuals diagnosed with leukemia\nThis only happens once, so how do we measure the rate here?\n\n\n \n\nSince rate involves the counts and time – we can use the time to diagnosis to estimate the rate\n\nOften expressed in units such as events per thousand person-years\n\n\n \n\nPerson-years allow us to follow individuals for different amounts of time"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#calculating-rate",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#calculating-rate",
    "title": "Lesson 16: Poisson Regression",
    "section": "Calculating Rate",
    "text": "Calculating Rate\n\nSuppose that we observe one event during the follow-up period, then\n\n\\[\\begin{aligned}\n\\text{Rate of event} &= \\dfrac{\\# events}{\\text{person-years}}= \\dfrac{1 \\text{ event}}{13.8 \\text{person-years}} \\\\ &= 0.072 \\text{ events per person−year} \\\\ &=72 \\text{ events per } 1000 \\text{ person−years} \\end{aligned}\\]\n\nNow our rate of event is measured per person-year\n\nOften use 1000 person-years to count the events with whole numbers"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-2-lung-cancer-incidence-1",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-2-lung-cancer-incidence-1",
    "title": "Lesson 16: Poisson Regression",
    "section": "Example 2: Lung Cancer Incidence",
    "text": "Example 2: Lung Cancer Incidence\n\n\n\nggplot(lc_inc, aes(x=age, y=cases, \n           color = city)) + \n  geom_point(size = 2)\n\n\n\n\n\n\nlc_mod = glm(cases ~ city + age, \n            offset=lpop, \n            family=poisson, \n            data=lc_inc)\n\n\n\nRegression table\ntidy(lc_mod, conf.int=T, \n     exponentiate=T) %&gt;% \n  gt() %&gt;% \n  tab_options(table.font.size = 30) %&gt;%\n  fmt_number(decimals = 3)\n\n\n\n\n\n\n  \n    \n      term\n      estimate\n      std.error\n      statistic\n      p.value\n      conf.low\n      conf.high\n    \n  \n  \n    (Intercept)\n0.004\n0.200\n−28.125\n0.000\n0.002\n0.005\n    cityHorsens\n0.719\n0.182\n−1.818\n0.069\n0.503\n1.026\n    cityKolding\n0.690\n0.188\n−1.978\n0.048\n0.476\n0.995\n    cityVejle\n0.762\n0.188\n−1.450\n0.147\n0.525\n1.099\n    age55-59\n3.007\n0.248\n4.434\n0.000\n1.843\n4.901\n    age60-64\n4.566\n0.232\n6.556\n0.000\n2.907\n7.236\n    age65-69\n5.857\n0.229\n7.704\n0.000\n3.748\n9.249\n    age70-74\n6.404\n0.235\n7.891\n0.000\n4.043\n10.212\n    age75+\n4.136\n0.250\n5.672\n0.000\n2.523\n6.762"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#poll-everywhere-question-3",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#poll-everywhere-question-3",
    "title": "Lesson 16: Poisson Regression",
    "section": "Poll Everywhere Question 3",
    "text": "Poll Everywhere Question 3"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#including-an-offset-when-we-have-different-follow-up-times",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#including-an-offset-when-we-have-different-follow-up-times",
    "title": "Lesson 16: Poisson Regression",
    "section": "Including an offset when we have different follow-up times",
    "text": "Including an offset when we have different follow-up times\n\nWhat if we have data that each observation has different period of time?\n\nFor example, we look at number of ED visits when subjects are enrolled in Oregon Health Plan, however, each subjects are enrolled in the plan for different length of time…\nHow do we incorporate this different length of time?\n\nNote we have: \\(\\mu = \\lambda t\\) and with predictor \\(X\\), \\(\\mu(X) = \\lambda(X) \\cdot t(X)\\)\n\n\\(t\\) is now a function of the individual, represented with \\(X\\)\n\nThen we construct:\n\n\\[\\begin{aligned} \\ln(\\lambda(X)) = & \\beta_0 + \\beta_1 X \\\\\n\\ln(\\lambda(X)) = \\ln\\left(\\dfrac{\\mu(X)}{t(X)}\\right) = \\ln(\\mu(X)) - \\ln(t(X)) = &\\beta_0 + \\beta_1 X \\\\\n\\ln(\\mu(X)) = & \\ln(t(X)) + \\beta_0 + \\beta_1 X\\\\ \\end{aligned}\\]"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#including-an-offset-when-we-have-different-follow-up-times-1",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#including-an-offset-when-we-have-different-follow-up-times-1",
    "title": "Lesson 16: Poisson Regression",
    "section": "Including an offset when we have different follow-up times",
    "text": "Including an offset when we have different follow-up times\n\nThat is, to incorporate the different lengths in the model,\n\n\\[\\ln(\\mu(X)) =  \\ln(t(X)) + \\beta_0 + \\beta_1 X\\]\n\nWe have one more term in the model and this term is called offset, a known term in the model since \\(t(X)\\) is known for each individual\n\\(\\ln(t(X))\\) is called the offset\nOffsets can also be something like the population size in a city…"
  },
  {
    "objectID": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-2-lung-cancer-incidence-2",
    "href": "lectures/16_Poisson_regression/16_Poisson_regression.html#example-2-lung-cancer-incidence-2",
    "title": "Lesson 16: Poisson Regression",
    "section": "Example 2: Lung Cancer Incidence",
    "text": "Example 2: Lung Cancer Incidence\n\nlc_mod = glm(cases ~ city + age, offset=lpop, family=poisson, data=lc_inc)\nsummary(lc_mod)\n\n\nCall:\nglm(formula = cases ~ city + age, family = poisson, data = lc_inc, \n    offset = lpop)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -5.6321     0.2003 -28.125  &lt; 2e-16 ***\ncityHorsens  -0.3301     0.1815  -1.818   0.0690 .  \ncityKolding  -0.3715     0.1878  -1.978   0.0479 *  \ncityVejle    -0.2723     0.1879  -1.450   0.1472    \nage55-59      1.1010     0.2483   4.434 9.23e-06 ***\nage60-64      1.5186     0.2316   6.556 5.53e-11 ***\nage65-69      1.7677     0.2294   7.704 1.31e-14 ***\nage70-74      1.8569     0.2353   7.891 3.00e-15 ***\nage75+        1.4197     0.2503   5.672 1.41e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 129.908  on 23  degrees of freedom\nResidual deviance:  23.447  on 15  degrees of freedom\nAIC: 137.84\n\nNumber of Fisher Scoring iterations: 5"
  }
]
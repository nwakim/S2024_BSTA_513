---
title: "Module A"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "01/12/2023"
categories: ["Week 1"]
format: 
  revealjs:
    theme: [default, simple_NW.scss]
    toc: true
    toc-depth: 2
    toc-title: Class Overview
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: Class 1 Slides
---

## Learning Objectives

1.  Learning 1

2.  Learning 2

::: lob
3.  Learning 3
:::

4.  Learning 4

## What is Linear Regression?

-   [Regression]{.underline}: a technique to study the association between two variables Response variable (outcome, dependent variable) Blood pressure Grouping variable (predictor, independent variable, explanatory variable) Male vs female -- within each group, the value of the grouping variable is constant Drug dosage -- continuous predictor of interest, infinitely many groups Adjustment for other variables

-   [Linear]{.underline} model: for our purposes, refers to linearity w.r.t. the parameters

-   Response variable is a linear function of *parameters*

-   Regression models describe association, not causality

## Why should I care?

Most widely used and most developed method in statistics Appropriate in many practical settings Important to understand limitations Easy to interpret

Fun fact: one of the most frequently asked data scientist interview question What are the assumptions underlying linear regression?

## Why should I care?

Most importantly, it serves as a building block Essential concepts and ideas extend well to other regression methods and other areas e.g. you will see the following generic equation frequently:\
$\sum_{i=1}^nX_i\trans(Y_i-\mu_i)=0$ where $\mu_i=X_i\trans\beta$ in linear regression\
It says "the residuals are orthogonal to the covariates"

Techniques used in linear regression apply to other regression methods e.g. estimation, hypothesis testing, model diagnosis

## Linear Regression: Objectives

Objectives of any data analysis can generally be categorized as either *inference* or *prediction*

rare that only one objective of interest in practice, both are usually of various degrees of importance

Inference:

Estimation Hypothesis testing

## Inference: Estimation

Includes both point and interval estimation

Sign of regression coefficient may be of interest; or, both sign and magnitude e.g., estimate mean change in serum cholesterol per unit increase in BMI model parameters must have clear interpretation (limits complexity of model)

## Inference: Hypothesis Testing

We observe the data in the study *sample*; what can we infer about the underlying *population* of interest?

Hypothesis testing reduces results of study down to a sequence of yes/no answers

Model parameters must be interpretable for inference to be meaningful

e.g., on average, does serum cholesterol change as BMI (body mass index) increases? what about each of the following: age, gender, race, SBP (systolic blood pressure)?

## Prediction

Using regression model to predict response for yet unobserved subjects Accuracy and precision of predictions take precedence over interpretability of regression parameters

e.g., develop a linear regression model to predict serum cholesterol given a set of patient characteristics (age, gender, race, BMI, SBP, etc)

## Data Analysis Process: Overview

Descriptive analysis often skipped or done carelessly

VERY important first step

Propose model often done in close consultation with investigators

Estimate model parameters

Assess underlying assumptions of model $\longrightarrow$ return to (1)?

Hypothesis testing and/or prediction

Interpretation, conclusions

# What will be covered in BIOSTAT 650?

## Simple Linear Regression: one predictor

SLR features one response variable and a single covariate

Model: $$Y_i = \beta_0 + \beta_1X_i + \epsilon_i \nonumber$$

$Y_i$: response $\beta_0$, $\beta_1$: parameters $X_i$: covariate $\epsilon_i$: error

"Simple": one covariate only

Rather restrictive since only one covariate is involved

often, interest lies in several covariates if only interested in one certain covariate, may need to adjust for other covariates

## Multiple Linear Regression: multiple predictors

Multiple Regression: $q(>1)$ covariates Model $$Y_i  =  \beta_0 + \beta_1X_{i1} + \beta_2X_{i2} +\ldots +
        \beta_qX_{iq} +  \epsilon_i$$

::: columns
::: {.column width="60%"}
-   For compactness, often use matrix notation Compared to simple linear regression $\beta_j$'s have much different interpretation

-   Much more complicated than SLR;\
    Much greater chance to mis-model the data set
:::

::: {.column width="40%"}
![image](pic/twoD){height="1.2in"}
:::
:::

## Residual Diagnostics

Most of the assumptions underlying linear regression model are about the error term: $$\begin{split}
            \epsilon_i & \sim  N(0,\sigma^2)  \\
            \epsilon_i & \perp \epsilon_j,\; \forall i, \;j,\; i\neq j
        \end{split}$$ Failure of any of these assumptions to hold could invalidate various aspects of the analysis

Residuals = estimated errors: $\widehat{\epsilon}_i$

Residual diagnostics aims to check validity of error assumptions after model fitting

results of residual diagnostics could imply that modifications to model are required

## Transformations of the outcome

Consider the following SLR model: $$Y_i = \beta_0 + \beta_1X_{i} +  \epsilon_i$$ It is possible that, instead, some function of $Y_i$, say $h(Y_i)$, should be modeled: $$h(Y_i) =\beta_0 + \beta_1X_{i} +  \epsilon_i$$ Possible choices for $h(\cdot)$: \$\$

```{=tex}
\begin{aligned}
            h(Y_i) & = & \log(Y_i) \nonumber \\
            h(Y_i) & = & \log_{10}(Y_i) \nonumber \\
            h(Y_i) & = & \sqrt{Y_i} \nonumber \\
            h(Y_i) & = & 1/Y_i \nonumber
            
\end{aligned}
```
\$\$

## Transformations of the covariate(s)

Consider the following SLR model: \$\$

```{=tex}
\begin{aligned}
        Y_i & = & \beta_0 + \beta_1X_{i} +  \beta_2 X^2_{i} + \epsilon_i \nonumber
        
\end{aligned}
```
\$\$

This is still a linear model! "Linear" refers to being linear in the coefficients Transformation of the covariate can include \$\$

```{=tex}
\begin{aligned}
            &   &  \log(X_i) \nonumber \\
            &   &X^2_{i}, \cdots, X^q_{i}   \nonumber \\
            &   &\sqrt{X_i} \nonumber \\
            &   & \mbox{splines:} ~~~ (X_i-a)_+\nonumber
            
\end{aligned}
```
\$\$

## Transformations

Need to be very careful when using transformations

interpretation of parameters changes completely transforming the outcome alters assumptions regarding error structure

## Outliers/Influence Diagnostics

Consider the fitted model: $$\widehat{Y}_i = \widehat{\beta}_0 + \widehat{\beta}_1X_{i}$$

What impact does $(X_i,Y_i)$ have on $\widehat{Y}_i$? How much does the $i$'th subject influence $\widehat{\beta}_0$? By what amount would $\widehat{\beta}_1$ change if the $i$'th subject were deleted? If the $i$'th observation heavily influences $\widehat{\beta}_1$, should it be removed?

## Multicollinearity

Consider: linear regression model with covariates $X_1,\ldots,X_q$ Covariates are not assumed to be independent (e.g., age, height, weight, etc) However, extreme correlations among the covariates interferes with the model fitting:

may be impossible to compute $\widehat{\beta}_j$'s computed $\widehat{\beta}_j$'s may have extremely large standard errors fitted model may be quite unreliable

## Model Selection

Consider: state-wide survey, wherein information on 30 covariates collected

When objective is to find the 'best' model, several algorithms may simplify the process

Forward Selection: start with 0 covariates add (most significant) covariates one at a time until further addition does not improve the model's fit

Backward Elimination: start with full model successively delete (least significant) covariates, until such deletions results in marked decrease in model's adequacy

Stepwise Selection: combination of forward selection and backward elimination

## Model Validation

Concept: how accurate is the fitted model on new data (not used to compute the parameter estimates)

e.g., data splitting:

split data set in two fit model to first half of data set, $\widehat{\beta}_1^*,\dots,\widehat{\beta}_p^*$ compare $Y_i$ and $\widehat{Y}_i^*$, in second half of data set

## Weighted Regression Units $i$ are not always sampled completely at

random Surveys may oversample certain subpopulations

Some units are given more weight during the estimation process

## Linear vs. *Generalized* Linear Models

Linear regression: $Y_i$ assumed to be continuous\
(and $\epsilon_i$ is normally distributed)

Suppose $Y_i$ is an indicator variable (e.g., 1=cancer; 0=cancer-free)

$Y_i$ follows a Bernoulli distribution assumptions of linear regression blatantly violated

Generalized Linear Model (GLM): used to model non-Normal responses; e.g.,

$Y_i\sim$ Bernoulli/Binomial: logistic regression $Y_i$ is a count: Poisson regression

Will study GLM extensively in BIOSTAT 651

## Multiple vs. *Multivari[ate]{.underline}* regression

Simple linear regression (this course): one outcome $Y$, one predictor variable $X$

Multiple linear regression (this course): one outcome $Y$, multiple predictor variables $X$

*Multivari[ate]{.underline}* regression: multiple outcome variables $Y$, e.g. same outcome measured repeatedly over time multiple related outcomes (e.g., height and weight) considered simultaneously as outcome can be of any distribution, i.e., multivariate GLM

Will study multivariate regression extensively in BIOSTAT 653

# In class exercise

Maternal Bone Lead as an Independent Risk Factor for Fetal Neurotoxicity: A Prospective Study

## Background

Lead (Pb) is a neurotoxicant It is associated with decreased mental development (in children), and accelerated mental decline in the elderly.

Studies typically measure lead concentrations in blood, and correlate them (e.g., linear regression) with measurements of intelligence (e.g., BAYLEY mental development index, IQ tests)

Prenatal exposure measurements typically assessed by amount of lead in umbilical cord

Novel biomarker is bone lead concentration---premise is that lead leaches out of the mother's bones throughout pregnancy

## Gomaa et al. Study Objective and Design

Prospective Study: Children-mothers recruited at birth, followed until children are 24 months old

Research question: is lead exposure associated with mental development?

Lead exposure is measured by: Lead concentration in umbilical blood Lead concentration in maternal bones

Mental development Is measured by: BAYLEY's index of mental development (MDI)

Statistical question: Test if umbilical cord blood lead and bone lead concentration are predictors of BAYLEY's MDI

## Confounding Variables

What are confounders? A confounder can make the observed association appear stronger than the true association, weaker than the true association, or even the reverse of the true association

Confounders in this case:

demographics (age, gender, education, marital status) Breastfeeding duration Maternal IQ

Sometimes confounders and predictor variables are called "covariates"

## Acknowledgement

|                                              |                                    |
|:--------------------------------------:|:------------------------------:|
| ![image](pic/mousumibanerjee){height="30mm"} | ![image](pic/brisa){height="30mm"} |
|               Mousumi Banerjee               |          Brisa N. Sánchez          |
|            University of Michigan            |         Drexel University          |

Thank you for your slides!

# Brief Review of Basic Statistics

## Basic Statistics

::: columns
::: column
0.5

Random variable $Y$ Sample $Y_i, i=1,\dots, n$

Summation: $\sum_{i=1}^n Y_i =Y_1 + Y_2 + \ldots + Y_n$

Product: $\prod_{i=1}^n Y_i = Y_1 \times Y_2 \times \ldots \times Y_n$

Expected Value (or mean): $\mu_Y= E[Y] = \int_{-\infty}^\infty y f(y) dy$ reflects 'center' of $Y$'s distribution
:::

::: column
0.5
:::
:::

## Rules of Expected Values

::: columns
::: column
0.5 Expectation of sum: $E\left[\sum_{i=1}^n Y_i \right] = \sum_{i=1}^n E[Y_i] \nonumber$ No assumption of independence required

Let $a_1,\ldots,a_n$ be constants $E[a_iY_i] = a_iE[Y_i]$\
$E\left[\sum_{i=1}^n a_iY_i \right] = \sum_{i=1}^n a_iE[Y_i]$

Expectation of product: $E[Y_iY_j] = E[Y_i]E[Y_j]$\
if $Y_i$ and $Y_j$ are independent
:::

::: column
0.5
:::
:::

:::

## Variance and standard deviation

::: columns
::: column
0.5

Variance: $$\begin{split}
        &Var(Y) =  E[(Y-\mu_Y)^2]\\
        = &\int_{-\infty}^\infty (y-\mu_Y)^2 f(y) dy
        %=  E[Y^2] - \mu_Y^2
        \end{split}$$ reflects spread of $Y$'s distribution units: (units of $Y$)$^2$

Standard deviation: $$\begin{split}
        SD(Y)  = &\; \sqrt{Var(Y)}  \\
        SD(aY)  = &\; aSD(Y)
        \end{split}$$

reflects dispersion in $Y$'s distribution measured in same unit as $Y$
:::

::: column
0.5
:::
:::

:::

## Rules of Variances

::: columns
::: column
0.5 Variance\
$Var(Y)=E[(Y-\mu_Y)^2]$\
${\color{white}{Var(Y)}} =E[Y^2] - \mu_Y^2$ Variance of linear combination: $$\begin{split}
        &~Var(aY+b)\\ 
         = &~ Var(aY) \\
         = &~ E[(aY-E[aY])^2]\\
         = &~ a^2 E[(Y-E[Y])^2]\\
         = &~ a^2 Var(Y)
        \end{split}$$ ![image](pic/shift){height="0.7in"}
:::

::: column
0.5
:::
:::

:::

## Covariance

::: columns
::: column
0.6 $X$, $Y$: random variables Covariance: $\mbox{cov}(Y,X)= E[(Y-\mu_Y)(X-\mu_X)]$

Measures (linear) association between $X$, $Y$\
$>0$, large values of $X$ tend to occur with large values of $Y$\
$<0$, large values of $X$ tend to coincide with small values of $Y$\
$=0$, size of $X$ provides no information on size of $Y$ When the covariance is calculated, the data are not standardized Not scale-invariant: can interpret direction but not magnitude
:::

::: column
0.4
:::
:::

:::

## Correlation

::: columns
::: column
0.5 $X$, $Y$: random variables Correlation: $\mbox{corr}(X,Y) = \frac{\mbox{cov}(X,Y)}{SD(X)SD(Y)}$

Scaled measure of linear association,

$-1 \leq \mbox{corr}(X,Y) \leq 1$ easier to interpret than covariance
:::

::: column
0.5
:::
:::

## Rules of covariance

$\mbox{cov}(Y,X)= E[(Y-\mu_Y)(X-\mu_X)]= E[XY]-E[X]E[Y]$ $\mbox{cov} (Y,Y) = \mbox{var} (Y)$ Independent $\stackrel{\Rightarrow}{\not\Leftarrow}$ uncorrelated If $X$ and $Y$ are independent, $\mbox{cov}(X,Y)=0$ If $\mbox{cov}(X,Y)=0$ and $(X,Y)\sim \text{Bivariate Normal}$, then $X$ and $Y$ are independent Covariance is symmetric, additive, and scale preserving $$\begin{aligned}
\mbox{cov} (X,Y) & = & \mbox{cov} (Y,X) \nonumber \\
\mbox{cov} (X,Y_1+Y_2) & = & \mbox{cov} (X,Y_1)+\mbox{cov} (X,Y_2) \nonumber \\
\mbox{cov} (X,aY) & = & a\;\mbox{cov} (X,Y) \nonumber 
%\mbox{cov} (Y,Y) & = & \mbox{var} (Y) \nonumber
\end{aligned}$$ :::

## Rules of variance

::: columns
::: column
0.6

Variance of sum: $$\setlength{\jot}{1pt}
        \begin{split}
        &Var\left(\sum_{i=1}^n Y_i\right)
        =  \sum_{i=1}^n\sum_{j=1}^n
        \mbox{cov}(Y_i,Y_j )  \\
        = & \sum_{i=1}^n Var(Y_i)   + \sum_{i=1}^n\sum_{j=1}^n I(j\neq i) \mbox{cov}(Y_i, Y_j )  \\
        = & \sum_{i=1}^n Var(Y_i) + 2\sum_{i=1}^n \sum_{j=i+1}^n
        \mbox{cov}(Y_i, Y_j ) 
        \end{split}$$ if $Y_1,\ldots,Y_n$ are mutually independent, then $Var\left(\sum_{i=1}^n Y_i\right) = \sum_{i=1}^n Var(Y_i)$
:::

::: column
0.4
:::
:::

:::

## Estimator of Mean

Suppose we obtained a simple random sample from some underlying population, then we can derive sample estimates of each of the population quantities defined previously Suppose $Y_1,\ldots,Y_n$ are *iid* with mean $\mu_Y$ and variance $\sigma^2_Y$

::: columns
::: column
0.6 Estimator of mean: $$\widehat{\mu}_Y = \frac{1}{n} \sum_{i=1}^n Y_i =
            \overline{Y}$$

$E[\overline{Y}]= \frac{1}{n} \sum_{i=1}^n E[Y_i] = \mu_Y$

$Var(\overline{Y})= n^{-2} \sum_{i=1}^n Var(Y_i) = \sigma_Y^2/n$
:::

::: column
0.37
:::
:::

:::

## Variance and Covariance Estimator

::: columns
::: column
0.6

Estimators of variance:

$\widehat{\sigma}^2_Y = \frac{1}{n} \sum_{i=1}^n (Y_i-E[Y_i])^2$\
if population mean is known $\widehat{\sigma}^2_Y = \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y})^2$\
if population mean is unknown

Estimator of covariance:\
Suppose pairs $(Y_1,X_1),\ldots,(Y_n,X_n)$ are *iid*. $\widehat{\mbox{cov}} (X,Y) = \frac{1}{n-1} \sum_{i=1}^n (Y_i-\overline{Y})(X_i-\overline{X})$ $\widehat{\mbox{corr}} (X,Y) =$ ?
:::

::: column
0.4
:::
:::

:::

## Distributions that will be used in this class

Normal distribution Chi-square distribution t distribution F distribution

## Normal Distribution

Density: $Y\sim N(\mu,\sigma^2)$, \$\$

```{=tex}
\begin{aligned}
        f_Y(y) & = &
        \frac{1}{\sigma\sqrt{2\pi}}\exp\left\{\frac{-1}{2}\left(\frac{y-\mu}{\sigma}
        \right)^2\right\} \nonumber
        
\end{aligned}
```
\$\$ If we know $E(Y)=\mu$, $Var(Y)=\sigma^2$ then

/3 of $Y$'s distribution lies within 1 $\sigma$ of $\mu$ % $\ldots$ $\ldots$ is within $\mu\pm 2\sigma$ $>99$% $\ldots$ $\ldots$ lies within $\mu\pm 3\sigma$

Arguably, the most important distribution in statistics

Linear combinations of Normals are Normal\
e.g., $(aY+b)\sim \mbox{N}(a\mu+b,\;a^2\sigma^2)$

Standard normal: \$\$

```{=tex}
\begin{aligned}
        Z=\frac{Y-\mu}{\sigma} \sim \mbox{N}(0,1) \nonumber
        
\end{aligned}
```
\$\$

## Chi-square Distribution

Notation: $X \sim \chi^2_{df}$ $df=$ degrees of freedom $E[X]=df$ $X$ takes on only positive values If $Z_i\sim \mbox{N}(0,1)$, then $Z_i^2\sim \chi^2_1$

If $Z_1,\ldots,Z_n$ are independent, with $Z_i\sim\mbox{N}(0,1)$, then \$\$

```{=tex}
\begin{aligned}
        \sum_{i=1}^n Z_i^2 & \sim & \chi^2_n \nonumber
        
\end{aligned}
```
\$\$

Used in hypothesis testing and CI's involving variance

## t Distribution

If $Z\sim \mbox{N}(0,1)$ and $S^2\sim \chi^2_{df}$ and $Z$ and $S^2$ are independent,

\$\$

```{=tex}
\begin{aligned}
        \frac{Z}{S/\sqrt{df}} & \sim & t_{df} \nonumber
        
\end{aligned}
```
\$\$

Symmetric, bell-shaped; tails heavier than Normal $E[t_{df}]=0$; $Var(t_{df})$ greater than 1 $\lim_{df\rightarrow \infty}t_{df} \rightarrow \mbox{N}(0,1)$ for $df>30$, the $t_{df}$ closely resembles the $\mbox{N}(0,1)$ distribution

In linear modeling, used for inference on individual regression parameters

## F Distribution

If $X_1^2\sim \chi^2_{df1}$ and $X_2^2\sim \chi^2_{df2}$, where $X_1^2\perp X_2^2$, then: \$\$

```{=tex}
\begin{aligned}
        \frac{X_1^2/df1}{X_2^2/df2} & \sim & F_{df1,df2} \nonumber
        
\end{aligned}
```
\$\$

only takes on positive values

connection to $t$ distribution:

\$\$

```{=tex}
\begin{aligned}
            \{ t_{df} \}^2 & \stackrel{{\cal D}}{=} & F_{1,df} \nonumber
            
\end{aligned}
```
\$\$

Used extensively in linear regression (hypothesis testing)

---
title: "SLR: More inference + Evaluation"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "01/24/2023"
categories: ["Week 1"]
format: 
  revealjs:
    theme: [default, simple_NW.scss]
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: SLR 3
    html-math-method: mathjax
    highlight-style: ayu
execute:
  echo: true
  freeze: auto  # re-render only when source changes
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(openintro)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) 
library(gridExtra) # NEW!!!

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_gray(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

gapm <- read_csv("data/lifeexp_femlit_2011.csv") %>% na.omit()

model1 <- lm(life_expectancy_years_2011 ~
               female_literacy_rate_2011,
                 data = gapm)
# Get regression table:
reg_tabel = tidy(model1) %>% gt() %>% 
 tab_options(table.font.size = 40) %>%
 fmt_number(decimals = 3)
```

# Learning Objectives

1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table

2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0

3.  Calculate and interpret the coefficient of determination

4.  Describe the model assumptions made in linear regression using
    ordinary least squares

## So far in our regression example...

::: columns
::: {.column width="49%"}
**Lesson 1 of SLR:**

-   Fit regression line
-   Calculate slope & intercept
-   Interpret slope & intercept

**Lesson 2 of SLR:**

-   Estimate variance of the residuals
-   Inference for slope & intercept: CI, p-value
-   Confidence bands of regression line for mean value of Y\|X
:::

::: {.column width="2%"}
:::

::: {.column width="49%"}
```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false

ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))

```
:::
:::

```{=tex}
\begin{aligned}
\widehat{Y} &= \widehat\beta_0 + \widehat\beta_1 \cdot X\\
\widehat{\text{life expectancy}} &= 50.9 + 0.232 \cdot \text{female literacy rate}
\end{aligned}
```
## Let's revisit the regression analysis process

::: box
![](images/arrow2.png){.absolute top="13.5%" right="62.1%" width="155"}
![](images/arrow2.png){.absolute top="13.5%" right="28.4%"
width="155"}![](images/arrow_back4.png){.absolute top="7.5%"
right="30.5%" width="820"} ![](images/arrow_down.png){.absolute
top="60.5%" right="48%" width="85"}

::: columns
::: {.column width="30%"}
::: RAP1
::: RAP1-title
Model Selection
:::

::: RAP1-cont
-   Building a model

-   Selecting variables

-   Prediction vs interpretation

-   Comparing potential models
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP2
::: RAP2-title
Model Fitting
:::

::: RAP2-cont
-   Find best fit line

-   Using OLS in this class

-   Parameter estimation

-   Categorical covariates

-   Interactions
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP3
::: RAP3-title
Model Evaluation
:::

::: RAP3-cont
-   Evaluation of model fit
-   Testing model assumptions
-   Residuals
-   Transformations
-   Influential points
-   Multicollinearity
:::
:::
:::
:::
:::

::: RAP4
::: RAP4-title
Model Use (Inference)
:::

::: RAP4-cont
::: columns
::: {.column width="50%"}
-   Inference for coefficients
-   Hypothesis testing for coefficients
:::

::: {.column width="50%"}
-   Inference for expected $Y$ given $X$
-   Prediction of new $Y$ given $X$
:::
:::
:::
:::

# Learning Objectives

::: lob
1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table
:::

2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0

3.  Calculate and interpret the coefficient of determination

4.  Describe the model assumptions made in linear regression using
    ordinary least squares

## Getting to the F-test

The F statistic in linear regression is essentially a proportion of the
variance explained by the model vs. the variance not explained by the
model

1.  Start with visual of explained vs. unexplained variation

2.  Figure out the mathematical representations of this variation

3.  Look at the ANOVA table to establish key values measuring our
    variance from our model

4.  Build the F-test

## Explained vs. Unexplained Variation

```{r}
#| echo: false
#| fig-align: center

regression_points <- augment(model1)
# summary(model1)
# sum(model1$residuals^2)

ggplot(regression_points, 
       aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_segment(aes(
    xend = female_literacy_rate_2011, 
    yend = .fitted), 
    alpha = 1, 
    color = "#4FADF3", 
    size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "#F14124", size=3) +
  # > Color adjustments made here...
  geom_point(color = "black", size = 4) +  # Color mapped here
  #scale_color_gradient2(low = "#213c96", mid = "white", high = "#F14124") +  # Colors to use here
    #guides(color = "none") +
  geom_point(aes(y = .fitted), shape = 1, size = 4) +
labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Average life expectancy vs. female literacy rate in 2011") +
    theme(axis.title = element_text(size = 20), 
        axis.text = element_text(size = 20), 
        title = element_text(size = 20))  
```

$$ \begin{aligned}
Y_i - \overline{Y} & = (Y_i - \hat{Y}_i) + (\hat{Y}_i- \overline{Y})\\
\text{Total unexplained variation} & = \text{Variation due to regression} + \text{Residual variation after regression}
\end{aligned}$$

## More on the equation

::: columns
::: {.column width="40%"}
$$Y_i - \overline{Y} = (Y_i - \hat{Y}_i) + (\hat{Y}_i- \overline{Y})$$

-   $Y_i - \overline{Y}$ = the deviation of $Y_i$ around the mean
    $\overline{Y}$
    -   (the **total** amount deviation unexplained at $X_i$ ).
-   $Y_i - \hat{Y}_i$ = the deviation of the observation $Y$ around the
    fitted regression line
    -   (the amount deviation **unexplained** by the regression at $X_i$
        ).
-   $\hat{Y}_i- \overline{Y}$ = the deviation of the fitted value
    $\hat{Y}_i$ around the mean $\overline{Y}$
    -   (the amount deviation **explained** by the regression at $X_i$ )
:::

::: {.column width="60%"}
   

![](05_SLR_Eval/SS.png){fig-align="center"}
:::
:::

## Poll Everywhere Question 1

## How is this actually calculated for our fitted model?

$$ \begin{aligned}
Y_i - \overline{Y} & = (Y_i - \hat{Y}_i) + (\hat{Y}_i- \overline{Y})\\
\text{Total unexplained variation} & = \text{Variation due to regression} + \text{Residual variation after regression}
\end{aligned}$$

$$\begin{aligned}
\sum_{i=1}^n (Y_i - \overline{Y})^2 & = \sum_{i=1}^n (\hat{Y}_i- \overline{Y})^2 + \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 \\
SSY & = SSR + SSE 
\end{aligned}$$
$$\text{Total Sum of Squares} = \text{Sum of Squares due to Regression} + \text{Sum of Squares due to Error (residuals)}$$

::: columns
::: {.column width="9%"}
:::

::: {.column width="82%"}
ANOVA table:

| Variation Source | df    | SS    | MS                      | test statistic        | p-value |
|------------|------------|------------|------------|------------|------------|
| Regression       | $1$   | $SSR$ | $MSR = \frac{SSR}{1}$   | $F = \frac{MSR}{MSE}$ |         |
| Error            | $n-2$ | $SSE$ | $MSE = \frac{SSE}{n-2}$ |                       |         |
| Total            | $n-1$ | $SSY$ |                         |                       |         |
:::

::: {.column width="9%"}
:::
:::

## Analysis of Variance (ANOVA) table in R

```{r}
# Fit regression model:
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
             data = gapm)

anova(model1)
anova(model1) %>% tidy() %>% gt() %>%
   tab_options(table.font.size = 40) %>%
   fmt_number(decimals = 3)
```

# Learning Objectives

1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table

::: lob
2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0
:::

3.  Calculate and interpret the coefficient of determination

4.  Describe the model assumptions made in linear regression using
    ordinary least squares

## What is the F statistic testing?

$$F = \frac{MSR}{MSE}$$

-   It can be shown that

$$E(MSE)=\sigma^2\ \text{and}\ E(MSR) = \sigma^2 + \beta_1^2\sum_{i=1}^n (X_i- \overline{X})^2$$

-   Recall that $\sigma^2$ is the variance of the residuals
-   Thus if
    -   $\beta_1 = 0$, then
        $F \approx \frac{\hat{\sigma}^2}{\hat{\sigma}^2} = 1$
    -   $\beta_1 \neq 0$, then
        $F \approx \frac{\hat{\sigma}^2 + \hat{\beta}_1^2\sum_{i=1}^n (X_i- \overline{X})^2}{\hat{\sigma}^2} > 1$
-   So the $F$ statistic can also be used to test $\beta_1$

## F-test vs. t-test for the population slope

The square of a $t$-distribution with $df = \nu$ is an $F$-distribution
with $df = 1, \nu$

$$T_{\nu}^2 \sim F_{1,\nu}$$

-   We can use either F-test or t-test to run the following hypothesis
    test:

```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
-   Note that the F-test does not support one-sided alternative tests,
    but the t-test does!

## Planting a seed about the F-test

We can think about the hypothesis test for the slope...

::: columns
::: {.column width="17%"}
:::

::: {.column width="33%"}
::: proof1
::: proof-title
Null $H_0$
:::

::: proof-cont
$\beta_1=0$
:::
:::
:::

::: {.column width="33%"}
::: definition
::: def-title
Alternative $H_1$
:::

::: def-cont
$\beta_1\neq0$
:::
:::
:::
:::

in a slightly different way...

::: columns
::: {.column width="17%"}
:::

::: {.column width="33%"}
::: proof1
::: proof-title
Null model ($\beta_1=0$)
:::

::: proof-cont
-   $Y = \beta_0 + \epsilon$
-   Smaller (reduced) model
:::
:::
:::

::: {.column width="33%"}
::: definition
::: def-title
Alternative model ($\beta_1\neq0$)
:::

::: def-cont
-   $Y = \beta_0 + \beta_1 X + \epsilon$
-   Larger (full) model
:::
:::
:::
:::

-   In multiple linear regression, we can start using this framework to
    test multiple coefficient parameters at once

    -   Decide whether or not to reject the smaller reduced model in
        favor of the larger full model

    -   Cannot do this with the t-test!

## Poll Everywhere Question 2

## F-test: general steps for hypothesis test for population **slope** $\beta_1$

::: columns
::: {.column width="48%"}
::: highlight-container
::: highlight
1.  For today's class, we are assuming that we have met the underlying
    assumptions
:::
:::

::: highlight-container
::: highlight
2.  State the null hypothesis.
:::
:::

Often, we are curious if the coefficient is 0 or not:

```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
::: highlight-container
::: highlight
3.  Specify the significance level.
:::
:::

Often we use $\alpha = 0.05$

::: highlight-container
::: highlight
4.  Specify the test statistic and its distribution under the null
:::
:::

The test statistic is $F$, and follows an F-distribution with numerator
$df=1$ and denominator $df=n-2$.
:::

::: {.column width="4%"}
:::

::: {.column width="48%"}
::: highlight-container
::: highlight
5.  Compute the value of the test statistic
:::
:::

The calculated **test statistic** for $\widehat\beta_1$ is

$$F = \frac{MSR}{MSE}$$

::: highlight-container
::: highlight
6.  Calculate the p-value
:::
:::

We are generally calculating: $P(F_{1, n-2} > F)$

::: highlight-container
::: highlight
7.  Write conclusion for hypothesis test
:::
:::

-   Reject: $P(F_{1, n-2} > F) < \alpha$
-   Fail to reject: $P(F_{1, n-2} > F) \geq \alpha$

We (reject/fail to reject) the null hypothesis that the slope is 0 at
the $100\alpha\%$ significiance level. There is
(sufficient/insufficient) evidence that there is significant association
between ($Y$) and ($X$) (p-value = $P(F_{1, n-2} > F)$).
:::
:::

## Life expectancy example: hypothesis test for population **slope** $\beta_1$

-   Steps 1-4 are setting up our hypothesis test: not much change from
    the general steps

::: highlight-container
::: highlight
1.  For today's class, we are assuming that we have met the underlying
    assumptions
:::
:::

::: highlight-container
::: highlight
2.  State the null hypothesis.
:::
:::

We are testing if the slope is 0 or not:

```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
::: highlight-container
::: highlight
3.  Specify the significance level.
:::
:::

Often we use $\alpha = 0.05$

::: highlight-container
::: highlight
4.  Specify the test statistic and its distribution under the null
:::
:::

::: columns
::: {.column width="70%"}
The test statistic is $F$, and follows an F-distribution with numerator
$df=1$ and denominator $df=n-2 = 80-2$.
:::

::: {.column width="30%"}
```{r}
nobs(model1)
```
:::
:::

## Life expectancy example: hypothesis test for population **slope** $\beta_1$ (2/4)

::: highlight-container
::: highlight
5.  Compute the value of the test statistic
:::
:::

```{r}
anova(model1) %>% tidy() %>% gt() %>%
  tab_options(table.font.size = 40)
```

-   **Option 1:** Calculate the test statistic using the values in the
    ANOVA table

$$F = \frac{MSR}{MSE} = \frac{2052.81}{37.73}=54.414$$

-   **Option 2:** Get the test statistic value (F) from the ANOVA table

::: hl
I tend to skip this step because I can do it all with step 6
:::

## Life expectancy example: hypothesis test for population **slope** $\beta_1$ (3/4)

::: highlight-container
::: highlight
6.  Calculate the p-value
:::
:::

-   As per Step 4, test statistic $F$ can be modeled by a
    $F$-distribution with $df1 = 1$ and $df2 = n-2$.

    -   We had 80 countries' data, so $n=80$

-   **Option 1:** Use `pf()` and our calculated test statistic

```{r}
# p-value is ALWAYS the right tail for F-test
pf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)
```

-   **Option 2:** Use the ANOVA table

```{r}
anova(model1) %>% tidy() %>% gt() %>%
  tab_options(table.font.size = 40)
```

## Life expectancy example: hypothesis test for population **slope** $\beta_1$ (4/4)

::: highlight-container
::: highlight
7.  Write conclusion for the hypothesis test
:::
:::

We reject the null hypothesis that the slope is 0 at the $5\%$
significance level. There is sufficient evidence that there is
significant association between female life expectancy and female
literacy rates (p-value \< 0.0001).

## Did you notice anything about the p-value?

The p-value of the t-test and F-test are the same!!

-   For the t-test:

```{r}
tidy(model1) %>% gt() %>%
  tab_options(table.font.size = 40)
```

-   For the F-test:

```{r}
anova(model1) %>% tidy() %>% gt() %>%
  tab_options(table.font.size = 40)
```

This is true when we use the F-test for a single coefficient!

# Learning Objectives

1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table

2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0

::: lob
3.  Calculate and interpret the coefficient of determination
:::

4.  Describe the model assumptions made in linear regression using
    ordinary least squares

## Correlation coefficient from 511

::: columns
::: {.column width="45%"}
Correlation coefficient $r$ can tell us about the strength of a
relationship

-   If $r = -1$, then there is a perfect negative linear relationship
    between $X$ and $Y$

-   If $r = 1$, then there is a perfect positive linear relationship
    between $X$ and $Y$

-   If $r = 0$, then there is no linear relationship between $X$ and $Y$

Note: All other values of $r$ tell us that the relationship between $X$
and $Y$ is not perfect. The closer $r$ is to 0, the weaker the linear
relationship.
:::

::: {.column width="55%"}
![](05_SLR_Eval/corr_coef.png){fig-align="center" width="878"}
:::
:::

## Correlation coefficient

::: columns
::: column
The (Pearson) correlation coefficient $r$ of variables $X$ and $Y$ can
be computed using the formula:

$$\begin{aligned}
r  & = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\Big(\sum_{i=1}^n (X_i - \overline{X})^2 \sum_{i=1}^n (Y_i - \overline{Y})^2\Big)^{1/2}} \\
& = \frac{SSXY}{\sqrt{SSX \cdot SSY}}
\end{aligned}$$

we have the relationship

$$\widehat{\beta}_1 = r\frac{SSY}{SSX},\ \ \text{or},\ \  r = \widehat{\beta}_1\frac{SSX}{SSY}$$
:::
::: column
```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false

mx <- mean(gapm$female_literacy_rate_2011, na.rm=T)
my <- mean(gapm$life_expectancy_years_2011, na.rm=T)


ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_vline(xintercept = mx, color = "#FF8021", size = 3) +
   geom_hline(yintercept = my, color = "#A7EA52", size = 3) +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))

```
:::
:::

## Coefficient of determination: $R^2$

It can be shown that the square of the correlation coefficient $r$ is
equal to

$$R^2 = \frac{SSR}{SSY} = \frac{SSY - SSE}{SSY}$$

-   $R^2$ is called the **coefficient of determination**.
-   **Interpretation**: The proportion of variation in the $Y$ values explained by the
        regression model
-   $R^2$ measures the strength of the linear relationship between $X$
    and $Y$:
    -   $R^2 = \pm 1$: Perfect relationship
        -   Happens when $SSE = 0$, i.e. no error, all points on the
            line
    -   $R^2 = 0$: No relationship
        -   Happens when $SSY = SSE$, i.e. using the line doesn't not
            improve model fit over using $\overline{Y}$ to model the $Y$
            values.

## Poll Everywhere Question

## Life expectancy example: correlation coeffiicent $r$ and coefficient of determination $R^2$

::: columns
::: {.column width="75%"}
```{r}
(r = cor(x = gapm$life_expectancy_years_2011, 
    y = gapm$female_literacy_rate_2011,
    use =  "complete.obs"))
r^2
(sum_m1 = summary(model1)) # for R^2 value
sum_m1$r.squared
```
:::

::: {.column width="25%"}
   

::: fact
::: fact-title
Interpretation
:::

::: fact-cont
41.1% of the variation in countries' average life expectancy is
explained by the linear model with female literacy rate as the
independent variable.
:::
:::
:::
:::

## What does $R^2$ not measure?

::: columns
::: {.column width="37%"}
-   $R^2$ is not a measure of the magnitude of the slope of the
    regression line

    -   Example: can have $R^2 = 1$ for many different slopes!!

-   $R^2$ is not a measure of the appropriateness of the straight-line
    model

    -   Example: figure
:::

::: {.column width="63%"}
![](05_SLR_Eval/anscombe.png){fig-align="center"}
:::
:::

# Learning Objectives

1.  Identify different sources of variation in an Analysis of Variance
    (ANOVA) table

2.  Using the F-test, determine if there is enough evidence that
    population slope $\beta_1$ is not 0

3.  Calculate and interpret the coefficient of determination

::: lob
4.  Describe the model assumptions made in linear regression using
    ordinary least squares
:::

## Least-squares model assumptions: eLINE

These are the model assumptions made in ordinary least squares:

 

-   **e** xistence: For any $X$, there exists a distribution for $Y$

 

-   **L** inearity of relationship between variables

 

-   **I** ndependence of the $Y$ values

 

-   **N** ormality of the $Y$'s given $X$ (residuals)

 

-   **E** quality of variance of the residuals (homoscedasticity)

## e: Existence of Y’s distribution

-   For any fixed value of the variable $X$, $Y$ is a
    -   random variable with a certain probability distribution
    -   having finite
        -   mean and
        -   variance
-   This leads to the normality assumption
-   Note: This is not about $Y$ alone, but $Y|X$

## L: Linearity

-   The relationship between the variables is linear (a straight line):
    -   The mean value of $Y$ given $X$, $\mu_{y|x}$ or $E[Y|X]$, is a
        straight-line function of $X$

$$\mu_{y|x} = \beta_0 + \beta_1 \cdot X$$

```{r}
#| echo: false
#| fig-align: center

ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 3, se=FALSE) +
  geom_smooth(method = "lm", se = FALSE, size = 2, colour="#F14124") +
  geom_smooth(size=2) +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)") +
    theme(axis.title = element_text(size = 20), 
        axis.text = element_text(size = 18))
```

## I: Independence of observations

-   The $Y$-values are statistically independent of one another

 

-   Examples of when they are *not* independent, include

     

    -   repeated measures (such as baseline, 3 months, 6 months)

     

    -   data from clusters, such as different hospitals or families

 

-   This condition is checked by reviewing the study *design* and not by
    inspecting the data

 

-   How to analyze data using regression models when the $Y$-values are
    not independent is covered in BSTA 519 (Longitudinal data)

## Poll Everywhere Question

## N: Normality

-   For any fixed value of $X$, $Y$ has normal distribution.
    -   Note: This is not about $Y$ alone, but $Y|X$
-   Equivalently, the measurement (random) errors $\epsilon_i$ ’s
    normally distributed
    -   This is more often what we check
-   We will discuss how to assess this in practice in Chapter 14
    (Regression Diagnostics)

```{r}
#| message: false
#| echo: false
#| fig-height: 8
#| fig-width: 11
#| fig-align: center
# code from https://drsimonj.svbtle.com/visualising-residuals

model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
regression_points <- augment(model1)
# summary(model1)
# sum(model1$residuals^2)

ggplot(regression_points, 
       aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_segment(aes(
    xend = female_literacy_rate_2011, 
    yend = .fitted), 
    alpha = 1, 
    color = "#4FADF3", 
    size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "#F14124", size=3) +
  # > Color adjustments made here...
  geom_point(color = "black", size = 4) +  # Color mapped here
  #scale_color_gradient2(low = "#213c96", mid = "white", high = "#F14124") +  # Colors to use here
    #guides(color = "none") +
  geom_point(aes(y = .fitted), shape = 1, size = 4) +
labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))  
```

## E: Equality of variance of the residuals

-   The variance of $Y$ given $X$ ($\sigma_{Y|X}^2$), is the same for
    any $X$

    -   We use just $\sigma^2$ to denote the common variance

-   This is also called **homoscedasticity**

-   We will discuss how to assess this in practice in Chapter 14
    (Regression Diagnostics)

![](05_SLR_Eval/homosced.webp){fig-align="center"}

## Summary of eLINE model assumptions

-   $Y$ values are independent (check study design!)

<br>

::: columns
::: column
-   The distribution of $Y$ given $X$ is
    -   normal
    -   with mean $\mu_{y|x} = \beta_0 + \beta_1 \cdot X$
    -   and common variance $\sigma^2$
:::

::: column
-   This means that the residuals are
    -   normal
    -   with mean = 0
    -   and common variance $\sigma^2$
:::
:::

## Anscombe's Quartet

![](05_SLR_Eval/anscombe.png){fig-align="center"}

---
title: "SLR: Model Evaluation"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "01/24/2023"
categories: ["Week 1"]
format: 
  revealjs:
    theme: [default, simple_NW.scss]
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: SLR 3
    html-math-method: mathjax
    highlight-style: ayu
execute:
  echo: true
  freeze: auto  # re-render only when source changes
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(openintro)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) 
library(gridExtra) # NEW!!!

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_gray(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

gapm <- read_csv("data/lifeexp_femlit_2011.csv")

model1 <- lm(life_expectancy_years_2011 ~
               female_literacy_rate_2011,
                 data = gapm)
# Get regression table:
reg_tabel = tidy(model1) %>% gt() %>% 
 tab_options(table.font.size = 40) %>%
 fmt_number(decimals = 3)
```

## Topics

-   ANOVA table
-   Explained and unexplained variation
-   F test
-   

# Learning Objectives

1.  

## So far in our regression example...

::: columns
::: {.column width="49%"}
**Lesson 1 of SLR:**

-   Fit regression line
-   Calculate slope & intercept
-   Interpret slope & intercept

**Lesson 2 of SLR:**

-   Estimate variance of the residuals
-   Inference for slope & intercept: CI, p-value
-   Confidence bands of regression line for mean value of Y\|X
:::

::: {.column width="2%"}
:::

::: {.column width="49%"}
```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false

gapm <- read_csv("data/lifeexp_femlit_2011.csv")
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))

```
:::
:::

```{=tex}
\begin{aligned}
\widehat{Y} &= \widehat\beta_0 + \widehat\beta_1 \cdot X\\
\widehat{\text{life expectancy}} &= 50.9 + 0.232 \cdot \text{female literacy rate}
\end{aligned}
```
## Let's revisit the regression analysis process

::: box
![](images/arrow2.png){.absolute top="13.5%" right="62.1%" width="155"}
![](images/arrow2.png){.absolute top="13.5%" right="28.4%"
width="155"}![](images/arrow_back4.png){.absolute top="7.5%"
right="30.5%" width="820"} ![](images/arrow_down.png){.absolute
top="60.5%" right="48%" width="85"}

::: columns
::: {.column width="30%"}
::: RAP1
::: RAP1-title
Model Selection
:::

::: RAP1-cont
-   Building a model

-   Selecting variables

-   Prediction vs interpretation

-   Comparing potential models
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP2
::: RAP2-title
Model Fitting
:::

::: RAP2-cont
-   Find best fit line

-   Using OLS in this class

-   Parameter estimation

-   Categorical covariates

-   Interactions
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP3
::: RAP3-title
Model Evaluation
:::

::: RAP3-cont
-   Evaluation of model fit
-   Testing model assumptions
-   Residuals
-   Transformations
-   Influential points
-   Multicollinearity
:::
:::
:::
:::
:::

::: RAP4
::: RAP4-title
Model Use (Inference)
:::

::: RAP4-cont
::: columns
::: {.column width="50%"}
-   Inference for coefficients
-   Hypothesis testing for coefficients
:::

::: {.column width="50%"}
-   Inference for expected $Y$ given $X$
-   Prediction of new $Y$ given $X$
:::
:::
:::
:::

## Quickly go over our estimates from last class???

Now we are making sure our whole model is trustworthy

# Learning Objectives

1.  

## Explained vs. Unexplained Variation

```{r}
#| echo: false
#| fig-align: center

regression_points <- augment(model1)
# summary(model1)
# sum(model1$residuals^2)

ggplot(regression_points, 
       aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_segment(aes(
    xend = female_literacy_rate_2011, 
    yend = .fitted), 
    alpha = 1, 
    color = "#4FADF3", 
    size = 2) +
  geom_smooth(method = "lm", se = FALSE, color = "#F14124", size=3) +
  # > Color adjustments made here...
  geom_point(color = "black", size = 4) +  # Color mapped here
  #scale_color_gradient2(low = "#213c96", mid = "white", high = "#F14124") +  # Colors to use here
    #guides(color = "none") +
  geom_point(aes(y = .fitted), shape = 1, size = 4) +
labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Average life expectancy vs. female literacy rate in 2011") +
    theme(axis.title = element_text(size = 20), 
        axis.text = element_text(size = 20), 
        title = element_text(size = 20))  
```

$$ \begin{aligned}
Y_i - \bar{Y} & = & (Y_i - \hat{Y}_i) & + (\hat{Y}_i- \bar{Y})\\
\text{Total unexplained variation} & = & \text{Variation due to regression} & + \text{Residual variation after regression}
\end{aligned}$$

## More of the equation

::: columns
::: {.column width="40%"}
$$Y_i - \bar{Y} = (Y_i - \hat{Y}_i) + (\hat{Y}_i- \bar{Y})$$

-   $Y_i - \bar{Y}$ = the deviation of $Y_i$ around the mean $\bar{Y}$
    -   (the **total** amount deviation unexplained at $X_i$ ).
-   $Y_i - \hat{Y}_i$ = the deviation of the observation $Y$ around the
    fitted regression line
    -   (the amount deviation **unexplained** by the regression at $X_i$
        ).
-   $\hat{Y}_i- \bar{Y}$ = the deviation of the fitted value $\hat{Y}_i$
    around the mean $\bar{Y}$
    -   (the amount deviation **explained** by the regression at $X_i$ )
:::

::: {.column width="60%"}
   

![](05_SLR_Eval/SS.png){fig-align="center"}
:::
:::

## How is this actually calculated for our fitted model?

$$ \begin{aligned}
Y_i - \bar{Y} & = & (Y_i - \hat{Y}_i) & + (\hat{Y}_i- \bar{Y})\\
\text{Total unexplained variation} & = & \text{Variation due to regression} & + \text{Residual variation after regression}
\end{aligned}$$

$$\begin{aligned}
\sum_{i=1}^n (Y_i - \bar{Y})^2 & = \sum_{i=1}^n (\hat{Y}_i- \bar{Y})^2 + \sum_{i=1}^n (Y_i - \hat{Y}_i)^2 \\
SSY & = SSR + SSE 
\end{aligned}$$
$$\text{Total Sum of Squares} = \text{Sum of Squares due to Regression} + \text{Sum of Squares due to Error (residuals)}$$

::: columns
::: {.column width="7%"}
:::

::: {.column width="86%"}
ANOVA table:

| Variation Source | df    | SS    | MS                      | test statistic        | p-value |
|------------------|-------|-------|-------------------------|-----------------------|---------|
| Regression       | $1$   | $SSR$ | $MSR = \frac{SSR}{1}$   | $F = \frac{MSR}{MSE}$ |         |
| Error            | $n-2$ | $SSE$ | $MSE = \frac{SSE}{n-2}$ |                       |         |
| Total            | $n-1$ | $SSY$ |                         |                       |         |
:::

::: {.column width="7%"}
:::
:::

## ANOVA table in R

```{r}
# Fit regression model:
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
             data = gapm)

anova(model1)
anova(model1) %>% tidy() %>% gt() %>%
   tab_options(table.font.size = 40) %>%
   fmt_number(decimals = 3)
```

## What is the F statistic testing?

$$F = \frac{MSR}{MSE}$$

-   It can be shown that

$$E(MSE)=\sigma^2\ \text{and}\ E(MSR) = \sigma^2 + \beta_1^2\sum_{i=1}^n (X_i- \bar{X})^2$$

-   Recall that $\sigma^2$ is the variance of the residuals
-   Thus if
    -   $\beta_1 = 0$, then
        $F \approx \frac{\hat{\sigma}^2}{\hat{\sigma}^2} = 1$
    -   $\beta_1 \neq 0$, then
        $F \approx \frac{\hat{\sigma}^2 + \hat{\beta}_1^2\sum_{i=1}^n (X_i- \bar{X})^2}{\hat{\sigma}^2} > 1$
-   So the $F$ statistic can be used to test $\beta_1$

## F-test for the slope

**Hypotheses**

$$H_0: \beta_1 = 0\\
H_A: \beta_1 \neq 0$$

**Test statistic and probability distribution**

$$F = \frac{MSR}{MSE} \sim F_{1, n-2}$$

-   The F statistic has an $F$-distribution with numerator $df = 1$ and
    denominator $df = n-2$.

**Decision**

**Critical value** approach:

Let $F_{1, n-2, 1-\alpha}$ be the critical value for the $F_{1, n-2}$
distribution with area $\alpha$ to the right.

-   If $F > F_{1, n-2, 1-\alpha}$, then reject $H_0$
-   If $F \leq F_{1, n-2, 1-\alpha}$, then fail to reject $H_0$

$p$-value approach:

-   If $p$-value $< \alpha$, then reject $H_0$
-   If $p$-value $\geq \alpha$, then fail to reject $H_0$

**Conclusion**

If $H_0$ is rejected, we conclude there is sufficient evidence that
there exists a linear relationship between $X$ and $Y$
($\beta_1 \neq 0$).

## F-test vs. t-test for the slope

The square of a $t$-distribution with $df = \nu$ is an $F$-distribution
with $df = 1, \nu$

$$T_{\nu}^2 \sim F_{1,\nu}$$

## Example: F-test for the slope

**Hypotheses**

$$H_0: \beta_1 = 0\\
H_A: \beta_1 \neq 0$$

**Test statistic and probability distribution**

$$F = \frac{MSR}{MSE} = \frac{2052.81}{37.73} = 54.414 \sim F_{1, 78}$$

**Decision**

**Critical value** approach:

```{r}
# critical value for F 1,78 with alpha = 0.05:
qf(.95, df1 = 1, df2 = 78)
```

Since $F = 54.414 > `r qf(.95, df1 = 1, df2 = 78)`$, we reject $H_0$.

$p$-value approach:

```{r}
# p-value is ALWAYS the right tail for F-test
pf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)
```

Since
$p\text{-value} = `r pf(54.414, df1 = 1, df2 = 78, lower.tail = FALSE)` < 0.05$,
we reject $H_0$.

**Conclusion**

There is sufficient evidence that there exists a linear relationship
between countries' female literacy rate and average life expectancy (p
\< 0.001).

# Chapter 6: The Correlation Coefficient and Straight-line Regression Analysis

## Correlation coefficient

The (Peasron) correlation coefficient $r$ of variables $X$ and $Y$ can
be computed using the formula:

$$r = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\Big(\sum_{i=1}^n (X_i - \bar{X})^2 \sum_{i=1}^n (Y_i - \bar{Y})^2\Big)^{1/2}} 
= \frac{SSXY}{\sqrt{SSX \cdot SSY}}$$

Since (see Ch 5)

$$\widehat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \bar{X})(Y_i - \bar{Y})}{\sum_{i=1}^n (X_i - \bar{X})^2}$$
we have the relationship

$$\widehat{\beta}_1 = r\frac{s_Y}{s_X},\ \ \text{or},\ \  r = \widehat{\beta}_1\frac{s_X}{s_Y}$$

## Coefficient of determination: $R^2$

It can be shown that the square of the correlation coefficient $r$ is
equal to

$$R^2 = \frac{SSR}{SSY} = \frac{SSY - SSE}{SSY}$$

-   $R^2$ is called the **coefficient of determination**.
-   Interpretation #1
    -   Numerator is the **reduction** in $SSY$ due to using $X$ to
        predict $Y$
    -   Thus the fraction is the proportionate reduction
-   Interpretation #2
    -   The proportion of variation in the $Y$ values explained by the
        regression model
-   $R^2$ measures the strength of the linear relationship between $X$
    and $Y$:
    -   $R^2 = \pm 1$: Perfect relationship
        -   Happens when $SSE = 0$, i.e. no error, all points on the
            line
    -   $R^2 = 0$: No relationship
        -   Happens when $SSY = SSE$, i.e. using the line doesn't not
            improve model fit over using $\bar{Y}$ to model the $Y$
            values.

## R Example

```{r}
cor(x = gapm$life_expectancy_years_2011, 
    y = gapm$female_literacy_rate_2011)

r <- cor(x = gapm$life_expectancy_years_2011, 
    y = gapm$female_literacy_rate_2011,
    use =  "complete.obs")
r
r^2
summary(model1) # for R^2 value
```

## Test $\rho = 0$

$\rho$ is the population parameter for the correlation coefficient $r$

**Hypotheses**

$$H_0: \rho = 0 \ \ \text{vs.} \ \ H_A: \rho \neq 0$$

**Test statistic and probability distribution**

$$T = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}} \sim t_{n-2}$$

-   The t statistic has a Student's $t$-distribution with $df = n-2$.

**Decision**

**Critical value** approach:

Let $t_{n-2, 1-\alpha/2}$ be the critical value for the $t_{n-2}$
distribution with area $\alpha/2$ in each of the tails.

-   If $|T| > t_{n-2, 1-\alpha/2}$, then reject $H_0$
-   If $|T| \leq t_{n-2, 1-\alpha/2}$, then fail to reject $H_0$

$p$-value approach:

-   If $p$-value $< \alpha$, then reject $H_0$
-   If $p$-value $\geq \alpha$, then fail to reject $H_0$

**Conclusion**

If $H_0$ is rejected, we conclude there is sufficient evidence that
there exists a linear correlation between $X$ and $Y$ ($\rho \neq 0$).

### Example: Test $\rho = 0$

**Hypotheses**

$$H_0: \rho = 0 \ \ \text{vs.} \ \ H_A: \rho \neq 0$$

**Test statistic and probability distribution**

```{r}
r # calculated previously
(teststat <- r*sqrt(78)/sqrt(1-r^2))
```

$$T = \frac{r\sqrt{n-2}}{\sqrt{1-r^2}} = 
\frac{`r r`\sqrt{80-2}}{\sqrt{1-`r r`^2}} = `r teststat` \sim t_{78}$$

```{r}
# critical value for t78 with alpha = 0.05:
qt(.975, df = 78)
```

Since $t = 7.376557 > `r qt(.975, df = 78)`$, we reject $H_0$.

$p$-value approach:

```{r}
2*pt(7.376557, df = 78, lower.tail = FALSE)
```

Since
$p\text{-value} = `r 2*pt(7.376557, df = 78, lower.tail = FALSE)` < 0.05$,
we reject $H_0$.

```{r}
cor.test(x = gapm$life_expectancy_years_2011, 
         y = gapm$female_literacy_rate_2011)

cor.test(x = gapm$life_expectancy_years_2011, 
         y = gapm$female_literacy_rate_2011) %>% tidy()

cor.test(x = gapm$life_expectancy_years_2011, 
         y = gapm$female_literacy_rate_2011) %>% tidy() %>% gt()
```

## CI for $\rho$

```{r}
r
(tstar <- qt(.975, 78))
(SErho <- sqrt((1-r^2)/sqrt(78)))
r + tstar*SErho
r - tstar*SErho
```

$$r \pm t^* \cdot \frac{\sqrt{1-r^2}}{\sqrt{n-2}}\\
`r r` \pm `r qt(.975, 78)` \cdot `r SErho`\\
(`r r - tstar*SErho`, `r r + tstar*SErho`) \\
(`r r - tstar*SErho`, 1)$$

## Testing $\rho$ for some value $\rho_0 \neq 0$

**Hypotheses**

$$H_0: \rho = \rho_0 \ \ \text{vs.} \ \ H_A: \rho \neq \rho_0$$

**Test statistic and probability distribution**

-   When $\rho \neq 0$, the distribution of the test statistic we used
    before is skewed
-   Thus we apply Fisher's Z transformation:
    $\frac{1}{2} \ln \frac{1+r}{1-r}$, with which the test statistic has
    an approximately standard normal distribution:

$$Z = \frac{\frac{1}{2} \ln \frac{1+r}{1-r} - \frac{1}{2} \ln \frac{1+\rho_0}{1-\rho_0}}{\frac{1}{\sqrt{n-3}}} \sim N(0,1)$$

**Decision**

**Critical value** approach:

Let $z_{1-\alpha/2}$ be the critical value for the $N(0,1)$ distribution
with area $\alpha/2$ in each of the tails.

-   If $|Z| > z_{1-\alpha/2}$, then reject $H_0$
-   If $|Z| \leq z_{1-\alpha/2}$, then fail to reject $H_0$

$p$-value approach:

-   If $p$-value $< \alpha$, then reject $H_0$
-   If $p$-value $\geq \alpha$, then fail to reject $H_0$

**Conclusion**

If $H_0$ is rejected, we conclude there is sufficient evidence that the
linear correlation between $X$ and $Y$ is significantly different from
$\rho_0$.

### Example

Test whether the correlation is different from 0.5.

**Hypotheses**

$$H_0: \rho = 0.5 \ \ \text{vs.} \ \ H_A: \rho \neq 0.5$$

**Test statistic and probability distribution**

```{r}
r # calculated previously
rho0 <- 0.5
(fisher_r <- 0.5*log((1+r)/(1-r)))
(fisher_rho0 <- 0.5*log((1+rho0)/(1-rho0)))
(z_stat <- (fisher_r - fisher_rho0) * sqrt(77))

```

$$Z = \frac{\frac{1}{2} \ln \frac{1+r}{1-r} - \frac{1}{2} \ln \frac{1+\rho_0}{1-\rho_0}}{\frac{1}{\sqrt{n-3}}} \\
= \frac{\frac{1}{2} \ln \frac{1+`r r`}{1-`r r`} - \frac{1}{2} \ln \frac{1+0.5}{1-0.5}}{\frac{1}{\sqrt{80-3}}} \\
= `r z_stat`$$

```{r}
# critical value for N(0,1) with alpha = 0.05:
qnorm(.975)
```

Since $Z = 1.848331 < `r qnorm(.975)`$, we fail to reject $H_0$.

$p$-value approach:

```{r}
2*pnorm(1.848331, lower.tail = FALSE)
```

Since
$p\text{-value} = `r 2*pnorm(1.848331, lower.tail = FALSE)` > 0.05$, we
fail to reject $H_0$.

**Conclusion**

There is insufficient evidence that the linear correlation between
countries' female literacy rate and average life expectancy is
significantly different from 0.5 (p = 0.06).

## CI for $\rho$ when $\rho \neq 0$

-   CI bounds for the Fisher transformed correlations:

$$\frac{1}{2} \ln \frac{1+r}{1-r} \pm z^* \cdot \frac{1}{\sqrt{n-3}}$$
\* After finding CI bounds ($z$), back transform the values to $r$:

$$r = \frac{e^{2z}-1}{e^{2z}+1}$$

### Example: CI for $\rho$ when $\rho \neq 0$

```{r}
fisher_r
(zstar <- qnorm(.975))
(SE_fisher <- 1/sqrt(77))

# Fisher transformed CI bounds
(LB_fish <- fisher_r - zstar * SE_fisher)
(UB_fish <- fisher_r + zstar * SE_fisher)

# Back transform to r values
(LB_r <- (exp(2*LB_fish) - 1)/(exp(2*LB_fish)+1))
(UB_r <- (exp(2*UB_fish) - 1)/(exp(2*UB_fish)+1))

```

-   CI bounds for the Fisher transformed correlations:

$$\frac{1}{2} \ln \frac{1+r}{1-r} \pm z^* \cdot \frac{1}{\sqrt{n-3}}\\
\frac{1}{2} \ln \frac{1+`r r`}{1-`r r`} \pm `r zstar` \cdot \frac{1}{\sqrt{77}}\\
`r fisher_r` \pm `r zstar` \cdot `r SE_fisher`\\
(`r LB_fish`, `r UB_fish`)$$ \* After finding CI bounds ($z$), back
transform the values to $r$:

$$LB = \frac{e^{2z}-1}{e^{2z}+1} = \frac{e^{2`r LB_fish`}-1}{e^{2`r LB_fish`}+1} = `r LB_r`\\
UB = \frac{e^{2z}-1}{e^{2z}+1} = \frac{e^{2`r UB_fish`}-1}{e^{2`r UB_fish`}+1} = `r UB_r`$$

We are 95% confident that the linear correlation coefficient between
countries' female literacy rate and average life expectancy is within
(`r round(LB_r, 2)`, `r round(UB_r, 2)`).

## CI using `psych` package

```{r}
# First install the psych package
# install.packages("psych")
library(psych)

# Fisher transformed values
fisherz(0.5)
fisherz(r)
# compare with values from computations above
fisher_rho0
fisher_r

#----
# CI with back transformed values
r.con(rho = r, n = 80, p=.95, twotailed=TRUE)
# compare with values from computations above
LB_r 
UB_r 
```

# $R^2$ = Coefficient of determination

Another way to assess model fit

## $R^2$ = Coefficient of determination (1/2)

-   Recall that the correlation coefficient $r$ measures the strength of
    the linear relationship between two numerical variables
-   $R^2$ is usually used to measure the strength of a *linear fit*
    -   For a simple linear regression model (one numerical predictor),
        $R^2$ is just the square of the correlation coefficient
-   In general, $R^2$ is the proportion of the variability of the
    dependent variable that is **explained** by the independent
    variable(s)

$$R^2 = \frac{\textrm{variance of predicted y-values}}
{\textrm{variance of observed y-values}} = \frac{\sum_{i=1}^n(\widehat{y}_i-\bar{y})^2}
{\sum_{i=1}^n(y_i-\bar{y})^2} 
 = \frac{s_y^2 - s_{\textrm{residuals}}^2}
{s_y^2}$$ $$R^2 = 1- \frac{s_{\textrm{residuals}}^2}
{s_y^2}$$ where $\frac{s_{\textrm{residuals}}^2}{s_y^2}$ is the
proportion of "unexplained" variability in the $y$ values,\
and thus $R^2 = 1- \frac{s_{\textrm{residuls}}^2}{s_y^2}$ is the
proportion of "explained" variability in the $y$ values

## $R^2$ = Coefficient of determination (2/2)

-   Recall, $-1<r<1$

-   Thus, $0<R^2<1$

-   In practice, we want "high" $R^2$ values, i.e. $R^2$ as close to 1
    as possible.

Calculating $R^2$ in R using `glance()` from the `broom` package:

```{r}
glance(model1)
glance(model1)$r.squared
```

::: callout-warning
-   A model can have a high $R^2$ value when there is a curved pattern.
-   Always first check whether a linear model is reasonable or not.
:::

## $R^2$ in `summary()` R output

```{r}
summary(model1)
```

Compare to the square of the correlation coefficient $r$:

```{r}
r <- cor(x = gapm$life_expectancy_years_2011, 
    y = gapm$female_literacy_rate_2011,
    use =  "complete.obs")
r
r^2
```

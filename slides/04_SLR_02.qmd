---
title: "Simple Linear Regression (SLR)"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "01/17/2023"
categories: ["Week 1"]
format: 
  revealjs:
    theme: [default, simple_NW.scss]
    toc: true
    toc-depth: 1
    toc-title: Class Overview
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: SLR 1
    html-math-method: mathjax
    highlight-style: ayu
execute:
  echo: true
  freeze: auto  # re-render only when source changes
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(openintro)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) 
library(gridExtra) # NEW!!!

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_gray(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

```

```{r}
gapm <- read_csv("data/lifeexp_femlit_2011.csv")
```

# Learning Objectives

1.  Identify the aims of your research and see how they align with the
    intended purpose of simple linear regression

2.  Identify the simple linear regression model and define statistics
    language for key notation

3.  Illustrate how ordinary least squares (OLS) finds the best model
    parameter estimates

4.  Solve the optimal coefficient estimates for simple linear regression
    using OLS

5.  Apply OLS in R for simple linear regression of real data

## Topics

Topics for SLR:

assumptions

parameter estimation and interpretation

properties of LSE

estimation of variance?

Topics

-   Inference for slope and intercept

    -   CI’s and hypothesis tests

-   Inference for mean value of y at specific values of x

    -   Confidence bands of best-fit line

-   Prediction intervals for individual predictions

    -   Prediction bands

-   *F-test for the slope*

-   *Pearson correlation coefficient r*

-   *Coefficient of determination R2*

-   *Testing the correlation coefficient !*

-   *CI for the correlation coefficient !*

# Next class???

## What are the LINE conditions?

For "good" model fit and to be able to make inferences and predictions
based on our models, 4 conditions need to be satisfied.

Briefly:

-   **L** inearity of relationship between variables
-   **I** ndependence of the Y values
-   **N** ormality of the residuals
-   **E** quality of variance of the residuals (homoscedasticity)

[More in
depth](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions):

-   **L** : there is a linear relationship between the mean response (Y)
    and the explanatory variable (X),
-   **I** : the errors are independent—there’s no connection between how
    far any two points lie from the regression line,
-   **N** : the responses are normally distributed at each level of X,
    and
-   **E** : the variance or, equivalently, the standard deviation of the
    responses is equal for all levels of X.

## L: Linearity of relationship between variables

Is the association between the variables linear?

-   Diagnostic tools:
    -   Scatterplot
    -   Residual plot (see later section for E : Equality of variance of
        the residuals)

```{r}
#| echo: false
#| fig.width: 12.0
#| fig.height: 7.0
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate in 2011") +  
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(se = FALSE, color = "black")
```

## I: Independence of the residuals ($Y$ values)

-   **Are the data points independent of each other?**

-   Examples of when they are *not* independent, include

    -   repeated measures (such as baseline, 3 months, 6 months)
    -   data from clusters, such as different hospitals or families

-   This condition is checked by reviewing the study *design* and not by
    inspecting the data

-   How to analyze data using regression models when the $Y$-values are
    not independent is covered in BSTA 519 (Longitudinal data)

# N: Normality of the residuals

-   Extract residuals from regression model in R
-   Diagnostic tools:
    -   Distribution plots of residuals
    -   QQ plots

## N: Normality of the residuals

-   The responses Y are normally distributed at each level of x

![](/img_slides/OLSassumptions-1.png){fig-align="center"}

::: {style="font-size: 60%;"}
<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>
:::

## Extract model's residuals in R

-   First extract the residuals' values from the model output using the
    `augment()` function from the `broom` package.
-   Get a tibble with the orginal data, as well as the residuals and
    some other important values.

```{r}
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, 
                data = gapm)
aug1 <- augment(model1) 

glimpse(aug1)
```

## Check normality with "usual" distribution plots

Note that below I save each figure, and then combine them together in
one row of output using `grid.arrange()` from the `gridExtra` package.

```{r}
#| fig.height: 4.0
#| fig.width: 12.0
hist1 <- ggplot(aug1, aes(x = .resid)) +
  geom_histogram()

density1 <- ggplot(aug1, aes(x = .resid)) +
  geom_density()

box1 <- ggplot(aug1, aes(x = .resid)) +
  geom_boxplot()

library(gridExtra) # NEW!!!
grid.arrange(hist1, density1, box1, nrow = 1)
```

## Normal QQ plots (QQ = quantile-quantile)

-   It can be tricky to eyeball with a histogram or density plot whether
    the residuals are normal or not
-   QQ plots are often used to help with this

::: columns
::: {.column width="60%"}
-   *Vertical axis*: **data quantiles**
    -   data points are sorted in order and
    -   assigned quantiles based on how many data points there are
-   *Horizontal axis*: **theoretical quantiles**
    -   mean and standard deviation (SD) calculated from the data points
    -   theoretical quantiles are calculated for each point, assuming
        the data are modeled by a normal distribution with the mean and
        SD of the data
:::

::: {.column width="40%"}
```{r}
#| fig.width: 5.0
#| fig.height: 5.0
#| echo: false
ggplot(aug1, aes(sample = .resid)) + 
  stat_qq() +     # points
  stat_qq_line()  # line
```
:::
:::

-   **Data are approximately normal if points fall on a line.**

See more info at
<https://data.library.virginia.edu/understanding-QQ-plots/>

## Examples of Normal QQ plots (1/5)

-   **Data**:
    -   Body measurements from 507 physically active individuals
    -   in their 20's or early 30's
    -   within normal weight range.

![](/img_slides/qq_wristdiam.png){fig-align="center"}

## Examples of Normal QQ plots (2/5)

Skewed right distribution

<br>

![](/img_slides/qq_weights.png){fig-align="center"}

## Examples of Normal QQ plots (3/5)

Long tails in distribution

<br>

![](/img_slides/qq_biliac.png){fig-align="center"}

## Examples of Normal QQ plots (4/5)

Bimodal distribution

<br>

![](/img_slides/qq_forearm.png){fig-align="center"}

## Examples of Normal QQ plots (5/5)

![](/img_slides/qq_forearm_gender.png){fig-align="center"}

## QQ plot of residuals of `model1`

```{r}
#| fig.width: 12.0
#| fig.height: 3.0
#| echo: false
grid.arrange(hist1, density1, box1, nrow = 1)
```

::: columns
::: {.column width="50%"}
<br>

```{r}
#| label: QQresid
#| fig.show: hide
#| fig.width: 5.0
#| fig.height: 5.0

ggplot(aug1, aes(sample = .resid)) + 
  stat_qq() +     # points
  stat_qq_line()  # line
```
:::

::: {.column width="50%"}
```{r}
#| ref.label: QQresid
#| echo: false
#| fig.width: 5.0
#| fig.height: 4.0
```
:::
:::

## Compare to randomly generated Normal QQ plots

How "*good*" we can expect a QQ plot to look depends on the sample size.

-   The QQ plots on the next slides are randomly generated

    -   using random samples from actual standard normal distributions
        $N(0,1)$.

-   Thus, all the points in the QQ plots **should theoretically** fall
    in a line

-   However, there is sampling variability...

## Randomly generated Normal QQ plots: n=100

-   Note that `stat_qq_line()` doesn't work with randomly generated
    samples, and thus the code below manually creates the line that the
    points should be on (which is $y=x$ in this case.)

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 100

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}
```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::

## Examples of simulated Normal QQ plots: n=10

With fewer data points,

-   simulated QQ plots are more likely to look "less normal"
-   even though the data points were sampled from normal distributions.

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 10  # only change made to code!

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}
```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::

## Examples of simulated Normal QQ plots: n=1,000

With more data points,

-   simulated QQ plots are more likely to look "more normal"

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 1000 # only change made to code!

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}
```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::

## Back to our example

::: columns
::: {.column width="40%"}
Residuals from Life Expectancy vs. Female Literacy Rate Regression

```{r}
#| label: QQplot
#| fig.show: hide
ggplot(aug1, 
      aes(sample = .resid)) + 
  stat_qq() + 
  stat_qq_line() 
```
:::

::: {.column width="60%"}
```{r}
#| ref.label: QQplot
#| echo: false
```
:::
:::

Simulated QQ plot of Normal Residuals with n = 80

::: columns
::: {.column width="40%"}
```{r}
# number of observations 
# in fitted model
nobs(model1) 
```

```{r}
#| label: QQplot_sim
#| fig.show: hide
ggplot() +
  stat_qq(aes(
    sample = rnorm(80))) + 
  geom_abline(
    intercept = 0, slope = 1, 
    color = "blue")
```
:::

::: {.column width="60%"}
```{r}
#| ref.label: QQplot_sim
#| echo: false
```
:::
:::

# E: Equality of variance of the residuals {.nostretch}

-   Homoscedasticity
-   Diagnostic tool: **residual plot**

![](/img_slides/OLSassumptions-1.png){fig-align="center" width="60%"}

::: {style="font-size: 60%;"}
<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>
:::

## Residual plot

-   $x$ = explanatory variable from regression model
    -   (or the fitted values for a multiple regression)
-   $y$ = residuals from regression model

::: columns
::: {.column width="50%"}
```{r}
names(aug1)
```

<br>

```{r}
#| label: resids_plot
#| fig.show: hide
ggplot(aug1, 
       aes(x = female_literacy_rate_2011, 
           y = .resid)) + 
  geom_point() +
  geom_abline(
    intercept = 0, 
    slope = 0, 
    color = "orange") +
  labs(title = "Residual plot")
```
:::

::: {.column width="50%"}
```{r}
#| ref.label: resids_plot
#| echo: false
#| fig.height: 6
#| fig.width: 6
```
:::
:::

## E: Equality of variance of the residuals (Homoscedasticity)

-   The **variance** or, equivalently, the standard deviation of the
    responses is **equal for all values of x**.
-   This is called **homoskedasticity** (top row)
-   If there is **heteroskedasticity** (bottom row), then the assumption
    is not met.

![](/img_slides/heteroskedastic.png){fig-align="center"}

# $R^2$ = Coefficient of determination

Another way to assess model fit

## $R^2$ = Coefficient of determination (1/2)

-   Recall that the correlation coefficient $r$ measures the strength of
    the linear relationship between two numerical variables
-   $R^2$ is usually used to measure the strength of a *linear fit*
    -   For a simple linear regression model (one numerical predictor),
        $R^2$ is just the square of the correlation coefficient
-   In general, $R^2$ is the proportion of the variability of the
    dependent variable that is **explained** by the independent
    variable(s)

$$R^2 = \frac{\textrm{variance of predicted y-values}}
{\textrm{variance of observed y-values}} = \frac{\sum_{i=1}^n(\widehat{y}_i-\bar{y})^2}
{\sum_{i=1}^n(y_i-\bar{y})^2} 
 = \frac{s_y^2 - s_{\textrm{residuals}}^2}
{s_y^2}$$ $$R^2 = 1- \frac{s_{\textrm{residuals}}^2}
{s_y^2}$$ where $\frac{s_{\textrm{residuals}}^2}{s_y^2}$ is the
proportion of "unexplained" variability in the $y$ values,\
and thus $R^2 = 1- \frac{s_{\textrm{residuls}}^2}{s_y^2}$ is the
proportion of "explained" variability in the $y$ values

## $R^2$ = Coefficient of determination (2/2)

-   Recall, $-1<r<1$

-   Thus, $0<R^2<1$

-   In practice, we want "high" $R^2$ values, i.e. $R^2$ as close to 1
    as possible.

Calculating $R^2$ in R using `glance()` from the `broom` package:

```{r}
glance(model1)
glance(model1)$r.squared
```

::: callout-warning
-   A model can have a high $R^2$ value when there is a curved pattern.
-   Always first check whether a linear model is reasonable or not.
:::

## $R^2$ in `summary()` R output

```{r}
summary(model1)
```

Compare to the square of the correlation coefficient $r$:

```{r}
r <- cor(x = gapm$life_expectancy_years_2011, 
    y = gapm$female_literacy_rate_2011,
    use =  "complete.obs")
r
r^2
```

# Regression inference

1.  Inference for population [**slope**]{style="color:green"} $\beta_1$
2.  CI for mean response $\mu_{Y|x^*}$
3.  Prediction interval for predicting **individual** observations

## Inference for population [**slope**]{style="color:green"} $\beta_1$

```{r}
# Fit regression model:
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
# Get regression table:
tidy(model1, conf.int = TRUE) %>% gt() # conf.int = TRUE part is new! 
```

```{=tex}
\begin{align}
\widehat{y} =& b_0 + b_1 \cdot x\\
\widehat{\text{life expectancy}} =& 50.9 + 0.232 \cdot \text{female literacy rate}
\end{align}
```
-   What are $H_0$ and $H_A$?
-   How do we calculate the standard error, statistic, *p*-value, and
    CI?

::: callout-note
-   We can also test & calculate CI for the population intercept
-   This will be covered in BSTA 512
:::

## Inference for the population [**slope**]{style="color:green"}: CI and hypothesis test

::: columns
::: {.column width="50%"}
**Population model**\
*line + random "noise"*

$$Y = \beta_0 + \beta_1 \cdot X + \varepsilon$$ with
$\varepsilon \sim N(0,\sigma)$\
$\sigma$ is the variability (SD) of the residuals

<br>

**Sample best-fit (least-squares) line:**

$$\widehat{y} = b_0 + b_1 \cdot x $$

Note: Some sources use $\widehat{\beta}$ instead of $b$.
:::

::: {.column width="50%"}
-   Construct a **95% confidence interval** for the **population slope**
    $\beta_1$

<br>

-   Conduct the **hypothesis test**

```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
<br>

*Note: R reports p-values for 2-sided tests*
:::
:::

## CI for population [**slope**]{style="color:green"} $\beta_1$

Recall the general CI formula:

$$\textrm{Point Estimate} \pm t^*\cdot SE_{\textrm{Point Estimate}}$$

For the CI of the coefficient $b_1$ this translates to

$$b_1 \pm t^*\cdot SE_{b_1}$$ where $t^*$ is the critical value from a
$t$-distribution with $df = n -2$.

<br>

*How is* $\text{SE}_{b_1}$ *calculated?* See next slide.

<br>

```{r}
tidy(model1, conf.int = TRUE)
```

## Standard error of fitted slope $b_1$

::: columns
::: {.column width="50%"}
$$\text{SE}_{b_1} = \frac{s_{\textrm{residuals}}}{s_x\sqrt{n-1}}$$
:::

::: {.column width="50%"}
$\text{SE}_{b_1}$ is the **variability** of the statistic $b_1$
:::
:::

::: {style="font-size: 90%;"}
::: columns
::: {.column width="32%"}
-   $s_{\textrm{residuals}}^2$ is the sd of the residuals
:::

::: {.column width="1%"}
:::

::: {.column width="32%"}
-   $s_x$ is the sample sd of the explanatory variable $x$
:::

::: {.column width="1%"}
:::

::: {.column width="32%"}
-   $n$ is the sample size, or the number of (complete) pairs of points
:::
:::
:::

```{r}
glance(model1)

# standard deviation of the residuals (Residual standard error in summary() output)
(s_resid <- glance(model1)$sigma)

# standard deviation of x's
(s_x <- sd(gapm$female_literacy_rate_2011))

# number of pairs of complete observations
(n <- nobs(model1))

(se_b1 <- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output
```

## Calculate CI for population [**slope**]{style="color:green"} $\beta_1$

::: columns
::: {.column width="50%"}
$$b_1 \pm t^*\cdot SE_{b_1}$$
:::

::: {.column width="50%"}
where $t^*$ is the $t$-distribution critical value with $df = n -2$.
:::
:::

```{r}
tidy(model1, conf.int = TRUE) %>% gt()
```

Save regression output for the row with the slope's information:

```{r}
model1_b1 <-tidy(model1) %>% filter(term == "female_literacy_rate_2011")
model1_b1 %>% gt()
```

::: columns
::: {.column width="50%"}
Save values needed for CI:

```{r}
b1 <- model1_b1$estimate
SE_b1 <- model1_b1$std.error
```

```{r}
nobs(model1) # sample size n
(tstar <- qt(.975, df = 80-2))
```
:::

::: {.column width="50%"}
Compare CI bounds below with the ones in the regression table above.

```{r}
(CI_LB <- b1 - tstar*SE_b1)
(CI_UB <- b1 + tstar*SE_b1)
```
:::
:::

## Hypothesis test for population [**slope**]{style="color:green"} $\beta_1$

::: columns
::: {.column width="40%"}
```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
:::

::: {.column width="60%"}
The **test statistic** for $b_1$ is

$$t = \frac{ b_1 - \beta_1}{ \text{SE}_{b_1}} = \frac{ b_1}{ \text{SE}_{b_1}}$$

when we assume $H_0: \beta_1 = 0$ is true.
:::
:::

```{r}
tidy(model1, conf.int = TRUE) %>% gt()
```

Calculate the test statistic using the values in the regression table:

```{r}
# recall model1_b1 is regression table restricted to b1 row
(TestStat <- model1_b1$estimate / model1_b1$std.error)
```

Compare this test statistic value to the one from the regression table
above

## $p$-value for testing population [**slope**]{style="color:green"} $\beta_1$

-   As usual, the $p$-value is the *probability of obtaining a test
    statistic* **just as extreme or more extreme** *than the observed
    test statistic assuming the null hypothesis* $H_0$ *is true.*

-   To calculate the $p$-value, we need to know the probability
    distribution of the test statistic (the *null distribution*)
    assuming $H_0$ is true.

-   Statistical theory tells us that the test statistic $t$ can be
    modeled by a [$t$-distribution]{style="color:green"} with
    [$df = n-2$]{style="color:green"}.

-   Recall that this is a 2-sided test:

```{r}
(pv = 2*pt(TestStat, df=80-2, lower.tail=F))
```

Compare the $p$-value to the one from the regression table below

```{r}
tidy(model1, conf.int = TRUE) %>% gt()  # compare p-value calculated above to p-value in table
```

# Prediction (& inference)

1.  Prediction for mean response
2.  Prediction for new individual observation

## Prediction with regression line

```{r}
#| echo: false
tidy(model1) %>% gt()
```

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot \textrm{female literacy rate} $$

What is the predicted life expectancy for a country with female literacy
rate 60%?

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot 60 = `r 50.9 + 0.232*60`$$

```{r}
(y_60 <- 50.9 + 0.232*60)
```

<br>

-   How do we interpret the predicted value?
-   How variable is it?

## Prediction with regression line

::: columns
::: {.column width="50%"}
Recall the population model:

*line + random "noise"*

$$Y = \beta_0 + \beta_1 \cdot X + \varepsilon$$ with
$\varepsilon \sim N(0,\sigma)$\
$\sigma$ is the variability (SD) of the residuals

<br>

-   When we take the expected value, at a given value $x^*$, we have
    that the predicted response is the average expected response at
    $x^*$:

$$\widehat{E[Y|x^*]} = b_0 + b_1 x^*$$
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig.height: 8.0
#| fig.width: 8.0
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate") +  
  geom_smooth(method = "lm", se = TRUE) +
  geom_vline(xintercept = 60, color = "green3")
```
:::
:::

-   These are the points on the regression line.
-   The mean responses has variability, and we can calculate a CI for
    it, for every value of $x^*$.

## CI for mean response $\mu_{Y|x^*}$

$$\widehat{E[Y|x^*]} \pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}$$

-   $SE_{\widehat{E[Y|x^*]}}$ is calculated using

$$SE_{\widehat{E[Y|x^*]}} = s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}$$

-   $\widehat{E[Y|x^*]}$ is the predicted value at the specified point
    $x^*$ of the explanatory variable
-   $s_{\textrm{residuals}}^2$ is the sd of the residuals
-   $n$ is the sample size, or the number of (complete) pairs of points
-   $\bar{x}$ is the sample mean of the explanatory variable $x$
-   $s_x$ is the sample sd of the explanatory variable $x$

<br>

-   Recall that $t_{n-2}^*$ is calculated using `qt()` and depends on
    the confidence level.

## Example: CI for mean response $\mu_{Y|x^*}$

**Find the 95% CI for the mean life expectancy when the female literacy
rate is 60.**

::: {style="font-size: 80%;"}
```{=tex}
\begin{align}
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}\\
64.8596 &\pm 1.990847 \cdot s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}\\
64.8596 &\pm 1.990847 \cdot 6.142157 \sqrt{\frac{1}{80} + \frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\
64.8596 &\pm 1.990847 \cdot 0.9675541\\
64.8596 &\pm 1.926252\\
(62.93335 &, 66.78586)
\end{align}
```
:::

::: {style="font-size: 90%;"}
::: columns
::: {.column width="50%"}
```{r}
(Y60 <- 50.9278981 + 0.2321951 * 60)
(tstar <- qt(.975, df = 78))
(s_resid <- glance(model1)$sigma)
```
:::

::: {.column width="50%"}
```{r}
(n <- nobs(model1))
(mx <- mean(gapm$female_literacy_rate_2011))
(s_x <- sd(gapm$female_literacy_rate_2011))
```
:::
:::

```{r}
(SE_Yx <- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))
```

::: columns
::: {.column width="32%"}
```{r}
(MOE_Yx <- SE_Yx*tstar)
```
:::

::: {.column width="1%"}
:::

::: {.column width="32%"}
```{r}
Y60 - MOE_Yx
```
:::

::: {.column width="1%"}
:::

::: {.column width="32%"}
```{r}
Y60 + MOE_Yx
```
:::
:::
:::

## Example: Using R for CI for mean response $\mu_{Y|x^*}$

**Find the 95% CI's for the mean life expectancy when the female
literacy rate is 40, 60, and 80.**

-   Use the base R `predict()` function
-   Requires specification of a `newdata` "value"
    -   The `newdata` value is $x^*$
    -   This has to be in the format of a data frame though
    -   with column name identical to the predictor variable in the
        model

```{r}
newdata <- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) 
newdata
```

::: columns
::: {.column width="50%"}
```{r}
predict(model1, 
        newdata=newdata, 
        interval="confidence")
```
:::

::: {.column width="50%"}
### Interpretation

We are 95% confident that the **average** life expectancy for a country
with a 60% female literacy rate will be between 62.9 and 66.8 years.
:::
:::

## Confidence bands for mean response $\mu_{Y|x^*}$

-   Often we plot the CI for many values of X, creating **confidence
    bands**
-   The confidence bands are what ggplot creates when we set `se = TRUE`
    within `geom_smooth`
-   For what values of x are the confidence bands (intervals) narrowest?

```{r}
ggplot(gapm,
       aes(x=female_literacy_rate_2011, 
           y=life_expectancy_years_2011)) +
  geom_point()+
  geom_smooth(method = lm, se=TRUE)+
  ggtitle("Life expectancy vs. female literacy rate") 
```

## Width of confidence bands for mean response $\mu_{Y|x^*}$

-   For what values of $x^*$ are the confidence bands (intervals)
    narrowest? widest?

```{=tex}
\begin{align}
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}\\
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}
\end{align}
```
```{r}
#| echo: false
ggplot(gapm,
       aes(x=female_literacy_rate_2011, 
           y=life_expectancy_years_2011)) +
  geom_point()+
  geom_smooth(method = lm, se=TRUE)+
  ggtitle("Life expectancy vs. female literacy rate") +
  geom_vline(xintercept = mx, color = "purple")
```

## Prediction interval for predicting **individual** observations

-   We do not call this interval a CI since $Y$ is a random variable
    instead of a parameter
-   The form is similar to a CI though:

$$\widehat{Y|x^*} \pm t_{n-2}^* \cdot s_{residuals} \sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}$$

-   Note that the only difference to the CI for a mean value of y is the
    additional `1+` under the square root.
    -   Thus the width is wider!

## Example: Prediction interval

**Find the 95% prediction interval for the life expectancy when the
female literacy rate is 60.**

```{=tex}
\begin{align}
\widehat{Y|x^*} &\pm t_{n-2}^* \cdot s_{residuals} \sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}\\
64.8596 &\pm 1.990847 \cdot 6.142157 \sqrt{1+\frac{1}{80} + \frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\
(52.48072 &, 77.23849)
\end{align}
```
::: {style="font-size: 90%;"}
::: columns
::: {.column width="50%"}
```{r}
(Y60 <- 50.9278981 + 0.2321951 * 60)
(tstar <- qt(.975, df = 78))
(s_resid <- glance(model1)$sigma)
```
:::

::: {.column width="50%"}
```{r}
(n <- nobs(model1))
(mx <- mean(gapm$female_literacy_rate_2011))
(s_x <- sd(gapm$female_literacy_rate_2011))
```
:::
:::

```{r}
(SE_Ypred <- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))
```

::: columns
::: {.column width="36%"}
```{r}
(MOE_Ypred <- SE_Ypred*tstar)
```
:::

::: {.column width="1%"}
:::

::: {.column width="30%"}
```{r}
Y60 - MOE_Ypred
```
:::

::: {.column width="1%"}
:::

::: {.column width="30%"}
```{r}
Y60 + MOE_Ypred
```
:::
:::
:::

## Example: Using R for prediction interval

**Find the 95% prediction intervals for the life expectancy when the
female literacy rate is 40, 60, and 80.**

```{r}
newdata  # previously defined for CI's

predict(model1, 
        newdata=newdata, 
        interval="prediction")  # prediction instead of "confidence"
```

<br>

### Interpretation

We are 95% confident that a new selected country with a 60% female
literacy rate will have a life expectancy between 52.5 and 77.2 years.

## Prediction bands vs. confidence bands (1/2)

Create a scatterplot with the regression line, 95% confidence bands, and
95% prediction bands.

-   First create a data frame with the original data points (both x and
    y values), their respective predicted values, andtheir respective
    prediction intervals
-   Can do this with `augment()` from the `broom` package.

```{r}
model1_pred_bands <- augment(model1, interval = "prediction")

# take a look at new object:
names(model1_pred_bands) 

# glimpse of select variables of interest:
model1_pred_bands %>% 
  select(life_expectancy_years_2011, female_literacy_rate_2011, 
         .fitted:.upper) %>% 
  glimpse()
```

## Prediction bands vs. confidence bands (2/2)

```{r}
names(model1_pred_bands) 
```

```{r}
ggplot(model1_pred_bands, 
       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +
  geom_point() +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands
              alpha = 0.2, fill = "red") +
  geom_smooth(method=lm) +  # confidence bands
  labs(title = "SLR with Confidence & Prediction Bands") 

```

# Caution...

## Corrrelation doesn't imply causation`*`!

-   This might seem obvious, but make sure to not write your analysis
    results in a way that implies causation if the study design doesn't
    warrant it (such as an observational study).

-   Beware of spurious correlations:
    <http://www.tylervigen.com/spurious-correlations>

![](/img_slides/spurious_corr_2.png)

-   `*`Caveat: there is a whole field of statistics/epidemiology on
    causal inference. <https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf>

## What's next?

<br> <br>

![](/img_slides/flowchart_only_continuous.jpg){fig-align="center"}

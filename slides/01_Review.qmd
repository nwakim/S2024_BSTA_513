---
title: "Review"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "01/8/2023"
categories: ["Week 1"]
format: 
  revealjs:
    theme: [default, simple_NW.scss]
    toc: true
    toc-depth: 1
    toc-title: Class Overview
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: Review
    mermaid: 
      theme: neutral
---

## What did we learn in 511?

-   In 511, we talked about *categorical* and *continuous* outcomes (dependent variables)

     

-   We also talked about their relationship with 1-2 *continuous* or *categorical* exposure (independent variables or predictor)

     

-   We had many good ways to assess the relationship between an outcome and exposure:

     

|                      |                                                  |                                                                                                                  |
|-------------------|-------------------|----------------------------------|
|                      | Continuous Outcome                               | Categorical Outcome                                                                                              |
| Continuous Exposure  | Correlation, simple linear regression            | ??                                                                                                               |
| Categorical Exposure | t-tests, paired t-tests, 2 sample t-tests, ANOVA | proportion t-test, Chi-squared goodness of fit test, Fisher's Exact test, Chi-squared test of independence, etc. |

: {tbl-colwidths="\[15, 35,35\]"}

## What did we learn in 511?

-   You set up a really **important foundation**

    -   Including distributions, mathematical definitions, hypothesis testing, and more!

     

-   Tests and statistical approaches learned are incredibly helpful!

     

-   While you had to learn a lot of different tests and approaches for each combination of categorical/continuous exposure with categorical/continuous outcome

    -   **Those tests cannot handle more complicated data**

     

-   **What happens when other variables influence the relationship between your exposure and outcome?**

    -   Do we just ignore them?

## What will we learn in this class?

-   We will be building towards models that can handle many variables!

     

    -   **Regression** is the building block for modeling multivariable relationships

     

-   In Linear Models we will *build, interpret, and evaluate* linear regression models

## Process of regression data analysis

![](01_Review/Reg_analysis_process.png){fig-align="center"}

## Main sections of the course

1.  Review

2.  Intro to SLR: estimation and testing

    -   [Model fitting]{style="color:#34AC8B;"}

3.  Intro to MLR: estimation and testing

    -   [Model fitting]{style="color:#34AC8B;"}

4.  Diving into our predictors: categorical variables, interactions between variable

    -   [Model fitting]{style="color:#34AC8B;"}

5.  Key ingredients: model evaluation, diagnostics, selection, and building

    -   [Model evaluation]{style="color:#A7EA52;"} and [Model selection]{style="color:#FF8021;"}

```{r}
library(ggplot2)
```

```{css}
code.sourceCode {
  font-size: 1.7em;
  /* or try font-size: xx-large; */
}

div.cell-output-stdout {
  font-size: 1.5em;
}
```

## Main sections of the course

::: lob
1.  Review
:::

2.  Intro to SLR: estimation and testing

    -   [Model fitting]{style="color:#34AC8B;"}

3.  Intro to MLR: estimation and testing

    -   [Model fitting]{style="color:#34AC8B;"}

4.  Diving into our predictors: categorical variables, interactions between variable

    -   [Model fitting]{style="color:#34AC8B;"}

5.  Key ingredients: model evaluation, diagnostics, selection, and building

    -   [Model evaluation]{style="color:#A7EA52;"} and [Model selection]{style="color:#FF8021;"}

## Before we begin

-   Meike has some really good online notes, code, and work on [her BSTA 511 page](https://niederhausen.github.io/BSTA_511_F23/)

# Quick basics

## Some Basic Statistics "Talk"

::: columns
::: column
-   Random variable $Y$

    -   Sample $Y_i, i=1,\dots, n$

-   Summation:

    $\sum_{i=1}^n Y_i =Y_1 + Y_2 + \ldots + Y_n$

-   Product:

    $\prod_{i=1}^n Y_i = Y_1 \times Y_2 \times \ldots \times Y_n$
:::

::: column
:::
:::

## Descriptive Statistics: continuous variables

::: columns
::: {.column width="40%"}
**Measures of central tendency**

-   Sample mean

    $$
    \bar{x} = \dfrac{x_1+x_2+...+x_n}{n}=\dfrac{\sum_{i=1}^nx_i}{n}
    $$

-   Median
:::

::: column
**Measures of variability (or dispersion)**

-   Sample variance

    -   Average of the squared deviations from the sample mean

-   Sample standard deviation

    $$
    s = \sqrt{\dfrac{(x_1-\bar{x})^2+(x_2-\bar{x})^2+...+(x_n-\bar{x})^2}{n-1}}=\sqrt{\dfrac{\sum_{i=1}^n(x_i-\bar{x})^2}{n-1}}
    $$

-   IQR

    -   Range from 1st to 3rd quartile
:::
:::

## Descriptive Statistics: continuous variables (R code)

::: columns
::: {.column width="40%"}
**Measures of central tendency**

-   Sample mean

    ```{r}
    #| eval: false
    #| echo: true

    mean( sample )
    ```

-   Median

    ```{r}
    #| eval: false
    #| echo: true

    median( sample )
    ```
:::

::: column
**Measures of variability (or dispersion)**

-   Sample variance

    ```{r}
    #| eval: false
    #| echo: true

    var( sample )
    ```

-   Sample standard deviation

    ```{r}
    #| eval: false
    #| echo: true

    sd( sample )
    ```

-   IQR

    ```{r}
    #| eval: false
    #| echo: true

    IQR( sample )
    ```
:::
:::

## Data visualization

-   Using the library `ggplot2` to visualize data

-   We will load the package:

```{r}
#| echo: true

library(ggplot2)
```

## Histogram using `ggplot2`

We can make a basic graph for a continuous variable:

::: columns
```{r}
data("dds.discr")
```

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-align: center

ggplot(data = dds.discr, 
       aes(x = age)) +
  geom_histogram()
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-align: center

ggplot() +
  geom_histogram(data = dds.discr, 
       aes(x = age))
```
:::
:::

[Some more information](https://www.sharpsightlabs.com/blog/histogram-r-ggplot2/) on histograms using `ggplot2`

## Spruced up histogram using `ggplot2`

We can make a more formal, presentable graph:

```{r}
#| echo: true
#| fig-align: center

ggplot(data = dds.discr, 
       aes(x = age)) +
  geom_histogram() +
  theme(text = element_text(size=20)) +
  labs(x = "Age", 
       y = "Count", 
       title = "Distribution of Age in Sample")
```

I would like you to turn in homework, labs, and project reports with graphs like these.

## Other basic plots from `ggplot2`

We can also make a density and boxplot for the continuous variable with `ggplot2`

::: columns
::: {.column width="50%"}
```{r}
#| echo: true

ggplot(data = dds.discr, 
       aes(x = age)) +
  geom_density()
```
:::

::: {.column width="50%"}
```{r}
#| echo: true

ggplot(data = dds.discr, 
       aes(x = age)) +
  geom_boxplot()
```
:::
:::

# Important Distributions

## Distributions that will be used in this class

-   Normal distribution

-   Chi-square distribution

-   t distribution

-   F distribution

## Normal Distribution

-   Notation: $Y\sim N(\mu,\sigma^2)$

-   Arguably, the most important distribution in statistics

-   If we know $E(Y)=\mu$, $Var(Y)=\sigma^2$ then

    -   2/3 of $Y$'s distribution lies within 1 $\sigma$ of $\mu$

    -   95% $\ldots$ $\ldots$ is within $\mu\pm 2\sigma$

    -   $>99$% $\ldots$ $\ldots$ lies within $\mu\pm 3\sigma$

-   Linear combinations of Normal's are Normal\
    e.g., $(aY+b)\sim \mbox{N}(a\mu+b,\;a^2\sigma^2)$

-   Standard normal: $Z=\frac{Y-\mu}{\sigma} \sim \mbox{N}(0,1)$

## Chi-square Distribution

-   Notation: $X \sim \chi^2_{df}$

    -   $df=$ degrees of freedom

    -   $E[X]=df$

    -   $X$ takes on only positive values

-   If $Z_i\sim \mbox{N}(0,1)$, then $Z_i^2\sim \chi^2_1$

-   If $Z_1,\ldots,Z_n$ are independent, with $Z_i\sim\mbox{N}(0,1)$, then

```{=tex}
\begin{aligned}
        \sum_{i=1}^n Z_i^2 & \sim & \chi^2_n \nonumber
        
\end{aligned}
```
-   Used in hypothesis testing and CI's involving variance

## t Distribution

-   If $Z\sim \mbox{N}(0,1)$ and $S^2\sim \chi^2_{df}$ and $Z$ and $S^2$ are independent,

    ```{=tex}
    \begin{aligned}        \frac{Z}{S/\sqrt{df}} & \sim & t_{df} \nonumber            \end{aligned}
    ```
    -   Symmetric, bell-shaped; tails heavier than Normal

    -   $E[t_{df}]=0$; $Var(t_{df})$ greater than 1

    -   $\lim_{df\rightarrow \infty}t_{df} \rightarrow \mbox{N}(0,1)$

    -   for $df>30$, the $t_{df}$ closely resembles the $\mbox{N}(0,1)$ distribution

-   In linear modeling, used for inference on individual regression parameters

## F Distribution

-   Model ratio of sample variances

    -   Ratio of variances is important for hypothesis testing of regression and ANOVA models

-   If $X_1^2\sim \chi^2_{df1}$ and $X_2^2\sim \chi^2_{df2}$, where $X_1^2\perp X_2^2$, then:

    ```{=tex}
    \begin{aligned}        \frac{X_1^2/df1}{X_2^2/df2} & \sim & F_{df1,df2} \nonumber        \end{aligned}
    ```
    -   only takes on positive values

-   Important relationship with $t$ distribution:

    -   The square of a t-distribution with $df=\nu$

    -   is an F-distrubtion with numerator df ($df1 = 1$) and denominator df ($df2 = \nu$)

```{=tex}
\begin{aligned}
             T^2  \stackrel{{D}}{\sim} & F_{1,\nu} \\
             \text{ if and only if } T \sim t_{\nu}
            
\end{aligned}
```
# Statistical inference: Estimation

## Confidence interval for one mean

::: columns
::: column
The confidence interval for population mean $\mu$:

$$
\bar{x} \pm t^{*}\dfrac{s}{\sqrt{n}}
$$

-   where $t^*$ is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on $df=n-1$

::: proposition
::: prop-title
We can use `R` to find the critical t-value, $t^*$
:::

::: prop-cont
For example the critical value for the 95% CI with $n=10$ subjects is...

```{r}
#| echo: true

qt(0.975, df=9)
```

-   Recall, that as the $df$ increases, the t-distribution converges towards the Normal distribution
:::
:::
:::

::: column
:::
:::

## Confidence interval for one mean

::: columns
::: column
The confidence interval for population mean $\mu$:

$$
\bar{x} \pm t^{*}\dfrac{s}{\sqrt{n}}
$$

-   where $t^*$ is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on $df=n-1$

::: proposition
::: prop-title
We can use `R` to find the critical t-value, $t^*$
:::

::: prop-cont
For example the critical value for the 95% CI with $n=10$ subjects is...

```{r}
#| echo: true

qt(0.975, df=9)
```

-   Recall, that as the $df$ increases, the t-distribution converges towards the Normal distribution
:::
:::
:::

::: column
We can also use `t.test` in R to calculate the confidence interval if we have a dataset.

```{r}
#| echo: true 

t.test(dds.discr$age)
```
:::
:::

## Confidence interval for two independent means

::: columns
::: column
The confidence interval for difference in independent population means, $\mu_1$ and $\mu_2$:

$$
\bar{x}_1 - \bar{x}_2 \pm t^{*}\sqrt{\dfrac{s_1^2}{n_1} + \dfrac{s_2^2}{n_2}}
$$

-   where $t^*$ is the critical value for the 95% (or other percent) corresponding to the t-distribution and dependent on $df=n_1 + n_2 -2$
:::

::: column
:::
:::

## Here's a decent source for other R code for tests in 511

[Website from UCLA](https://stats.oarc.ucla.edu/r/whatstat/what-statistical-analysis-should-i-usestatistical-analyses-using-r/)

# Statistical inference: Hypothesis testing

## Steps in hypothesis testing

![](01_Review/hypothesis_test_steps.png){fig-align="center"}

## Example: one sample t-test using ***p-value approach***

We want to see what the mean population body temperature is.

2.  State the null and alternative hypotheses:

    |                      |                                                                       |
    |-------------------|-----------------------------------------------------|
    | $H_0: \mu = 98.6$    | $H_0$: The population mean body temperature is 98.6 degrees F         |
    | $H_A: \mu \neq 98.6$ | $H_A$: The population mean body temperature is **not** 98.6 degrees F |

3.  The significance level is $\alpha = 0.05$

4.  The test statistic, $t_{\bar{x}}$ follows a student's t-distribution with $df = n-1 = 129$

5.  The test statistic is: $t_{\bar{x}} = \dfrac{\bar{x}-\mu_0}{\dfrac{s}{\sqrt{n}}}$ and with the data: $t_{\bar{x}} = \dfrac{98.25-98.6}{\dfrac{0.73}{\sqrt{130}}} = -5.45$

6.  Calculate the p-value: $p-value = P(t \leq -5.45) + P(t \geq 5.45)$

    ```{r}
    #| echo: true

    2*pt(-5.4548, df = 130-1, lower.tail=T)
    ```

7.  Conclusion: We reject the null hypothesis. There is sufficient evidence that the (population) mean body temperature after is different from 98.6 degree ( $p-value < 0.001$).

## Example: one sample t-test using *critical values approach*

We want to see what the mean population body temperature is.

2.  State the null and alternative hypotheses:

    |                      |                                                                       |
    |-------------------|-----------------------------------------------------|
    | $H_0: \mu = 98.6$    | $H_0$: The population mean body temperature is 98.6 degrees F         |
    | $H_A: \mu \neq 98.6$ | $H_A$: The population mean body temperature is **not** 98.6 degrees F |

3.  The significance level is $\alpha = 0.05$

4.  The test statistic, $t_{\bar{x}}$ follows a student's t-distribution with $df = n-1 = 129$

5.  Decision rule (critical value): For $\alpha=0.05$ , $2*P(t \geq t^*) = 0.05$

    ```{r}
    #| echo: true

    qt(0.05/2, df = 130-1, lower.tail=F)
    ```

6.  The test statistic is: $t_{\bar{x}} = \dfrac{\bar{x}-\mu_0}{\dfrac{s}{\sqrt{n}}}$ and with the data: $t_{\bar{x}} = \dfrac{98.25-98.6}{\dfrac{0.73}{\sqrt{130}}} = -5.45$

7.  Conclusion: We reject the null hypothesis. There is sufficient evidence that the (population) mean body temperature after is different from 98.6 degree ( 95% CI: $98.12, 98.38$).

## How did we get the 95% CI?

-   The `t.test` function can help us answer this, and give us the needed information for both approaches.

```{r}
#| echo: true

BodyTemps = read.csv("data/BodyTemperatures.csv")

t.test(x = BodyTemps$Temperature, 
       # alternative = "two-sided", 
       mu = 98.6)
```

# Error Rates and Power

## Type 1 and 2 errors

![](01_Review/Type_1_2_error.png){fig-align="center"}

## Power

-   Power is $1-\beta$

    -   The probability of correctly rejecting the null hypothesis

![](01_Review/Power.png){fig-align="center"}

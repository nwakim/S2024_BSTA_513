---
title: "Simple Linear Regression (SLR)"
author: "Nicky Wakim"
title-slide-attributes:
    data-background-color: "#213c96"
date: "01/17/2023"
categories: ["Week 1"]
format: 
  revealjs:
    theme: [default, simple_NW.scss]
    toc: true
    toc-depth: 1
    toc-title: Class Overview
    chalkboard: true
    slide-number: true
    show-slide-number: all
    width: 1955
    height: 1100
    footer: SLR 1
    html-math-method: mathjax
    highlight-style: ayu
execute:
  echo: true
  freeze: auto  # re-render only when source changes
editor: 
  markdown: 
    wrap: 72
---

```{r}
#| label: "setup" 
#| include: false
#| message: false
#| warning: false

library(tidyverse)    
library(openintro)
library(janitor)
library(rstatix)
library(knitr)
library(gtsummary)
library(moderndive)
library(gt)
library(broom) 
library(here) 
library(pwr) 
library(gridExtra) # NEW!!!

# terminal: for icons
# quarto install extension quarto-ext/fontawesome

# set ggplot theme for slides 
theme_set(theme_gray(base_size = 22))
# theme_update(text = element_text(size=16))  # set global text size for ggplots

```

# Learning Objectives

1.  Identify the aims of your research and see how they align with the
    intended purpose of simple linear regression

2.  Identify the simple linear regression model and define statistics
    language for key notation

3.  Illustrate how ordinary least squares (OLS) finds the best model
    parameter estimates

4.  Solve the optimal coefficient estimates for simple linear regression
    using OLS

5.  Apply OLS in R for simple linear regression of real data

## Topics

Topics for SLR:

assumptions

parameter estimation and interpretation

properties of LSE

estimation of variance?

Topics

-   Inference for slope and intercept

    -   CI’s and hypothesis tests

-   Inference for mean value of y at specific values of x

    -   Confidence bands of best-fit line

-   Prediction intervals for individual predictions

    -   Prediction bands

-   *F-test for the slope*

-   *Pearson correlation coefficient r*

-   *Coefficient of determination R2*

-   *Testing the correlation coefficient !*

-   *CI for the correlation coefficient !*

## Let's start with an example

::: columns
::: {.column width="55%"}
```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false

gapm <- read_csv("data/lifeexp_femlit_2011.csv")
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))

```
:::

::: {.column width="45%"}
Average life expectancy vs. female literacy rate $\beta$

-   Each point on the plot is for a different country

-   $X$ = country's adult female literacy rate

-   $Y$ = country's average life expectancy (years)

-   Data are from Gapminder (2011)
:::
:::

::: heq
$$\widehat{\text{life expectancy}} =  50.9 + 0.232\cdot\text{female literacy rate}$$
:::

## Reference: How did I code that?

![](pause.png){.absolute top="100%" right="0%" width="120" height="120"}

```{r}
#| fig-height: 6.5
#| fig-width: 10
#| fig-align: center

ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))

```

# Learning Objectives

::: lob
1.  Identify the aims of your research and see how they align with the
    intended purpose of simple linear regression
:::

2.  Identify the simple linear regression model and define statistics
    language for key notation

3.  Illustrate how ordinary least squares (OLS) finds the best model
    parameter estimates

4.  Solve the optimal coefficient estimates for simple linear regression
    using OLS

5.  Apply OLS in R for simple linear regression of real data

## Questions we can ask with this model

::: columns
::: {.column width="55%"}
```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false

gapm <- read_csv("data/lifeexp_femlit_2011.csv")
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(
        size = 30), 
        axis.text = element_text(
        size = 25), 
        title = element_text(
        size = 30))

```
:::

::: {.column width="45%"}
-   How do we...
    -   calculate slope & intercept?
    -   interpret slope & intercept?
    -   do inference for slope & intercept?
        -   CI, p-value
    -   do prediction with regression line?
        -   CI for prediction?
-   Does the model fit the data well?
    -   Should we be using a line to model the data?
-   Should we add additional variables to the model?
    -   multiple/multivariable regression
:::
:::

::: heq
$$\widehat{\text{life expectancy}} =  50.9 + 0.232\cdot\text{female literacy rate}$$
:::

## Association vs. prediction

::: columns
::: {.column width="50%"}
::: proof1
::: proof-title
Association
:::

::: proof-cont
-   What is the association between countries’ life expectancy and
    female literacy rate?
-   Use the slope of the line or correlation coefficient
:::
:::
:::

::: {.column width="50%"}
::: definition
::: def-title
Prediction
:::

::: def-cont
-   What is the expected average life expectancy for a country with a
    specified female literacy rate?    
:::
:::
:::
:::

$$\widehat{\text{life expectancy}} =  50.9 + 0.232\cdot\text{female literacy rate}$$

```{r}
#| fig-height: 7
#| fig-width: 12
#| echo: false
#| fig-align: center

gapm <- read_csv("data/lifeexp_femlit_2011.csv")
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(
        size = 30), 
        axis.text = element_text(
        size = 25), 
        title = element_text(
        size = 30))

```

## Dataset description

-   Data file: `lifeexp_femlit_water_2011.csv`

-   Data were downloaded from <https://www.gapminder.org/data/>

-   2011 is the most recent year with the most complete data

-   **Life expectancy** = the average number of years a newborn child
    would live if current mortality patterns were to stay the same.
    Source: <https://www.gapminder.org/data/documentation/gd004/>

-   **Adult literacy rate** is the percentage of people ages 15 and
    above who can, with understanding, read and write a short, simple
    statement on their everyday life. Source:
    <http://data.uis.unesco.org/>

-   **At least basic water source (%)** = the percentage of people using
    at least basic water services. This indicator encompasses both
    people using basic water services as well as those using safely
    managed water services. Basic drinking water services is defined as
    drinking water from an improved source, provided collection time is
    not more than 30 minutes for a round trip. Improved water sources
    include piped water, boreholes or tubewells, protect dug wells,
    protected springs, and packaged or delivered water.

## Get to know the data

-   Load data

```{r}
gapm_original <- read_csv(here::here("data", "lifeexp_femlit_water_2011.csv"))
```

-   Glimpse of the data

```{r}
glimpse(gapm_original)
```

-   Note the missing values for our variables of interest

```{r}
gapm_original %>% select(life_expectancy_years_2011, female_literacy_rate_2011) %>% 
  get_summary_stats()
```

## Remove missing values

-   Remove rows with missing data for life expectancy and female
    literacy rate

```{r}
gapm <- gapm_original %>% 
  drop_na(life_expectancy_years_2011, female_literacy_rate_2011)

glimpse(gapm)

```

-   No missing values now for our variables of interest

```{r}
gapm %>% select(life_expectancy_years_2011, female_literacy_rate_2011) %>% 
  get_summary_stats()
```

::: callout-important
-   Removing the rows with missing data was not needed to run the
    regression model.
-   I did this step since later we will be calculating the standard
    deviations of the explanatory and response variables for *just the
    values included in the regression model*. It'll be easier to do this
    if we remove the missing values now.
:::

## Three types of study design (there are more)

::: columns
::: {.column width="33%"}
::: definition
::: def-title
Experiment
:::

::: def-cont
-   Observational units are randomly assigned to important predictor
    levels

    -   Random assignment controls for confounding variables (age,
        gender, race, etc.)

    -   “gold standard” for determining causality

    -   Observational unit is often at the participant-level
:::
:::
:::

::: {.column width="33%"}
::: example
::: ex-title
Quasi-experiment
:::

::: ex-cont
-   Participants are assigned to intervention levels without
    randomization

-   Not common study design
:::
:::
:::

::: {.column width="33%"}
::: proposition
::: prop-title
Observational
:::

::: prop-cont
-   No randomization or assignment of intervention conditions

-   In general cannot infer causality

    -   However, there are casual inference methods…
:::
:::
:::
:::

## Let's revisit the regression analysis process

   

![](images/arrow2.png){.absolute top="17%" right="63.15%" width="155"}
![](images/arrow2.png){.absolute top="17%" right="28.7%"
width="155"}![](images/arrow_back4.png){.absolute top="10%"
right="30.7%" width="840"}

::: columns
::: {.column width="30%"}
::: RAP1
::: RAP1-title
Model Selection
:::

::: RAP1-cont
-   Building a model

-   Prediction vs interpretation

-   Comparing models
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP2
::: RAP2-title
Model Fitting
:::

::: RAP2-cont
-   Parameter estimation

-   Interpret model parameters

-   Hypothesis tests for coefficients

-   Categorical covariates

-   Interactions
:::
:::
:::

::: {.column width="4%"}
:::

::: {.column width="30%"}
::: RAP3
::: RAP3-title
Model Evaluation
:::

::: RAP3-cont
-   Evaluation of full and reduced models
-   Testing model assumptions
-   Residuals
-   Transformations
-   Influential points
-   Multicollinearity
:::
:::
:::
:::

# Learning Objectives

1.  Identify the aims of your research and see how they align with the
    intended purpose of simple linear regression

::: lob
2.  Identify the simple linear regression model and define statistics
    language for key notation
:::

3.  Illustrate how ordinary least squares (OLS) finds the best model
    parameter estimates

4.  Solve the optimal coefficient estimates for simple linear regression
    using OLS

5.  Apply OLS in R for simple linear regression of real data

## Simple Linear Regression Model

The (population) regression model is denoted by:

 

::: heq
$$Y =  \beta_0 + \beta_1X + \epsilon$$
:::

 

::: columns
::: column
#### Observable sample data

-   $Y$ is our dependent variable

    -   Aka outcome or response variable

-   $X$ is our independent variable

    -   Aka predictor, regressor, exposure variable
:::

::: column
#### Unobservable population parameters

-   $\beta_0$ and $\beta_1$ are **unknown** population parameters

-   $\epsilon$ (epsilon) is the error about the line

    -   It is assumed to be a random variable with a...

        -   Normal distribution with mean 0 and constant variance
            $\sigma^2$

        -   i.e. $\epsilon \sim N(0, \sigma^2)$
:::
:::

## Simple Linear Regression Model (another way to view components)

The (population) regression model is denoted by:

 

::: heq
$$Y =  \beta_0 + \beta_1X + \epsilon$$
:::

 

### Components

|            |                                            |
|------------|--------------------------------------------|
| $Y$        | response, outcome, dependent variable      |
| $\beta_0$  | intercept                                  |
| $\beta_1$  | slope                                      |
| $X$        | predictor, covariate, independent variable |
| $\epsilon$ | residuals, error term                      |

## If the population parameters are unobservable, how did we get the line for life expectancy?

```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false
#| fig-align: center

gapm <- read_csv("data/lifeexp_femlit_2011.csv")
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))

```

::: hl
Note: the **population model is the true, underlying model** that we are
trying to estimate using our sample data

-   Our goal in simple linear regression is to estimate $\beta_0$ and
    $\beta_1$
:::

## Okay, so how do we estimate the regression line?

 

At this point, we are going to move over to an R shiny app that I made.

 

Let's see if we can eyeball the best-fit line!

## Regression line = best-fit line

::: columns
::: {.column width="50%"}
$$\widehat{Y} = \widehat{\beta}_0 + \widehat{\beta}_1 X $$

-   $\widehat{Y}$ is the predicted outcome for a specific value of $X$
-   $\widehat{\beta}_0$ is the intercept *of the best-fit line*
-   $\widehat{\beta}_1$ is the slope *of the best-fit line*, i.e., the
    increase in $\widehat{Y}$ for every increase of one (unit increase)
    in $X$
    -   slope = *rise over run*
:::

::: {.column width="50%"}
```{r}
#| fig-height: 8
#| fig-width: 11
#| echo: false

gapm <- read_csv("data/lifeexp_femlit_2011.csv")
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point(size = 4) +
  geom_smooth(method = "lm", se = FALSE, size = 3, colour="#F14124") +
  labs(x = "Female literacy rate (%)", 
       y = "Life expectancy (years)",
       title = "Relationship between life expectancy and \n the female literacy rate in 2011") +
    theme(axis.title = element_text(size = 30), 
        axis.text = element_text(size = 25), 
        title = element_text(size = 30))
```
:::
:::

## Simple Linear Regression Model

::: columns
::: {.column width="50%"}
#### Population regression line

::: lob
$$Y =  \beta_0 + \beta_1X + \epsilon$$
:::

 

##### Components

|            |                                            |
|------------|--------------------------------------------|
| $Y$        | response, outcome, dependent variable      |
| $\beta_0$  | intercept                                  |
| $\beta_1$  | slope                                      |
| $X$        | predictor, covariate, independent variable |
| $\epsilon$ | residuals, error term                      |
:::

::: {.column width="50%"}
#### Estimated regression line

::: hl
$$\widehat{Y} =  \widehat{\beta}_0 + \widehat{\beta}_1X$$
:::

 

##### Components

|                     |                                                   |
|---------------------|---------------------------------------------------|
| $\widehat{Y}$       | *estimated expected* response given predictor $X$ |
| $\widehat{\beta}_0$ | *estimated* intercept                             |
| $\widehat{\beta}_1$ | *estimated* slope                                 |
| $X$                 | predictor, covariate, independent variable        |
:::
:::

## We get it, Nicky! How do we estimate the regression line?

# Learning Objectives

1.  Identify the aims of your research and see how they align with the
    intended purpose of simple linear regression

2.  Identify the simple linear regression model and define statistics
    language for key notation

::: lob
3.  Illustrate how ordinary least squares (OLS) finds the best model
    parameter estimates
:::

4.  Solve the optimal coefficient estimates for simple linear regression
    using OLS

5.  Apply OLS in R for simple linear regression of real data

## It all starts with a residual...

::: columns
::: {.column width="50%"}
-   Recall, one characteristic of our population model was that the
    residuals, $\epsilon$, were Normally distributed:
    $\epsilon \sim N(0, \sigma^2)$

-   In our population regression model, we had:
    $$Y =  \beta_0 + \beta_1X + \epsilon$$

-   We can also take the average (expected) value of the population
    model

-   We take the expected value of both sides and get:

$$\begin{aligned} 
        E[Y] & = E[\beta_0 + \beta_1X + \epsilon] \\
        E[Y] & = E[\beta_0] + E[\beta_1X] + E[\epsilon] \\
        E[Y] & = \beta_0 + \beta_1X + E[\epsilon] \\
        E[Y|X] & = \beta_0 + \beta_1X \\
\end{aligned}$$

-   We call $E[Y|X]$ the expected value of $Y$ given $X$
:::

::: {.column width="50%"}
![](03_SLR_01/OLSassumptions-1.png){fig-align="center"}
:::
:::

## So now we have two representations of our population model

::: columns
::: {.column width="50%"}

::: definition
::: def-title
With observed values:
:::

::: def-cont
$$Y =  \beta_0 + \beta_1X + \epsilon$$
:::
:::
:::

::: {.column width="50%"}

::: fact
::: fact-title
With the population expected value of $Y$ given $X$:
:::

::: fact-cont
$$E[Y|X] = \beta_0 + \beta_1X$$
:::
:::
:::
:::

Using the two forms of the model, we can figure out a formula for our
residuals:

$$\begin{aligned}
Y & = (\beta_0 + \beta_1X) + \epsilon \\
Y & = E[Y|X] + \epsilon \\
Y - E[Y|X] & = \epsilon \\ 
\epsilon & = Y - E[Y|X]
\end{aligned}$$

And so we have our true, population model, residuals!

::: hl
This is an important fact! For the population model, the residuals: $\epsilon  = Y - E[Y|X]$
:::

## Back to our estimated model

Get to: $\widehat{\epsilon}  = Y - \widehat{E}[Y|X] = Y - \widehat{Y}$


## Translate eyeballing to mathematical form??

![](images/Screenshot%202024-01-13%20at%2011.41.54%20AM.png)

## Coefficient estimates

# Learning Objectives

1.  Identify the aims of your research and see how they align with the
    intended purpose of simple linear regression

2.  Identify the simple linear regression model and define statistics
    language for key notation

3.  Illustrate how ordinary least squares (OLS) finds the best model
    parameter estimates

::: lob
4.  Solve the optimal coefficient estimates for simple linear regression
    using OLS
:::

5.  Apply OLS in R for simple linear regression of real data

## Regression in R: `lm()`, `summary()`, & `tidy()`

```{r}
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
summary(model1)

tidy(model1) %>% gt()
```

Regression equation for our model:

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot \textrm{female literacy rate} $$

## Residuals

::: columns
::: {.column width="40%"}
-   **Observed values** $y_i$
    -   the values in the dataset
-   **Fitted values** $\widehat{y}_i$
    -   the values that fall on the best-fit line for a specific $x_i$
-   **Residuals** $e_i = y_i - \widehat{y}_i$
    -   the differences between the observed and fitted values
:::

::: {.column width="60%"}
```{r}
#| echo: false
#| fig.height: 7.0
#| fig.width: 8.0
# code from https://drsimonj.svbtle.com/visualising-residuals

model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
regression_points <- augment(model1)
summary(model1)
sum(model1$residuals^2)

ggplot(regression_points, 
       aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_smooth(method = "lm", se = FALSE, color = "lightgrey") +
  geom_segment(aes(
    xend = female_literacy_rate_2011, 
    yend = .fitted), 
    alpha = .2) +
  # > Color adjustments made here...
  geom_point(aes(color = .resid), size = 4) +  # Color mapped here
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +  # Colors to use here
  guides(color = "none") +
  geom_point(aes(y = .fitted), shape = 1, size = 3) +
  labs(x = "Female literacy rate", 
       y = "Average life expectancy",
       title = "Regression line with residuals") +
  theme_bw() + 
  theme(text = element_text(size = 20))    
```
:::
:::

## The (population) regression model

::: columns
::: {.column width="50%"}
-   The (population) regression model is denoted by

$$Y = \beta_0 + \beta_1 \cdot X + \epsilon$$

-   $\beta_0$ and $\beta_1$ are unknown population parameters
-   $\epsilon$ (epsilon) is the error about the line
    -   It is assumed to be a random variable:
        -   $\epsilon \sim N(0, \sigma^2)$
        -   variance $\sigma^2$ is constant
:::

::: {.column width="50%"}
![](/img_slides/OLSassumptions-1.png){fig-align="center"}

::: {style="font-size: 60%;"}
<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>
:::
:::
:::

-   The [**line**]{style="color:blue"} is the average (expected) value
    of $Y$ given a value of $x$: [$E(Y|x)$]{style="color:blue"}.

-   The point estimates for $\beta_0$ and $\beta_1$ based on a sample
    are denoted by $b_0, b_1, s_{residuals}^2$

    -   Note: also common notation is
        $\widehat{\beta}_0, \widehat{\beta}_1, \widehat{\sigma}^2$

## How do we interpret the coefficients?

-   **Intercept**
    -   The expected outcome for the $y$-variable when the $x$-variable
        is 0
-   **Slope**
    -   For every increase of 1 unit in the $x$-variable, there is an
        expected increase of, on average, $b_1$ units in the
        $y$-variable.
    -   We only say that there is an expected increase and not
        necessarily a causal increase.

# Next class???

## What are the LINE conditions?

For "good" model fit and to be able to make inferences and predictions
based on our models, 4 conditions need to be satisfied.

Briefly:

-   **L** inearity of relationship between variables
-   **I** ndependence of the Y values
-   **N** ormality of the residuals
-   **E** quality of variance of the residuals (homoscedasticity)

[More in
depth](https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions):

-   **L** : there is a linear relationship between the mean response (Y)
    and the explanatory variable (X),
-   **I** : the errors are independent—there’s no connection between how
    far any two points lie from the regression line,
-   **N** : the responses are normally distributed at each level of X,
    and
-   **E** : the variance or, equivalently, the standard deviation of the
    responses is equal for all levels of X.

## L: Linearity of relationship between variables

Is the association between the variables linear?

-   Diagnostic tools:
    -   Scatterplot
    -   Residual plot (see later section for E : Equality of variance of
        the residuals)

```{r}
#| echo: false
#| fig.width: 12.0
#| fig.height: 7.0
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate in 2011") +  
  geom_smooth(method = "lm", se = FALSE) +
  geom_smooth(se = FALSE, color = "black")
```

## I: Independence of the residuals ($Y$ values)

-   **Are the data points independent of each other?**

-   Examples of when they are *not* independent, include

    -   repeated measures (such as baseline, 3 months, 6 months)
    -   data from clusters, such as different hospitals or families

-   This condition is checked by reviewing the study *design* and not by
    inspecting the data

-   How to analyze data using regression models when the $Y$-values are
    not independent is covered in BSTA 519 (Longitudinal data)

# N: Normality of the residuals

-   Extract residuals from regression model in R
-   Diagnostic tools:
    -   Distribution plots of residuals
    -   QQ plots

## N: Normality of the residuals

-   The responses Y are normally distributed at each level of x

![](/img_slides/OLSassumptions-1.png){fig-align="center"}

::: {style="font-size: 60%;"}
<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>
:::

## Extract model's residuals in R

-   First extract the residuals' values from the model output using the
    `augment()` function from the `broom` package.
-   Get a tibble with the orginal data, as well as the residuals and
    some other important values.

```{r}
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011, 
                data = gapm)
aug1 <- augment(model1) 

glimpse(aug1)
```

## Check normality with "usual" distribution plots

Note that below I save each figure, and then combine them together in
one row of output using `grid.arrange()` from the `gridExtra` package.

```{r}
#| fig.height: 4.0
#| fig.width: 12.0
hist1 <- ggplot(aug1, aes(x = .resid)) +
  geom_histogram()

density1 <- ggplot(aug1, aes(x = .resid)) +
  geom_density()

box1 <- ggplot(aug1, aes(x = .resid)) +
  geom_boxplot()

library(gridExtra) # NEW!!!
grid.arrange(hist1, density1, box1, nrow = 1)
```

## Normal QQ plots (QQ = quantile-quantile)

-   It can be tricky to eyeball with a histogram or density plot whether
    the residuals are normal or not
-   QQ plots are often used to help with this

::: columns
::: {.column width="60%"}
-   *Vertical axis*: **data quantiles**
    -   data points are sorted in order and
    -   assigned quantiles based on how many data points there are
-   *Horizontal axis*: **theoretical quantiles**
    -   mean and standard deviation (SD) calculated from the data points
    -   theoretical quantiles are calculated for each point, assuming
        the data are modeled by a normal distribution with the mean and
        SD of the data
:::

::: {.column width="40%"}
```{r}
#| fig.width: 5.0
#| fig.height: 5.0
#| echo: false
ggplot(aug1, aes(sample = .resid)) + 
  stat_qq() +     # points
  stat_qq_line()  # line
```
:::
:::

-   **Data are approximately normal if points fall on a line.**

See more info at
<https://data.library.virginia.edu/understanding-QQ-plots/>

## Examples of Normal QQ plots (1/5)

-   **Data**:
    -   Body measurements from 507 physically active individuals
    -   in their 20's or early 30's
    -   within normal weight range.

![](/img_slides/qq_wristdiam.png){fig-align="center"}

## Examples of Normal QQ plots (2/5)

Skewed right distribution

<br>

![](/img_slides/qq_weights.png){fig-align="center"}

## Examples of Normal QQ plots (3/5)

Long tails in distribution

<br>

![](/img_slides/qq_biliac.png){fig-align="center"}

## Examples of Normal QQ plots (4/5)

Bimodal distribution

<br>

![](/img_slides/qq_forearm.png){fig-align="center"}

## Examples of Normal QQ plots (5/5)

![](/img_slides/qq_forearm_gender.png){fig-align="center"}

## QQ plot of residuals of `model1`

```{r}
#| fig.width: 12.0
#| fig.height: 3.0
#| echo: false
grid.arrange(hist1, density1, box1, nrow = 1)
```

::: columns
::: {.column width="50%"}
<br>

```{r}
#| label: QQresid
#| fig.show: hide
#| fig.width: 5.0
#| fig.height: 5.0

ggplot(aug1, aes(sample = .resid)) + 
  stat_qq() +     # points
  stat_qq_line()  # line
```
:::

::: {.column width="50%"}
```{r}
#| ref.label: QQresid
#| echo: false
#| fig.width: 5.0
#| fig.height: 4.0
```
:::
:::

## Compare to randomly generated Normal QQ plots

How "*good*" we can expect a QQ plot to look depends on the sample size.

-   The QQ plots on the next slides are randomly generated

    -   using random samples from actual standard normal distributions
        $N(0,1)$.

-   Thus, all the points in the QQ plots **should theoretically** fall
    in a line

-   However, there is sampling variability...

## Randomly generated Normal QQ plots: n=100

-   Note that `stat_qq_line()` doesn't work with randomly generated
    samples, and thus the code below manually creates the line that the
    points should be on (which is $y=x$ in this case.)

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 100

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}
```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::

## Examples of simulated Normal QQ plots: n=10

With fewer data points,

-   simulated QQ plots are more likely to look "less normal"
-   even though the data points were sampled from normal distributions.

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 10  # only change made to code!

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}
```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::

## Examples of simulated Normal QQ plots: n=1,000

With more data points,

-   simulated QQ plots are more likely to look "more normal"

::: columns
::: {.column width="50%"}
::: {style="font-size: 90%;"}
```{r}
#| fig.height: 5.0
#| fig.width: 5.0
samplesize <- 1000 # only change made to code!

rand_qq1 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  # line y=x
  geom_abline(intercept = 0, slope = 1, 
              color = "blue") 

rand_qq2 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq3 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")

rand_qq4 <- ggplot() +
  stat_qq(aes(sample = rnorm(samplesize))) + 
  geom_abline(intercept = 0, slope = 1, 
              color = "blue")
```
:::
:::

::: {.column width="50%"}
```{r}
#| fig.height: 8.0
#| fig.width: 8.0

grid.arrange(rand_qq1, rand_qq2, 
             rand_qq3, rand_qq4, ncol =2)
```
:::
:::

## Back to our example

::: columns
::: {.column width="40%"}
Residuals from Life Expectancy vs. Female Literacy Rate Regression

```{r}
#| label: QQplot
#| fig.show: hide
ggplot(aug1, 
      aes(sample = .resid)) + 
  stat_qq() + 
  stat_qq_line() 
```
:::

::: {.column width="60%"}
```{r}
#| ref.label: QQplot
#| echo: false
```
:::
:::

Simulated QQ plot of Normal Residuals with n = 80

::: columns
::: {.column width="40%"}
```{r}
# number of observations 
# in fitted model
nobs(model1) 
```

```{r}
#| label: QQplot_sim
#| fig.show: hide
ggplot() +
  stat_qq(aes(
    sample = rnorm(80))) + 
  geom_abline(
    intercept = 0, slope = 1, 
    color = "blue")
```
:::

::: {.column width="60%"}
```{r}
#| ref.label: QQplot_sim
#| echo: false
```
:::
:::

# E: Equality of variance of the residuals {.nostretch}

-   Homoscedasticity
-   Diagnostic tool: **residual plot**

![](/img_slides/OLSassumptions-1.png){fig-align="center" width="60%"}

::: {style="font-size: 60%;"}
<https://bookdown.org/roback/bookdown-bysh/ch-MLRreview.html#ordinary-least-squares-ols-assumptions>
:::

## Residual plot

-   $x$ = explanatory variable from regression model
    -   (or the fitted values for a multiple regression)
-   $y$ = residuals from regression model

::: columns
::: {.column width="50%"}
```{r}
names(aug1)
```

<br>

```{r}
#| label: resids_plot
#| fig.show: hide
ggplot(aug1, 
       aes(x = female_literacy_rate_2011, 
           y = .resid)) + 
  geom_point() +
  geom_abline(
    intercept = 0, 
    slope = 0, 
    color = "orange") +
  labs(title = "Residual plot")
```
:::

::: {.column width="50%"}
```{r}
#| ref.label: resids_plot
#| echo: false
#| fig.height: 6
#| fig.width: 6
```
:::
:::

## E: Equality of variance of the residuals (Homoscedasticity)

-   The **variance** or, equivalently, the standard deviation of the
    responses is **equal for all values of x**.
-   This is called **homoskedasticity** (top row)
-   If there is **heteroskedasticity** (bottom row), then the assumption
    is not met.

![](/img_slides/heteroskedastic.png){fig-align="center"}

# $R^2$ = Coefficient of determination

Another way to assess model fit

## $R^2$ = Coefficient of determination (1/2)

-   Recall that the correlation coefficient $r$ measures the strength of
    the linear relationship between two numerical variables
-   $R^2$ is usually used to measure the strength of a *linear fit*
    -   For a simple linear regression model (one numerical predictor),
        $R^2$ is just the square of the correlation coefficient
-   In general, $R^2$ is the proportion of the variability of the
    dependent variable that is **explained** by the independent
    variable(s)

$$R^2 = \frac{\textrm{variance of predicted y-values}}
{\textrm{variance of observed y-values}} = \frac{\sum_{i=1}^n(\widehat{y}_i-\bar{y})^2}
{\sum_{i=1}^n(y_i-\bar{y})^2} 
 = \frac{s_y^2 - s_{\textrm{residuals}}^2}
{s_y^2}$$ $$R^2 = 1- \frac{s_{\textrm{residuals}}^2}
{s_y^2}$$ where $\frac{s_{\textrm{residuals}}^2}{s_y^2}$ is the
proportion of "unexplained" variability in the $y$ values,\
and thus $R^2 = 1- \frac{s_{\textrm{residuls}}^2}{s_y^2}$ is the
proportion of "explained" variability in the $y$ values

## $R^2$ = Coefficient of determination (2/2)

-   Recall, $-1<r<1$

-   Thus, $0<R^2<1$

-   In practice, we want "high" $R^2$ values, i.e. $R^2$ as close to 1
    as possible.

Calculating $R^2$ in R using `glance()` from the `broom` package:

```{r}
glance(model1)
glance(model1)$r.squared
```

::: callout-warning
-   A model can have a high $R^2$ value when there is a curved pattern.
-   Always first check whether a linear model is reasonable or not.
:::

## $R^2$ in `summary()` R output

```{r}
summary(model1)
```

Compare to the square of the correlation coefficient $r$:

```{r}
r <- cor(x = gapm$life_expectancy_years_2011, 
    y = gapm$female_literacy_rate_2011,
    use =  "complete.obs")
r
r^2
```

# Regression inference

1.  Inference for population [**slope**]{style="color:green"} $\beta_1$
2.  CI for mean response $\mu_{Y|x^*}$
3.  Prediction interval for predicting **individual** observations

## Inference for population [**slope**]{style="color:green"} $\beta_1$

```{r}
# Fit regression model:
model1 <- lm(life_expectancy_years_2011 ~ female_literacy_rate_2011,
                 data = gapm)
# Get regression table:
tidy(model1, conf.int = TRUE) %>% gt() # conf.int = TRUE part is new! 
```

```{=tex}
\begin{align}
\widehat{y} =& b_0 + b_1 \cdot x\\
\widehat{\text{life expectancy}} =& 50.9 + 0.232 \cdot \text{female literacy rate}
\end{align}
```
-   What are $H_0$ and $H_A$?
-   How do we calculate the standard error, statistic, *p*-value, and
    CI?

::: callout-note
-   We can also test & calculate CI for the population intercept
-   This will be covered in BSTA 512
:::

## Inference for the population [**slope**]{style="color:green"}: CI and hypothesis test

::: columns
::: {.column width="50%"}
**Population model**\
*line + random "noise"*

$$Y = \beta_0 + \beta_1 \cdot X + \varepsilon$$ with
$\varepsilon \sim N(0,\sigma)$\
$\sigma$ is the variability (SD) of the residuals

<br>

**Sample best-fit (least-squares) line:**

$$\widehat{y} = b_0 + b_1 \cdot x $$

Note: Some sources use $\widehat{\beta}$ instead of $b$.
:::

::: {.column width="50%"}
-   Construct a **95% confidence interval** for the **population slope**
    $\beta_1$

<br>

-   Conduct the **hypothesis test**

```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
<br>

*Note: R reports p-values for 2-sided tests*
:::
:::

## CI for population [**slope**]{style="color:green"} $\beta_1$

Recall the general CI formula:

$$\textrm{Point Estimate} \pm t^*\cdot SE_{\textrm{Point Estimate}}$$

For the CI of the coefficient $b_1$ this translates to

$$b_1 \pm t^*\cdot SE_{b_1}$$ where $t^*$ is the critical value from a
$t$-distribution with $df = n -2$.

<br>

*How is* $\text{SE}_{b_1}$ *calculated?* See next slide.

<br>

```{r}
tidy(model1, conf.int = TRUE)
```

## Standard error of fitted slope $b_1$

::: columns
::: {.column width="50%"}
$$\text{SE}_{b_1} = \frac{s_{\textrm{residuals}}}{s_x\sqrt{n-1}}$$
:::

::: {.column width="50%"}
$\text{SE}_{b_1}$ is the **variability** of the statistic $b_1$
:::
:::

::: {style="font-size: 90%;"}
::: columns
::: {.column width="32%"}
-   $s_{\textrm{residuals}}^2$ is the sd of the residuals
:::

::: {.column width="1%"}
:::

::: {.column width="32%"}
-   $s_x$ is the sample sd of the explanatory variable $x$
:::

::: {.column width="1%"}
:::

::: {.column width="32%"}
-   $n$ is the sample size, or the number of (complete) pairs of points
:::
:::
:::

```{r}
glance(model1)

# standard deviation of the residuals (Residual standard error in summary() output)
(s_resid <- glance(model1)$sigma)

# standard deviation of x's
(s_x <- sd(gapm$female_literacy_rate_2011))

# number of pairs of complete observations
(n <- nobs(model1))

(se_b1 <- s_resid/(s_x * sqrt(n-1))) # compare to SE in regression output
```

## Calculate CI for population [**slope**]{style="color:green"} $\beta_1$

::: columns
::: {.column width="50%"}
$$b_1 \pm t^*\cdot SE_{b_1}$$
:::

::: {.column width="50%"}
where $t^*$ is the $t$-distribution critical value with $df = n -2$.
:::
:::

```{r}
tidy(model1, conf.int = TRUE) %>% gt()
```

Save regression output for the row with the slope's information:

```{r}
model1_b1 <-tidy(model1) %>% filter(term == "female_literacy_rate_2011")
model1_b1 %>% gt()
```

::: columns
::: {.column width="50%"}
Save values needed for CI:

```{r}
b1 <- model1_b1$estimate
SE_b1 <- model1_b1$std.error
```

```{r}
nobs(model1) # sample size n
(tstar <- qt(.975, df = 80-2))
```
:::

::: {.column width="50%"}
Compare CI bounds below with the ones in the regression table above.

```{r}
(CI_LB <- b1 - tstar*SE_b1)
(CI_UB <- b1 + tstar*SE_b1)
```
:::
:::

## Hypothesis test for population [**slope**]{style="color:green"} $\beta_1$

::: columns
::: {.column width="40%"}
```{=tex}
\begin{align}
H_0 &: \beta_1 = 0\\
\text{vs. } H_A&: \beta_1 \neq 0
\end{align}
```
:::

::: {.column width="60%"}
The **test statistic** for $b_1$ is

$$t = \frac{ b_1 - \beta_1}{ \text{SE}_{b_1}} = \frac{ b_1}{ \text{SE}_{b_1}}$$

when we assume $H_0: \beta_1 = 0$ is true.
:::
:::

```{r}
tidy(model1, conf.int = TRUE) %>% gt()
```

Calculate the test statistic using the values in the regression table:

```{r}
# recall model1_b1 is regression table restricted to b1 row
(TestStat <- model1_b1$estimate / model1_b1$std.error)
```

Compare this test statistic value to the one from the regression table
above

## $p$-value for testing population [**slope**]{style="color:green"} $\beta_1$

-   As usual, the $p$-value is the *probability of obtaining a test
    statistic* **just as extreme or more extreme** *than the observed
    test statistic assuming the null hypothesis* $H_0$ *is true.*

-   To calculate the $p$-value, we need to know the probability
    distribution of the test statistic (the *null distribution*)
    assuming $H_0$ is true.

-   Statistical theory tells us that the test statistic $t$ can be
    modeled by a [$t$-distribution]{style="color:green"} with
    [$df = n-2$]{style="color:green"}.

-   Recall that this is a 2-sided test:

```{r}
(pv = 2*pt(TestStat, df=80-2, lower.tail=F))
```

Compare the $p$-value to the one from the regression table below

```{r}
tidy(model1, conf.int = TRUE) %>% gt()  # compare p-value calculated above to p-value in table
```

# Prediction (& inference)

1.  Prediction for mean response
2.  Prediction for new individual observation

## Prediction with regression line

```{r}
#| echo: false
tidy(model1) %>% gt()
```

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot \textrm{female literacy rate} $$

What is the predicted life expectancy for a country with female literacy
rate 60%?

$$\widehat{\textrm{life expectancy}} = 50.9 + 0.232 \cdot 60 = `r 50.9 + 0.232*60`$$

```{r}
(y_60 <- 50.9 + 0.232*60)
```

<br>

-   How do we interpret the predicted value?
-   How variable is it?

## Prediction with regression line

::: columns
::: {.column width="50%"}
Recall the population model:

*line + random "noise"*

$$Y = \beta_0 + \beta_1 \cdot X + \varepsilon$$ with
$\varepsilon \sim N(0,\sigma)$\
$\sigma$ is the variability (SD) of the residuals

<br>

-   When we take the expected value, at a given value $x^*$, we have
    that the predicted response is the average expected response at
    $x^*$:

$$\widehat{E[Y|x^*]} = b_0 + b_1 x^*$$
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| fig.height: 8.0
#| fig.width: 8.0
ggplot(gapm, aes(x = female_literacy_rate_2011,
                 y = life_expectancy_years_2011)) +
  geom_point() +
  labs(x = "female literacy rate", 
       y = "life expectancy",
       title = "Life expectancy vs. female literacy rate") +  
  geom_smooth(method = "lm", se = TRUE) +
  geom_vline(xintercept = 60, color = "green3")
```
:::
:::

-   These are the points on the regression line.
-   The mean responses has variability, and we can calculate a CI for
    it, for every value of $x^*$.

## CI for mean response $\mu_{Y|x^*}$

$$\widehat{E[Y|x^*]} \pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}$$

-   $SE_{\widehat{E[Y|x^*]}}$ is calculated using

$$SE_{\widehat{E[Y|x^*]}} = s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}$$

-   $\widehat{E[Y|x^*]}$ is the predicted value at the specified point
    $x^*$ of the explanatory variable
-   $s_{\textrm{residuals}}^2$ is the sd of the residuals
-   $n$ is the sample size, or the number of (complete) pairs of points
-   $\bar{x}$ is the sample mean of the explanatory variable $x$
-   $s_x$ is the sample sd of the explanatory variable $x$

<br>

-   Recall that $t_{n-2}^*$ is calculated using `qt()` and depends on
    the confidence level.

## Example: CI for mean response $\mu_{Y|x^*}$

**Find the 95% CI for the mean life expectancy when the female literacy
rate is 60.**

::: {style="font-size: 80%;"}
```{=tex}
\begin{align}
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}\\
64.8596 &\pm 1.990847 \cdot s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}\\
64.8596 &\pm 1.990847 \cdot 6.142157 \sqrt{\frac{1}{80} + \frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\
64.8596 &\pm 1.990847 \cdot 0.9675541\\
64.8596 &\pm 1.926252\\
(62.93335 &, 66.78586)
\end{align}
```
:::

::: {style="font-size: 90%;"}
::: columns
::: {.column width="50%"}
```{r}
(Y60 <- 50.9278981 + 0.2321951 * 60)
(tstar <- qt(.975, df = 78))
(s_resid <- glance(model1)$sigma)
```
:::

::: {.column width="50%"}
```{r}
(n <- nobs(model1))
(mx <- mean(gapm$female_literacy_rate_2011))
(s_x <- sd(gapm$female_literacy_rate_2011))
```
:::
:::

```{r}
(SE_Yx <- s_resid *sqrt(1/n + (60 - mx)^2/((n-1)*s_x^2)))
```

::: columns
::: {.column width="32%"}
```{r}
(MOE_Yx <- SE_Yx*tstar)
```
:::

::: {.column width="1%"}
:::

::: {.column width="32%"}
```{r}
Y60 - MOE_Yx
```
:::

::: {.column width="1%"}
:::

::: {.column width="32%"}
```{r}
Y60 + MOE_Yx
```
:::
:::
:::

## Example: Using R for CI for mean response $\mu_{Y|x^*}$

**Find the 95% CI's for the mean life expectancy when the female
literacy rate is 40, 60, and 80.**

-   Use the base R `predict()` function
-   Requires specification of a `newdata` "value"
    -   The `newdata` value is $x^*$
    -   This has to be in the format of a data frame though
    -   with column name identical to the predictor variable in the
        model

```{r}
newdata <- data.frame(female_literacy_rate_2011 = c(40, 60, 80)) 
newdata
```

::: columns
::: {.column width="50%"}
```{r}
predict(model1, 
        newdata=newdata, 
        interval="confidence")
```
:::

::: {.column width="50%"}
### Interpretation

We are 95% confident that the **average** life expectancy for a country
with a 60% female literacy rate will be between 62.9 and 66.8 years.
:::
:::

## Confidence bands for mean response $\mu_{Y|x^*}$

-   Often we plot the CI for many values of X, creating **confidence
    bands**
-   The confidence bands are what ggplot creates when we set `se = TRUE`
    within `geom_smooth`
-   For what values of x are the confidence bands (intervals) narrowest?

```{r}
ggplot(gapm,
       aes(x=female_literacy_rate_2011, 
           y=life_expectancy_years_2011)) +
  geom_point()+
  geom_smooth(method = lm, se=TRUE)+
  ggtitle("Life expectancy vs. female literacy rate") 
```

## Width of confidence bands for mean response $\mu_{Y|x^*}$

-   For what values of $x^*$ are the confidence bands (intervals)
    narrowest? widest?

```{=tex}
\begin{align}
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot SE_{\widehat{E[Y|x^*]}}\\
\widehat{E[Y|x^*]} &\pm t_{n-2}^* \cdot s_{residuals} \sqrt{\frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}
\end{align}
```
```{r}
#| echo: false
ggplot(gapm,
       aes(x=female_literacy_rate_2011, 
           y=life_expectancy_years_2011)) +
  geom_point()+
  geom_smooth(method = lm, se=TRUE)+
  ggtitle("Life expectancy vs. female literacy rate") +
  geom_vline(xintercept = mx, color = "purple")
```

## Prediction interval for predicting **individual** observations

-   We do not call this interval a CI since $Y$ is a random variable
    instead of a parameter
-   The form is similar to a CI though:

$$\widehat{Y|x^*} \pm t_{n-2}^* \cdot s_{residuals} \sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}$$

-   Note that the only difference to the CI for a mean value of y is the
    additional `1+` under the square root.
    -   Thus the width is wider!

## Example: Prediction interval

**Find the 95% prediction interval for the life expectancy when the
female literacy rate is 60.**

```{=tex}
\begin{align}
\widehat{Y|x^*} &\pm t_{n-2}^* \cdot s_{residuals} \sqrt{1 + \frac{1}{n} + \frac{(x^* - \bar{x})^2}{(n-1)s_x^2}}\\
64.8596 &\pm 1.990847 \cdot 6.142157 \sqrt{1+\frac{1}{80} + \frac{(60 - 81.65375)^2}{(80-1)21.95371^2}}\\
(52.48072 &, 77.23849)
\end{align}
```
::: {style="font-size: 90%;"}
::: columns
::: {.column width="50%"}
```{r}
(Y60 <- 50.9278981 + 0.2321951 * 60)
(tstar <- qt(.975, df = 78))
(s_resid <- glance(model1)$sigma)
```
:::

::: {.column width="50%"}
```{r}
(n <- nobs(model1))
(mx <- mean(gapm$female_literacy_rate_2011))
(s_x <- sd(gapm$female_literacy_rate_2011))
```
:::
:::

```{r}
(SE_Ypred <- s_resid *sqrt(1 + 1/n + (60 - mx)^2/((n-1)*s_x^2)))
```

::: columns
::: {.column width="36%"}
```{r}
(MOE_Ypred <- SE_Ypred*tstar)
```
:::

::: {.column width="1%"}
:::

::: {.column width="30%"}
```{r}
Y60 - MOE_Ypred
```
:::

::: {.column width="1%"}
:::

::: {.column width="30%"}
```{r}
Y60 + MOE_Ypred
```
:::
:::
:::

## Example: Using R for prediction interval

**Find the 95% prediction intervals for the life expectancy when the
female literacy rate is 40, 60, and 80.**

```{r}
newdata  # previously defined for CI's

predict(model1, 
        newdata=newdata, 
        interval="prediction")  # prediction instead of "confidence"
```

<br>

### Interpretation

We are 95% confident that a new selected country with a 60% female
literacy rate will have a life expectancy between 52.5 and 77.2 years.

## Prediction bands vs. confidence bands (1/2)

Create a scatterplot with the regression line, 95% confidence bands, and
95% prediction bands.

-   First create a data frame with the original data points (both x and
    y values), their respective predicted values, andtheir respective
    prediction intervals
-   Can do this with `augment()` from the `broom` package.

```{r}
model1_pred_bands <- augment(model1, interval = "prediction")

# take a look at new object:
names(model1_pred_bands) 

# glimpse of select variables of interest:
model1_pred_bands %>% 
  select(life_expectancy_years_2011, female_literacy_rate_2011, 
         .fitted:.upper) %>% 
  glimpse()
```

## Prediction bands vs. confidence bands (2/2)

```{r}
names(model1_pred_bands) 
```

```{r}
ggplot(model1_pred_bands, 
       aes(x=female_literacy_rate_2011, y=life_expectancy_years_2011)) +
  geom_point() +
  geom_ribbon(aes(ymin = .lower, ymax = .upper), # prediction bands
              alpha = 0.2, fill = "red") +
  geom_smooth(method=lm) +  # confidence bands
  labs(title = "SLR with Confidence & Prediction Bands") 

```

# Caution...

## Corrrelation doesn't imply causation`*`!

-   This might seem obvious, but make sure to not write your analysis
    results in a way that implies causation if the study design doesn't
    warrant it (such as an observational study).

-   Beware of spurious correlations:
    <http://www.tylervigen.com/spurious-correlations>

![](/img_slides/spurious_corr_2.png)

-   `*`Caveat: there is a whole field of statistics/epidemiology on
    causal inference. <https://ftp.cs.ucla.edu/pub/stat_ser/r350.pdf>

## What's next?

<br> <br>

![](/img_slides/flowchart_only_continuous.jpg){fig-align="center"}
